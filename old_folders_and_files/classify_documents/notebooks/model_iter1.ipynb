{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string, re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "INTER_PATH = os.path.join(\"data\", \"interim\")\n",
    "with open(os.path.join(INTER_PATH, \"pdf_files.json\")) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "#data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_stopwords = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_freq_words(doc_list, stopwords_set, num_freq_words):\n",
    "    filtered_words = list()\n",
    "    for doc in doc_list:\n",
    "        tokens = [word.lower() for word in word_tokenize(doc)]\n",
    "        filtered_words.extend([word for word in tokens if word.isalpha() and word not in stopwords_set and len(word) > 1])\n",
    "    \n",
    "    return FreqDist(filtered_words).most_common(num_freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_from_json(data):\n",
    "    doc_list = list()\n",
    "    for doc_json in data.values():\n",
    "        doc_list.append(doc_json[\"Text\"])\n",
    "    \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = docs_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_stopwords.add('ley')\n",
    "spa_stopwords.add('artículo')\n",
    "freq_words_map = most_freq_words(all_docs, spa_stopwords, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [sent.lower() for sent in sent_tokenize(sample_txt)]\n",
    "filtered_sents = [\" \".join([word for word in sent.split() if word.isalpha() and word not in spa_stopwords]) for sent in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDist(filtered_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Cleaning\n",
    "\n",
    "- 'CreditoGanadero_Mexico': not readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_spanish = set(stopwords.words('spanish')).union(set(['ley', 'artículo']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "puntuation = [p for p in set(string.punctuation) if p not in (\".\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_stopwords(df):\n",
    "    df = df.lower().replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip(\" \")\n",
    "    df = \"\".join( c for c in df.split(\" \") if c not in puntuation)\n",
    "    df = \" \".join([w for w in df.split() if w not in stopwords_spanish])\n",
    "    #df = \" \".join([w for w in df.split() if w not in additional_stopwords])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Json to DF\n",
    "clean_docs = pd.DataFrame([])\n",
    "for doc_name, doc_json in data.items():\n",
    "    doc = pd.json_normalize(doc_json)\n",
    "    doc.insert(0, \"Document\", doc_name) \n",
    "    #print(doc)\n",
    "    clean_docs = clean_docs.append(doc, ignore_index = True)\n",
    "\n",
    "# Set label\n",
    "clean_docs.insert(11,'Label','Relevant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POLICY_ID</th>\n",
       "      <th>FULL_DOC_LINK</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>DATE</th>\n",
       "      <th>SUBJECT</th>\n",
       "      <th>KEYWORDS</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>IS_RELEVANT</th>\n",
       "      <th>REVIEWED_FULL_DOC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>LEX-FAOC113577</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/pdf/per113577.pdf</td>\n",
       "      <td>Resolución Nº 012/12/AG/SENASA/DSV - Requisito...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Cultivated plants</td>\n",
       "      <td>Plant protection, Textile plants/fibres, Pests...</td>\n",
       "      <td>La presente Resolución establece los requisito...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>LEX-FAOC142723</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/pdf/per142723.pdf</td>\n",
       "      <td>Resolución Nº 045/14/MINAGRI/SENASA/DSV - Reti...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Cultivated plants</td>\n",
       "      <td>Hygiene/sanitary procedures, Planting material...</td>\n",
       "      <td>La presente Resolución retira determinadas pla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>LEX-FAOC125593</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/pdf/per125593.pdf</td>\n",
       "      <td>Resolución Nº 178/13/MINCETUR - Reglamento Int...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Agricultural &amp; rural development</td>\n",
       "      <td>Indigenous peoples, Cultural heritage, Extensi...</td>\n",
       "      <td>La presente Resolución aprueba el Reglamento I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>LEX-FAOC142231</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/pdf/per142231.pdf</td>\n",
       "      <td>Resolución Nº 034/14/MINAGRI/SENASA/DSV - Modi...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Cultivated plants</td>\n",
       "      <td>Plant protection, Pests/diseases, Planting mat...</td>\n",
       "      <td>La presente Resolución modifica la que estable...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>LEX-FAOC105348</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/pdf/per105348.pdf</td>\n",
       "      <td>Decreto Supremo Nº 048/11/EM - Modifica el Dec...</td>\n",
       "      <td>2011</td>\n",
       "      <td>Energy, Mineral resources</td>\n",
       "      <td>Energy conservation/energy production, Oil, En...</td>\n",
       "      <td>El presente Decreto Supremo modifica el Glosar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          POLICY_ID                                      FULL_DOC_LINK  \\\n",
       "995  LEX-FAOC113577  http://extwprlegs1.fao.org/docs/pdf/per113577.pdf   \n",
       "996  LEX-FAOC142723  http://extwprlegs1.fao.org/docs/pdf/per142723.pdf   \n",
       "997  LEX-FAOC125593  http://extwprlegs1.fao.org/docs/pdf/per125593.pdf   \n",
       "998  LEX-FAOC142231  http://extwprlegs1.fao.org/docs/pdf/per142231.pdf   \n",
       "999  LEX-FAOC105348  http://extwprlegs1.fao.org/docs/pdf/per105348.pdf   \n",
       "\n",
       "                                                 TITLE  DATE  \\\n",
       "995  Resolución Nº 012/12/AG/SENASA/DSV - Requisito...  2012   \n",
       "996  Resolución Nº 045/14/MINAGRI/SENASA/DSV - Reti...  2014   \n",
       "997  Resolución Nº 178/13/MINCETUR - Reglamento Int...  2013   \n",
       "998  Resolución Nº 034/14/MINAGRI/SENASA/DSV - Modi...  2014   \n",
       "999  Decreto Supremo Nº 048/11/EM - Modifica el Dec...  2011   \n",
       "\n",
       "                              SUBJECT  \\\n",
       "995                 Cultivated plants   \n",
       "996                 Cultivated plants   \n",
       "997  Agricultural & rural development   \n",
       "998                 Cultivated plants   \n",
       "999         Energy, Mineral resources   \n",
       "\n",
       "                                              KEYWORDS  \\\n",
       "995  Plant protection, Textile plants/fibres, Pests...   \n",
       "996  Hygiene/sanitary procedures, Planting material...   \n",
       "997  Indigenous peoples, Cultural heritage, Extensi...   \n",
       "998  Plant protection, Pests/diseases, Planting mat...   \n",
       "999  Energy conservation/energy production, Oil, En...   \n",
       "\n",
       "                                              ABSTRACT  IS_RELEVANT  \\\n",
       "995  La presente Resolución establece los requisito...          NaN   \n",
       "996  La presente Resolución retira determinadas pla...          NaN   \n",
       "997  La presente Resolución aprueba el Reglamento I...          NaN   \n",
       "998  La presente Resolución modifica la que estable...          NaN   \n",
       "999  El presente Decreto Supremo modifica el Glosar...          NaN   \n",
       "\n",
       "     REVIEWED_FULL_DOC  \n",
       "995                NaN  \n",
       "996                NaN  \n",
       "997                NaN  \n",
       "998                NaN  \n",
       "999                NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "INTER_PATH = os.path.join(\"data\", \"interim\")\n",
    "data = pd.read_excel(os.path.join(INTER_PATH, \"WRI_Ecolex_relevance_classification_sample.xlsx\"))\n",
    "\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(608, 10)\n"
     ]
    }
   ],
   "source": [
    "data.dropna(how='any', inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['IS_RELEVANT'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['REVIEWED_FULL_DOC'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_docs=data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = clean_docs.ABSTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [w.split() for w in corpus]\n",
    "\n",
    "all_flat_words = [ewords for words in all_words for ewords in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing all the stop words from the corpus\n",
    "all_flat_words_ns = [w for w in all_flat_words if w not in stopwords_spanish]\n",
    "\n",
    "#removing all duplicates\n",
    "set_nf = set(all_flat_words_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique vocabulary words in the text_description column of the dataframe: 6239\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique vocabulary words in the text_description column of the dataframe: %d\"%len(set_nf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "\n",
    "The following steps are performed:\n",
    "   - Converting all of the data into lower case.\n",
    "   - FInd the root of the words to further reduce the feature size\n",
    "\n",
    "to do: When the document have 'Keywords', the keywords are added to the description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['POLICY_ID', 'FULL_DOC_LINK', 'TITLE', 'DATE', 'SUBJECT', 'KEYWORDS',\n",
       "       'ABSTRACT', 'IS_RELEVANT', 'REVIEWED_FULL_DOC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_docs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = SnowballStemmer('spanish')\n",
    "#porter=nltk.PorterStemmer()\n",
    "\n",
    "for each_row in clean_docs.itertuples():\n",
    "    # Add Keywords and Subject as part of the corpus\n",
    "    m1=map(lambda x: x , (str(each_row[5])+' '+str(each_row[6])).lower().split())\n",
    "    #m1 = map(lambda x: x , (each_row[11]).lower().split())\n",
    "    #print(each_row[11])\n",
    "    #Using Porter Stemmer in NLTK, find root\n",
    "    m2 = map(lambda x: porter.stem(x), m1)\n",
    "    #pre-processed string is stored in new column\n",
    "    clean_docs.loc[each_row[0],'Desc'] = ' '.join(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POLICY_ID</th>\n",
       "      <th>FULL_DOC_LINK</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>DATE</th>\n",
       "      <th>SUBJECT</th>\n",
       "      <th>KEYWORDS</th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>IS_RELEVANT</th>\n",
       "      <th>REVIEWED_FULL_DOC</th>\n",
       "      <th>Desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEX-FAOC091761</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/pdf/per91761.pdf</td>\n",
       "      <td>Resolución Nº 050/09/AG - Crea el Consejo Naci...</td>\n",
       "      <td>2009</td>\n",
       "      <td>Cultivated plants</td>\n",
       "      <td>Institution, Textile plants/fibres, Contract/a...</td>\n",
       "      <td>La presente Resolución crea el Consejo Naciona...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cultivat plants institution, textil plants/fib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LEX-FAOC122131</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/texts/mex12213...</td>\n",
       "      <td>Acuerdo que adiciona párrafos a la especificac...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Wild species &amp; ecosystems</td>\n",
       "      <td>Aquatic animals, Management/conservation, Enda...</td>\n",
       "      <td>Estas disposiciones incorporan una nueva norma...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wild speci &amp; ecosystems aquatic animals, manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEX-FAOC035623</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/texts/per35623...</td>\n",
       "      <td>Resolución Nº 016/02/CONAM - Crea el Grupo Téc...</td>\n",
       "      <td>2002</td>\n",
       "      <td>Environment gen., Waste &amp; hazardous substances</td>\n",
       "      <td>Institution, Hazardous substances, Pesticides,...</td>\n",
       "      <td>La presente Resolución crea el Grupo Técnico d...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>environment gen., wast &amp; hazardous substanc in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEX-FAOC137450</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/pdf/chi137450.pdf</td>\n",
       "      <td>Decreto Supremo Nº 66 - Aprueba reglamento que...</td>\n",
       "      <td>2013</td>\n",
       "      <td>Land &amp; soil</td>\n",
       "      <td>Agricultural land, Land tenure, Traditional ri...</td>\n",
       "      <td>En virtud del presente Decreto se aprueba el r...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>land &amp; soil agricultural land, land tenure, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LEX-FAOC134825</td>\n",
       "      <td>http://extwprlegs1.fao.org/docs/pdf/per134825.pdf</td>\n",
       "      <td>Resolución Nº 027/14/MINAGRI/SENASA/DSV - Requ...</td>\n",
       "      <td>2014</td>\n",
       "      <td>Cultivated plants</td>\n",
       "      <td>Plant protection, Pests/diseases, Planting mat...</td>\n",
       "      <td>La presente Resolución establece los requisito...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cultivat plants plant protection, pests/diseas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        POLICY_ID                                      FULL_DOC_LINK  \\\n",
       "0  LEX-FAOC091761   http://extwprlegs1.fao.org/docs/pdf/per91761.pdf   \n",
       "1  LEX-FAOC122131  http://extwprlegs1.fao.org/docs/texts/mex12213...   \n",
       "2  LEX-FAOC035623  http://extwprlegs1.fao.org/docs/texts/per35623...   \n",
       "3  LEX-FAOC137450  http://extwprlegs1.fao.org/docs/pdf/chi137450.pdf   \n",
       "4  LEX-FAOC134825  http://extwprlegs1.fao.org/docs/pdf/per134825.pdf   \n",
       "\n",
       "                                               TITLE  DATE  \\\n",
       "0  Resolución Nº 050/09/AG - Crea el Consejo Naci...  2009   \n",
       "1  Acuerdo que adiciona párrafos a la especificac...  2013   \n",
       "2  Resolución Nº 016/02/CONAM - Crea el Grupo Téc...  2002   \n",
       "3  Decreto Supremo Nº 66 - Aprueba reglamento que...  2013   \n",
       "4  Resolución Nº 027/14/MINAGRI/SENASA/DSV - Requ...  2014   \n",
       "\n",
       "                                          SUBJECT  \\\n",
       "0                               Cultivated plants   \n",
       "1                       Wild species & ecosystems   \n",
       "2  Environment gen., Waste & hazardous substances   \n",
       "3                                     Land & soil   \n",
       "4                               Cultivated plants   \n",
       "\n",
       "                                            KEYWORDS  \\\n",
       "0  Institution, Textile plants/fibres, Contract/a...   \n",
       "1  Aquatic animals, Management/conservation, Enda...   \n",
       "2  Institution, Hazardous substances, Pesticides,...   \n",
       "3  Agricultural land, Land tenure, Traditional ri...   \n",
       "4  Plant protection, Pests/diseases, Planting mat...   \n",
       "\n",
       "                                            ABSTRACT  IS_RELEVANT  \\\n",
       "0  La presente Resolución crea el Consejo Naciona...          0.0   \n",
       "1  Estas disposiciones incorporan una nueva norma...          0.0   \n",
       "2  La presente Resolución crea el Grupo Técnico d...          0.0   \n",
       "3  En virtud del presente Decreto se aprueba el r...          0.0   \n",
       "4  La presente Resolución establece los requisito...          0.0   \n",
       "\n",
       "   REVIEWED_FULL_DOC                                               Desc  \n",
       "0                0.0  cultivat plants institution, textil plants/fib...  \n",
       "1                0.0  wild speci & ecosystems aquatic animals, manag...  \n",
       "2                0.0  environment gen., wast & hazardous substanc in...  \n",
       "3                0.0  land & soil agricultural land, land tenure, tr...  \n",
       "4                0.0  cultivat plants plant protection, pests/diseas...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_docs.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New corpus\n",
    "corpus = clean_docs['Desc']\n",
    "#Initializing TFIDF vectorizer to conver the raw corpus to a matrix of TFIDF features \n",
    "# and also enabling the removal of stopwords\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating TFIDF features sparse matrix by fitting it on the specified corpus \n",
    "tfidf_matrix = vectorizer.fit_transform(corpus).todense()\n",
    "\n",
    "#Grabbing the name of the features.\n",
    "tfidf_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TF-IDF Features:  498\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of TF-IDF Features: \", tfidf_matrix.shape[1])\n",
    "\n",
    "# There are  34,617 columns that will be used for training the classifier\n",
    "# These are much smaller than the total number of unique vocabulary words\n",
    "# (79,262) that previously calculated\n",
    "\n",
    "# With only the abstract\n",
    "# 498 columns vs 6239 unique vocabulary words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time_container={'b_naive_bayes':0,'mn_naive_bayes':0,'random_forest':0,'linear_svm':0}\n",
    "prediction_time_container={'b_naive_bayes':0,'mn_naive_bayes':0,'random_forest':0,'linear_svm':0}\n",
    "\n",
    "accuracy_container={'b_naive_bayes':0,'mn_naive_bayes':0,'random_forest':0,'linear_svm':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "\n",
    "First, split our existing dataset into training and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test data (70-30 ratio)\n",
    "\n",
    "#considering the TFIDF features as the input\n",
    "variables = tfidf_matrix\n",
    "#labels for the classifier\n",
    "labels = clean_docs.IS_RELEVANT\n",
    "#splitting the data\n",
    "var_train, var_test, labels_train, labels_test = train_test_split(variables, labels, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Data: (425, 498)\n",
      "Shape of Test Data: (183, 498)\n"
     ]
    }
   ],
   "source": [
    "#analyzing the shape of the training and test data-set:\n",
    "print('Shape of Training Data: '+str(var_train.shape))\n",
    "print('Shape of Test Data: '+str(var_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define classifier\n",
    "log_reg = LogisticRegression()\n",
    "# Train classifier\n",
    "log_reg = log_reg.fit(var_train, labels_train)\n",
    "# Predictions\n",
    "log_predictions = log_reg.predict(var_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Logistic Regression *****\n",
      "Accuracy Score: 0.852459\n",
      "Recall Score: 0.384615\n",
      "Confusion Matrix output: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  141   3\n",
       "1   24  15"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Metrics\n",
    "print(\"***** Logistic Regression *****\")\n",
    "print(\"Accuracy Score: %f\"%sklearn.metrics.accuracy_score(labels_test, log_predictions))\n",
    "print(\"Recall Score: %f\"%sklearn.metrics.recall_score(labels_test, log_predictions))\n",
    "print(\"Confusion Matrix output: \")\n",
    "con_mat = pd.DataFrame(sklearn.metrics.confusion_matrix(labels_test, log_predictions))\n",
    "con_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 59.22222222222222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression()\n",
    "\n",
    "# Hyperparameters\n",
    "hyper_param_grid = {'penalty': ['l1', 'l2', 'elasticnet', 'none'], \n",
    "                   }\n",
    "\n",
    "# Gid search\n",
    "grid_search = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'recall',\n",
    "                           cv = 10, \n",
    "                           n_jobs = -1,\n",
    "                           verbose = 3)\n",
    "grid_search.fit(var_train, labels_train)\n",
    "\n",
    "# Best score\n",
    "print('Best score:', grid_search.best_score_ * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7978142076502732\n",
      "Recall Score:  0.5128205128205128\n",
      "Confusion Matrix: \n",
      "[[126  18]\n",
      " [ 19  20]]\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "pred_grid_search = grid_search.predict(var_test)\n",
    "\n",
    "print (\"Accuracy Score: \", sklearn.metrics.accuracy_score(labels_test, pred_grid_search))\n",
    "print (\"Recall Score: \", sklearn.metrics.recall_score(labels_test, pred_grid_search))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(sklearn.metrics.confusion_matrix(labels_test, pred_grid_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "Naive Bayes is one of the most widely used classification algorithm in text mining applications. The main assumption is that all the features are independent of each other. The condition of independence may not be valid in many circumstances but as a base line model, its a good starting point.\n",
    "\n",
    "Naive Bayes uses the probabilities of each attribute belonging to each class to make a prediction. There are two forms of Naive Bayes:\n",
    "\n",
    "   1. Bernoulli, designed for boolean/binary features i.e. just considers the presence or absense of a feature.\n",
    "   2. Multinomial, which also considers the occurrence counts of the feature.\n",
    "\n",
    "We will apply both and then will assess their respective accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    0.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 81.66666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "classifier = BernoulliNB()\n",
    "\n",
    "# Hyperparameters\n",
    "hyper_param_grid = {'alpha': [0.1, 0.25, 0.5, 0.75, 1],\n",
    "                    'fit_prior':['False', 'True']\n",
    "                   }\n",
    "\n",
    "# Gid search\n",
    "grid_search = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'recall',\n",
    "                           cv = 10, \n",
    "                           n_jobs = -1,\n",
    "                           verbose = 3)\n",
    "grid_search.fit(var_train, labels_train)\n",
    "\n",
    "# Best score\n",
    "print('Best score:', grid_search.best_score_ * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.7814207650273224\n",
      "Recall Score:  0.7948717948717948\n",
      "Confusion Matrix: \n",
      "[[112  32]\n",
      " [  8  31]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.78      0.85       144\n",
      "         1.0       0.49      0.79      0.61        39\n",
      "\n",
      "    accuracy                           0.78       183\n",
      "   macro avg       0.71      0.79      0.73       183\n",
      "weighted avg       0.84      0.78      0.80       183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "pred_grid_search = grid_search.predict(var_test)\n",
    "\n",
    "print (\"Accuracy Score: \", sklearn.metrics.accuracy_score(labels_test, pred_grid_search))\n",
    "print (\"Recall Score: \", sklearn.metrics.recall_score(labels_test, pred_grid_search))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(sklearn.metrics.confusion_matrix(labels_test, pred_grid_search))\n",
    "print(\"Classification Metrics: \")\n",
    "print(sklearn.metrics.classification_report(labels_test, pred_grid_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes\n",
    "\n",
    "Bernoulli Naive Bayes only considers whether a feature is present or not. However, if we also take into account the occurrence weight or count of the feature as well (in our case, the TF-IDF weight of each feature), we can hypothesize that the performance of such classifier will be equally good, if not better. That is assumption for the Multi-nomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:    0.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 66.44444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    1.5s finished\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "\n",
    "# Hyperparameters\n",
    "hyper_param_grid = {'alpha': [0.1, 0.25, 0.5, 0.75, 1],\n",
    "                    'fit_prior':['False', 'True']\n",
    "                   }\n",
    "\n",
    "# Gid search\n",
    "grid_search = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'recall',\n",
    "                           cv = 10, \n",
    "                           n_jobs = -1,\n",
    "                           verbose = 3)\n",
    "grid_search.fit(var_train, labels_train)\n",
    "\n",
    "# Best score\n",
    "print('Best score:', grid_search.best_score_ * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.8469945355191257\n",
      "Recall Score:  0.717948717948718\n",
      "Confusion Matrix: \n",
      "[[127  17]\n",
      " [ 11  28]]\n",
      "Classification Metrics: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.88      0.90       144\n",
      "         1.0       0.62      0.72      0.67        39\n",
      "\n",
      "    accuracy                           0.85       183\n",
      "   macro avg       0.77      0.80      0.78       183\n",
      "weighted avg       0.86      0.85      0.85       183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "pred_grid_search = grid_search.predict(var_test)\n",
    "\n",
    "print (\"Accuracy Score: \", sklearn.metrics.accuracy_score(labels_test, pred_grid_search))\n",
    "print (\"Recall Score: \", sklearn.metrics.recall_score(labels_test, pred_grid_search))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(sklearn.metrics.confusion_matrix(labels_test, pred_grid_search))\n",
    "print(\"Classification Metrics: \")\n",
    "print(sklearn.metrics.classification_report(labels_test, pred_grid_search))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "\n",
    "In Random Forest, a subset of the training data is fit on a number of decision trees. Random Forests have the characteristic to minimize variance if its there in the data-set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   38.6s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de la mejor métrica para el modelo de Random Forest: 50.11111111111111\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Hyperparameters\n",
    "hyper_param_grid = {'n_estimators': [100, 200], \n",
    "                     'max_depth': [5,10,20,50,100], \n",
    "                     'max_features': ['sqrt','log2'],\n",
    "                     'min_samples_split': [2,5,10,20,50]}\n",
    "\n",
    "# Gid search\n",
    "grid_search = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'recall',\n",
    "                           cv = 10, \n",
    "                           n_jobs = -1,\n",
    "                           verbose = 3)\n",
    "grid_search.fit(var_train, labels_train)\n",
    "\n",
    "# Best params\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Best score\n",
    "print('Valor de la mejor métrica para el modelo de Random Forest:', grid_search.best_score_ * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score of Random Forests Classifier:  0.8360655737704918\n",
      "Recall Score of Random Forests Classifier:  0.4358974358974359\n",
      "Confusion Matrix: \n",
      "[[136   8]\n",
      " [ 22  17]]\n"
     ]
    }
   ],
   "source": [
    "pred_grid_search = grid_search.predict(var_test)\n",
    "\n",
    "print (\"Accuracy Score of Random Forests Classifier: \", sklearn.metrics.accuracy_score(labels_test, pred_grid_search))\n",
    "print (\"Recall Score of Random Forests Classifier: \", sklearn.metrics.recall_score(labels_test, pred_grid_search))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(sklearn.metrics.confusion_matrix(labels_test, pred_grid_search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wri",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
