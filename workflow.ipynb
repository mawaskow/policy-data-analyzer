{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract body text from all PDFs into json. \n",
    "Split body text for each PDF into sentences.\n",
    "\n",
    "\n",
    "/extract_text/ > make_pdfs.py\n",
    "**Extract whole raw text from PDFs**\n",
    "input: extract_text/input/onedrive_docs.zip\n",
    "output: extract_text/output/new/pdf_files.json\n",
    "\n",
    "/text_preprocessing/ > sentence_split_local.py\n",
    "**Turn raw text into sentences**\n",
    "input: extract_text/output/new/pdf_files.json\n",
    "output: text_preprocessing/output/new/* ([filename]_[country]_sents.json)\n",
    "\n",
    "/data_augmentation/ > assisted_labeling.py\n",
    "**Use sentence similarity to get more labeled data (can be sample or all sents)**\n",
    "input: text_preprocessing/output/new/* ([filename]_[country]_sents.json)\n",
    "output: data_augmentation/output/new/* ([incentive]_[country].csv)\n",
    "                                        embeddings.json\n",
    "                                        pre-tagged.json ***this is the important output\n",
    "**Turns scored sentence dct into labelled sentences**\n",
    "input: text_preprocessing/output/new/pre-tagged.json\n",
    "output: data_augmentation/output/new/pre-tagged_fixed.json\n",
    "\n",
    "********** when do we do zero-shot classification?\n",
    "\n",
    "whyyy are they broken into country as well? why not just topic??\n",
    "> it's because they wanted to account for dialect differences\n",
    "where are latent embeddings classifier and nli topic classifier getting used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/extract_text/ > pdf_annots.py\n",
    "**Extract labelled sentences from PDFs (highlights and comments)**\n",
    "input: extract_text/input/onedrive_docs.zip\n",
    "output: extract_text/output/pdf_extract.json\n",
    "\n",
    "/text_preprocessing/ > cleaning_annots.py\n",
    "**Clean raw labels from the PDFs**\n",
    "input: extract_text/output/pdf_extract.json\n",
    "output: text_preprocessing/output/fixed_labels.json\n",
    "**THEN**\n",
    "**Remove empty labels and dictionary entries**\n",
    "input: text_preprocessing/output/fixed_labels.json\n",
    "output: text_preprocessing/output/fixed_empty.json\n",
    "\n",
    "/text_preprocessing/ > cleaning_hlts.py\n",
    "**Remove duplicate sentences from the highlights**\n",
    "input: text_preprocessing/output/fixed_empty.json\n",
    "output: text_preprocessing/output/fixed_hlts.json\n",
    "\n",
    "/text_preprocessing/ > sent_split_hlts.py\n",
    "**Clean labeled sentences**\n",
    "(however, it leaves some very large paragraphs)\n",
    "input: text_preprocessing/output/fixed_hlts.json\n",
    "output: text_preprocessing/output/sents_join.json\n",
    "**OR**\n",
    "**Clean labeled sentences and split into sentences**\n",
    "(however, it has some non-incentive sentence fragments labeled as incentives)\n",
    "input: text_preprocessing/output/fixed_hlts.json\n",
    "output: text_preprocessing/output/sents_split.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert sents_join.json\n",
    "convert sents_split.json\n",
    "convert pre-tagged.json\n",
    "convert text_preprocessing/tagged_sentences_all.csv\n",
    "into [sentences] [labels] jsons\n",
    "\n",
    "**turns sentences:labels dct into separate list files for fxns**\n",
    "/text_preprocessing/ > conv_to_indiv.py\n",
    "input: data_augmentation/output/new/pre-tagged_fixed.json\n",
    "output: text_preprocessing/output/reduced/\n",
    "                                        [Experiment]_sentences.json\n",
    "                                        [Experiment]_labels.json\n",
    "**turns highlight-extracted sentences into list files for fxns**\n",
    "input: data_augmentation/output/new/text_preprocessing/output/sents_[split/join].json\n",
    "output: text_preprocessing/output/reduced/\n",
    "                                        [Experiment]_sentences.json\n",
    "                                        [Experiment]_labels.json\n",
    "> first of all, there look to possibly be empty sentences.\n",
    "> whats that about\n",
    "> go figure it out\n",
    "> at another point\n",
    "> additionally, need to figure out how to sort out having multiple labels?\n",
    "> need to figure out how to duplicate sentence entries and each have separate label instead of all in one?\n",
    "> and figure out where empty sentences are happening\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classifier\n",
    "\n",
    "Binary Labelling\n",
    "data augmentation\n",
    "trying to get dissimilar sentences in the queries to have examples of non-incentives\n",
    "makes sentence list jsons of each dissimilar and the previosuly-generated similar jsons\n",
    "since they'll have binary labels anyways\n",
    "\n",
    "binary classifier working well, performing well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Classifier\n",
    "\n",
    "IPYNB atm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
