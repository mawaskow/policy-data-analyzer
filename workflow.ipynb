{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract body text from all PDFs into json. \n",
    "Split body text for each PDF into sentences.\n",
    "\n",
    "\n",
    "/extract_text/ > make_pdfs.py\n",
    "**Extract whole raw text from PDFs**\n",
    "input: extract_text/input/onedrive_docs.zip\n",
    "output: extract_text/output/new/pdf_files.json\n",
    "\n",
    "/text_preprocessing/ > sentence_split_local.py\n",
    "**Turn raw text into sentences**\n",
    "input: extract_text/output/new/pdf_files.json\n",
    "output: text_preprocessing/output/new/* ([filename]_[country]_sents.json)\n",
    "\n",
    "/data_augmentation/ > assisted_labeling.py\n",
    "**Use sentence similarity to get more labeled data (can be sample or all sents)**\n",
    "input: text_preprocessing/output/new/* ([filename]_[country]_sents.json)\n",
    "output: data_augmentation/output/new/* ([incentive]_[country].csv)\n",
    "                                        embeddings.json\n",
    "                                        pre-tagged.json ***this is the important output\n",
    "                                    \n",
    "********** when do we convert pre-tagged.json into labeled sentences?\n",
    "\n",
    "********** when do we do zero-shot classification?\n",
    "\n",
    "whyyy are they broken into country as well? why not just topic??\n",
    "> it's because they wanted to account for dialect differences\n",
    "where are latent embeddings classifier and nli topic classifier getting used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/extract_text/ > pdf_annots.py\n",
    "**Extract labelled sentences from PDFs (highlights and comments)**\n",
    "input: extract_text/input/onedrive_docs.zip\n",
    "output: extract_text/output/pdf_extract.json\n",
    "\n",
    "/text_preprocessing/ > cleaning_annots.py\n",
    "**Clean raw labels from the PDFs**\n",
    "input: extract_text/output/pdf_extract.json\n",
    "output: text_preprocessing/output/fixed_labels.json\n",
    "**THEN**\n",
    "**Remove empty labels and dictionary entries**\n",
    "input: text_preprocessing/output/fixed_labels.json\n",
    "output: text_preprocessing/output/fixed_empty.json\n",
    "\n",
    "/text_preprocessing/ > cleaning_hlts.py\n",
    "**Remove duplicate sentences from the highlights**\n",
    "input: text_preprocessing/output/fixed_empty.json\n",
    "output: text_preprocessing/output/fixed_hlts.json\n",
    "\n",
    "/text_preprocessing/ > sent_split_hlts.py\n",
    "**Clean labeled sentences**\n",
    "(however, it leaves some very large paragraphs)\n",
    "input: text_preprocessing/output/fixed_hlts.json\n",
    "output: text_preprocessing/output/sents_join.json\n",
    "**OR**\n",
    "**Clean labeled sentences and split into sentences**\n",
    "(however, it has some non-incentive sentence fragments labeled as incentives)\n",
    "input: text_preprocessing/output/fixed_hlts.json\n",
    "output: text_preprocessing/output/sents_split.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
