{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a search engine based on sBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there is a basic implementation of sBERT for searching a database of sentences with queries.\n",
    "\n",
    "The goal is to increase the amount of labeled data that we have in order to later fine tune a model to be used for sentence classification. First of all we have to find a pool of queries that represent the six labels of the six policy instruments. With these queries we can pull a set of sentences that can be automaticaly labeled with the same label of the query. In this way we can increase the diversity of labeled sentences in each label category. This approach will be complemented with a manual curation step to produce a high quality training data set.\n",
    "\n",
    "The policy instruments that we want to find and that correspond to the different labels are:\n",
    "* Direct payment (PES)\n",
    "* Tax deduction\n",
    "* Credit/guarantee\n",
    "* Technical assistance\n",
    "* Supplies\n",
    "* Fines\n",
    "\n",
    "This notebook is intended for the following purposes:\n",
    "* Try different query strategies to find the optimal retrieval of sentences in each policy instrument category\n",
    "* Try different transformers\n",
    "* Be the starting point for further enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is self contained, it does not depend on any other class of the sBERT folder.\n",
    "\n",
    "You just have to create an environment where you install the external dependencies. Usually the dependencies that you have to install are:\n",
    "\n",
    "**For the basic sentence similarity calculation**\n",
    "*  pandas\n",
    "*  boto3\n",
    "*  pytorch\n",
    "*  sentence_transformers\n",
    "\n",
    "**If you want to use ngrams to generate queries**\n",
    "*  nltk\n",
    "*  plotly\n",
    "*  wordcloud\n",
    "\n",
    "**If you want to do evaluation and ploting with pyplot**\n",
    "*  matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your environment is called nlp then you execute this cell otherwise you change the name of the environment\n",
    "!conda activate nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Model libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Libraries for model evaluation\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Libraries to be used in the process of definig queries\n",
    "import nltk # imports the natural language toolkit\n",
    "import plotly\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents from El Salvador have been preprocessed and their contents saved in a JSON file. In the JSON file there are the sentences of interest.\n",
    "\n",
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep the credentials in a local folder out of GitHub, you can change the path to adapt it to your needs.\n",
    "# Please, comment out other users lines and set your own\n",
    "# path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in desktop\n",
    "path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in laptop\n",
    "# path = \"\"\n",
    "#If you put the credentials file in the same \"notebooks\" folder then you can use the following path\n",
    "# path = \"\"\n",
    "filename = \"Omdena_key_S3.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_dict:\n",
    "    KEY = key\n",
    "    SECRET = key_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentence database from El Salvador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'JSON/ElSalvador.json'\n",
    "\n",
    "obj = s3.Object('wri-latin-talent',filename)\n",
    "serializedObject = obj.get()['Body'].read()\n",
    "policy_list = json.loads(serializedObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a list of potentially relevant sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going through the dictionary to retrieve sentences, we define a function to reduce de number of sentences in the final \"sentences\" dictionary. This is just for testing purposes. The reason being that running the sentence embedding function takes time. So for initial testing purposes we can reduce the number of sentences in the testing dataset.\n",
    "\n",
    "The variable \"slim_by\" is the reduction factor. If it is set to 1, there will be no reduction and we will be working with the full dataset. It it is set to two, we will take one every two sentences and so one.\n",
    "\n",
    "<span style=\"color:red\">REMEMBER</span> that you have to re-run the function \"get_sentences_dict\" with the \"slim_by\" variable set to 1 when you want to go for the final shoot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slim_dict(counter, slim_factor): # This is to shrink the sentences dict by a user set factor. It will pick only one sentence every \"slim_factor\"\n",
    "    if counter % slim_factor == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_sentences_dict(slim_factor, docs_dict, is_not_incentive_dict):\n",
    "    count = 0\n",
    "    result = {}\n",
    "    for key, value in docs_dict.items():\n",
    "        for item in value: \n",
    "            if item in is_not_incentive_dict:\n",
    "                continue\n",
    "            else:\n",
    "                for sentence in docs_dict[key][item]['sentences']:\n",
    "                    count += 1\n",
    "                    if slim_dict(count, slim_by):\n",
    "                        result[sentence] = docs_dict[key][item]['sentences'][sentence]\n",
    "                    else:\n",
    "                        continue\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will run the function to get your sentences list in a dictionary of this form:\n",
    "\n",
    "{\"\\<sentence id\\>\" : \"\\<text of the sentence\\>\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "                    \"POR TANTO\" : 0,\n",
    "                    \"DISPOSICIONES GENERALES\" : 0,\n",
    "                    \"OBJETO\" : 0,\n",
    "                    \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0}\n",
    "\n",
    "slim_by = 1 # REMEMBER to set this variable to the desired value. \n",
    "\n",
    "sentences = get_sentences_dict(slim_by, policy_list, is_not_incentive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this data set there are 349 policies and 40124 sentences\n"
     ]
    }
   ],
   "source": [
    "print(\"In this data set there are {} policies and {} sentences\".format(len(policy_list),len(sentences)))\n",
    "# for sentence in sentences:\n",
    "#     print(sentences[sentence]['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we use the excel file with the selected phrases of each country, process them and get N-grams to define basic queries for the SBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'WRI_Policy_Tags (1).xlsx', sheet_name = None)\n",
    "df = None\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        if not isinstance(df,pd.DataFrame):\n",
    "            df = value\n",
    "        else:\n",
    "            df = df.append(value)\n",
    "else:\n",
    "    df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = df[\"relevant sentences\"].apply(lambda x: x.split(\";\") if isinstance(x,str) else x)\n",
    "tagged_sentence = []\n",
    "\n",
    "for elem in tagged_sentences:\n",
    "    if isinstance(elem,float) or len(elem) == 0:\n",
    "        continue\n",
    "    elif isinstance(elem,list):\n",
    "        for i in elem:\n",
    "            if len(i.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                tagged_sentence.append(i.strip())\n",
    "    else:\n",
    "        if len(elem.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            tagged_sentence.append(elem.strip())\n",
    "\n",
    "tagged_sentence\n",
    "words_per_sentence = [len(x.split(\" \")) for x in tagged_sentence]\n",
    "plt.hist(words_per_sentence, bins = 50)\n",
    "plt.title(\"Histogram of number of words per sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "\n",
    "noise_words = []\n",
    "stopwords_corpus = nltk.corpus.stopword\n",
    "sp_stop_words = stopwords_corpus.words('spanish')\n",
    "noise_words.extend(sp_stop_words)\n",
    "print(len(noise_words))\n",
    "\n",
    "if \"no\" in noise_words:\n",
    "    noise_words.remove(\"no\")\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(''.join(tagged_sentence))\n",
    "word_freq = Counter(tokenized_words)\n",
    "# word_freq.most_common(20)\n",
    "# list(ngrams(tokenized_words, 3))\n",
    "\n",
    "word_tokens_clean = [re.findall(r\"[a-zA-Z]+\",each) for each in tokenized_words if each.lower() not in noise_words and len(each.lower()) > 1]\n",
    "word_tokens_clean = [each[0].lower() for each in word_tokens_clean if len(each)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the size of the n-gram that we want to find. The larger it is, the less frequent it will be, unless we substantially increase the number of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 2\n",
    "\n",
    "top_k_ngrams(word_tokens_clean, n_grams, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building queries with Parts-Of-Speech\n",
    "\n",
    "The following functions take a specific word and find the next or previous words according to the POS tags.\n",
    "\n",
    "An example is shown below with the text: <br>\n",
    "\n",
    "text = \"Generar empleo y garantizara la población campesina el bienestar y su participación e incorporación en el desarrollo nacional, y fomentará la actividad agropecuaria y forestal para el óptimo uso de la tierra, con obras de infraestructura, insumos, créditos, servicios de capacitación y asistencia técnica\" <br>\n",
    "\n",
    "next_words(text, \"empleo\", 3) <br>\n",
    "prev_words(text, \"garantizara\", 6) <br>\n",
    "\n",
    "Will return: <br>\n",
    "\n",
    ">['garantizara', 'población', 'campesina'] <br>\n",
    ">['Generar', 'empleo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = es_core_news_md.load()\n",
    "\n",
    "def ExtractInteresting(sentence, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "#     interesting = [k for k,v in nltk.pos_tag(words) if v in match]\n",
    "    doc = nlp(sentence)\n",
    "    interesting = [k.text for k in doc if k.pos_ in match]\n",
    "    return(interesting)\n",
    "\n",
    "def next_words(sentence, word, num_words, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "\n",
    "    items = list()\n",
    "    doc = nlp(sentence)\n",
    "    text = [i.text for i in doc]\n",
    "\n",
    "    if word not in text: return \"\"\n",
    "    \n",
    "    idx = text.index(word)\n",
    "    for num in range(num_words):\n",
    "        \n",
    "        pos_words = [k.text for k in doc[idx:] if k.pos_ in match]\n",
    "        if len(pos_words) > 1: \n",
    "            items.append(pos_words[1])\n",
    "            idx = text.index(pos_words[1])\n",
    "    \n",
    "    return items\n",
    "    \n",
    "def prev_words(sentence, word, num_words, match = [\"ADJ\",\"ADV\", \"NOUN\", \"NUM\", \"VERB\", \"AUX\"]):\n",
    "    \n",
    "    items = list()\n",
    "    doc = nlp(sentence)\n",
    "    text = [i.text for i in doc]\n",
    "\n",
    "    if word not in text: return \"\"\n",
    "    \n",
    "    idx = text.index(word)\n",
    "    for num in range(num_words):\n",
    "        pos_words = [k.text for k in doc[:idx] if k.pos_ in match]\n",
    "        if len(pos_words) >= 1: \n",
    "            items.insert(0, pos_words[-1]) #Add element in order and take the last element since it is the one before the word\n",
    "            idx = text.index(pos_words[-1])\n",
    "    \n",
    "    return items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression to find incentive policy instruments\n",
    "keywords = re.compile(r'(asistencia tecnica)|ayuda\\s*s*\\s*\\b|\\bbono\\s*s*\\b\\s*|credito\\s*s*\\b\\s*|deduccion\\s*(es)*\\b\\s*|devolucion\\s*(es)*\\b\\s*|incentivo\\s*s*\\b\\s*|insumo\\s*s*\\b\\s*|multa\\s*s*\\b\\s*')\n",
    "\n",
    "# Function to change accented words by non-accented counterparts. It depends on the dictionary \"accent_marks_bugs\" \n",
    "accents_out = re.compile(r'[áéíóúÁÉÍÓÚ]')\n",
    "accents_dict = {\"á\":\"a\",\"é\":\"e\",\"í\":\"i\",\"ó\":\"o\",\"ú\":\"u\",\"Á\":\"A\",\"É\":\"E\",\"Í\":\"I\",\"Ó\":\"O\",\"Ú\":\"U\"}\n",
    "def remove_accents(string):\n",
    "    for accent in accents_out.findall(string):\n",
    "        string = string.replace(accent, accents_dict[accent])\n",
    "    return string\n",
    "# Dictionary to merge variants of a word\n",
    "families = {\n",
    "    \"asistencia tecnica\" : \"asistencia técnica\",\n",
    "    \"ayuda\" : \"ayuda\",\n",
    "    \"ayudas\" : \"ayuda\",\n",
    "    \"bono\" : \"bono\",\n",
    "    \"bonos\" : \"bono\",\n",
    "    \"credito\":  \"crédito\",\n",
    "    \"creditos\":  \"crédito\",\n",
    "    \"deduccion\" : \"deducción\",\n",
    "    \"deducciones\" : \"deducción\",\n",
    "    \"devolucion\" : \"devolución\",\n",
    "    \"devoluciones\" : \"devolución\",\n",
    "    \"incentivo\" : \"incentivo\",\n",
    "    \"incentivos\" : \"incentivo\",\n",
    "    \"insumo\" : \"insumo\",\n",
    "    \"insumos\" : \"insumo\",\n",
    "    \"multa\" : \"multa\",\n",
    "    \"multas\" : \"multa\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_in_sentences = []\n",
    "            \n",
    "for sentence in sentences:\n",
    "    line = remove_accents(sentences[sentence]['text'])\n",
    "    hit = keywords.search(line)\n",
    "    if hit:\n",
    "        keyword = hit.group(0).rstrip().lstrip()\n",
    "        keyword_in_sentences.append([families[keyword], sentences[sentence]['text']])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['bono',\n",
       "  'La Municipalidad en coordinacion con la Mancomunidad Trinacional Fronteriza Rio Lempa, Entregar anualmente implementaran anualmente la entrega, a los propietarios de bosques y areas naturales, bonos o incentivos forestales, con el fin de lograr la conservacion de dichas areas'],\n",
       " ['bono',\n",
       "  'La Municipalidad en coordinacion con la Mancomunidad Trinacional Fronteriza Rio Lempa, implementaran anualmente la entrega, a los propietarios de bosques y areas naturales, bonos o incentivos forestales, con el fin de lograr la conservacion de dichas areas'],\n",
       " ['bono', 'Emisiones de bonos u otros titulos-valores; y'],\n",
       " ['bono',\n",
       "  'Entregar anualmente, a los propietarios de bosques y areas naturales, bonos o incentivos forestales, con el fin de lograr la conservacion de dichas areas'],\n",
       " ['bono',\n",
       "  'En el caso del Fondo de Saneamiento y Fortalecimiento Financiero, este debera recibir el pago referido, ya sea por medio de efectivo, de los antes relacionados bonos, o con bienes inmuebles, cuyos precios se estableceran por medio de un perito valuador inscrito en la Superintendencia del Sistema Financiero'],\n",
       " ['bono',\n",
       "  '- Autorizase al BFA para que transfiera al FOSAFRI, los prestamos de su cartera que sean objeto de aplicacion de esta Ley, y a este para que entregue en pago de dicha cartera bonos de la Reforma Agraria']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(keyword_in_sentences))\n",
    "keyword_in_sentences = sorted(keyword_in_sentences, key = lambda x : x[0])\n",
    "# print(keyword_in_sentences[0:20])\n",
    "filtered = [row for row in keyword_in_sentences if row[0] == \"bono\"]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asistencia técnica -- 19\n",
      "ayuda -- 14\n",
      "bono -- 6\n",
      "crédito -- 65\n",
      "deducción -- 2\n",
      "devolución -- 6\n",
      "incentivo -- 60\n",
      "insumo -- 29\n",
      "multa -- 1384\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in families.items():\n",
    "    if i % 2 == 0:\n",
    "        print(value, \"--\", len([row for row in keyword_in_sentences if row[0] == value]))\n",
    "    i += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the sBERT model. Several transformers are available and documentation is here: https://github.com/UKPLab/sentence-transformers <br>\n",
    "\n",
    "Then we build a simple function that takes four inputs:\n",
    "1. The model as we have set it in the previous line of code\n",
    "2. A dictionary that contains the sentences {\"\\<sentence_ID\\>\" : {\"text\" : \"The actual sentence\", labels : []}\n",
    "3. A query in the form of a string\n",
    "4. A similarity treshold. It is a float that we can use to limit the results list to the most relevant.\n",
    "\n",
    "The output of the function is a list with three columns with the following content:\n",
    "1. Column 1 contains the id of the sentence\n",
    "2. Column 2 contains the similarity score\n",
    "3. Column 3 contains the text of the sentence that has been compared with the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_name='xlm-r-100langs-bert-base-nli-mean-tokens'\n",
    "transformer_name = \"distiluse-base-multilingual-cased\"\n",
    "model = SentenceTransformer(transformer_name)\n",
    "\n",
    "def highlight(model, sentences_dict, query, similarity_treshold):\n",
    "    query_embedding = model.encode(query)\n",
    "    highlights = []\n",
    "    for sentence in sentences_dict:\n",
    "        sentence_embedding = model.encode(sentence)\n",
    "        score = 1 - distance.cosine(sentence_embedding, query_embedding)\n",
    "        if score > similarity_treshold:\n",
    "            highlights.append([sentence, score, sentences_dict[sentence]['text']])\n",
    "    highlights = sorted(highlights, key = lambda x : x[1], reverse = True)\n",
    "    return highlights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ti = time.perf_counter()\n",
    "\n",
    "highlighter_query = \"multa\"\n",
    "highlighter_precision = 0.2\n",
    "\n",
    "label_1 = highlight(model, sentences, highlighter_query, highlighter_precision)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"similarity search for El Salvador sentences done in {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_1))\n",
    "label_1[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further filtering of the results by using the similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_treshold = 0.5\n",
    "filtered = [row for row in label_1 if row[1] > similarity_treshold]\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "export_query = pd.DataFrame(label_1)\n",
    "#export file \n",
    "export_query = pd.DataFrame(label_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "757px",
    "left": "586px",
    "top": "630px",
    "width": "309px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
