{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import boto3\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sentencepiece\n",
    "from scipy.spatial import distance\n",
    "from json import JSONEncoder\n",
    "import sys\n",
    "# sys.path.append(\"/Users/dafirebanks/Projects/policy-data-analyzer/\")\n",
    "# sys.path.append(\"C:/Users/jordi/Documents/GitHub/policy-data-analyzer/\")\n",
    "import os\n",
    "os.chdir(\"../../../../\")\n",
    "from tasks.data_loading.src.utils import *\n",
    "from tasks.data_loading.src.s3_client_utils import *\n",
    "os.chdir(\"/home/propietari/Documents/GitHub/policy-data-analyzer/tasks/data_augmentation/notebooks/assisted_labeling/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_credentials_from_file(f_name):\n",
    "    with open(f_name, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "    \n",
    "    return creds[\"aws\"][\"id\"], creds[\"aws\"][\"secret\"]\n",
    "\n",
    "def aws_credentials(path, filename):\n",
    "    file = path + filename\n",
    "    with open(file, 'r') as dict:\n",
    "        key_dict = json.load(dict)\n",
    "    for key in key_dict:\n",
    "        KEY = key\n",
    "        SECRET = key_dict[key]\n",
    "    return KEY, SECRET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimized full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_for_language(language):\n",
    "    if \"Spanish\" in language:\n",
    "        return \"spanish\", \"Spanish_queries.xlsx\"\n",
    "    elif \"English\" in language:\n",
    "        return \"english\", \"English_queries.xlsx\"\n",
    "    else:\n",
    "        print(\"\\n BEWARE!! the values for language are \\\"English\\\" or \\\"Spanish\\\".\\n Please adjust and re-run.\")\n",
    "        sys.exit()\n",
    "    \n",
    "def aws_credentials(path, filename):\n",
    "    file = path + filename\n",
    "    with open(file, 'r') as dict:\n",
    "        key_dict = json.load(dict)\n",
    "    for key in key_dict:\n",
    "        KEY = key\n",
    "        SECRET = key_dict[key]\n",
    "    return KEY, SECRET\n",
    "\n",
    "def aws_credentials_from_file(f_name):\n",
    "    with open(f_name, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "    \n",
    "    return creds[\"aws\"][\"id\"], creds[\"aws\"][\"secret\"]\n",
    "\n",
    "def load_all_sentences(language, s3, bucket_name, init_doc, end_doc):\n",
    "    policy_dict = {}\n",
    "    sents_folder = f\"{language}_documents/sents\"\n",
    "    \n",
    "    for i, obj in enumerate(s3.Bucket(bucket_name).objects.all().filter(Prefix = sents_folder)):\n",
    "        if i < 2: #TO REMOVE\n",
    "            if not obj.key.endswith(\"/\") and init_doc <= i < end_doc:#and \"version\" in obj.key \n",
    "                print(obj.key)\n",
    "                serializedObject = obj.get()['Body'].read()\n",
    "                policy_dict = {**policy_dict, **json.loads(serializedObject)}\n",
    "                print(policy_dict)\n",
    "        else: #TO REMOVE\n",
    "            break\n",
    "            \n",
    "    return labeled_sentences_from_dataset(policy_dict)\n",
    "\n",
    "def save_results_as_separate_csv(results_dictionary, queries_dictionary, init_doc, results_limit, aws_id, aws_secret):\n",
    "    path = \"s3://wri-nlp-policy/english_documents/assisted_labeling\"\n",
    "    col_headers = [\"sentence_id\", \"similarity_score\", \"text\"]\n",
    "    for i, query in enumerate(results_dictionary.keys()):\n",
    "        filename = f\"{path}/query_{queries_dictionary[query]}_{i}_results_{init_doc}.csv\"\n",
    "        pd.DataFrame(results_dictionary[query], columns=col_headers).head(results_limit).to_csv(filename, storage_options={\"key\": aws_id, \"secret\": aws_secret})\n",
    "\n",
    "def labeled_sentences_from_dataset(dataset):\n",
    "    sentence_tags_dict = {}\n",
    "\n",
    "    for document in dataset.values():\n",
    "        print(document['sentences'])\n",
    "        sentence_tags_dict.update(document['sentences'])\n",
    "\n",
    "    return sentence_tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up AWS\n",
    "credentials_file = '/Users/dafirebanks/Documents/credentials.json'\n",
    "aws_id, aws_secret = aws_credentials_from_file(credentials_file)\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = aws_id,\n",
    "    aws_secret_access_key = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up AWS Jordi's way\n",
    "\n",
    "path = \"C:/Users/jordi/Documents/claus/\"\n",
    "path = \"/home/propietari/Documents/claus/\"\n",
    "filename = \"AWS_S3_keys_wri.json\"\n",
    "aws_id, aws_secret = aws_credentials(path, filename)\n",
    "region = 'us-east-1'\n",
    "bucket = 'wri-nlp-policy'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = aws_id,\n",
    "    aws_secret_access_key = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "language = \"spanish\"\n",
    "bucket_name = 'wri-nlp-policy'\n",
    "ori_sents_folder = f\"{language}_documents/sentences\"\n",
    "dest_sents_folder = f\"{language}_documents/sents/\"\n",
    "\n",
    "\n",
    "versioning_s3_objects(s3, bucket_name, ori_sents_folder, dest_sents_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spanish_documents/sents/000bd482c20f22b801193f1897ed1eca89318a53_sents\n",
      "{'/000bd482c20f22b801193f1897ed1eca89318a53': {'metadata': {'n_sentences': 6, 'language': 'spanish'}, 'sentences': ['SECRETARIA DE DESARROLLO URBANO Y OBRAS PUBLICAS DEL ESTADO DE QUERETARO DEPARTAMENTO DE CONCURSOS RESUMEN DE CONVOCATORIA 001 LICITACION PUBLICA NACIONAL De conformidad con la Ley de Obras Publicas y Servicios Relacionados con las Mismas del Sector Publico, se convoca a los interesados a participar en la licitacion publica nacional con numero 51005002-001-19 cuya Convocatoria contiene las bases de participacion disponible para consulta en Internet: [URL] o bien en: Francisco I. Madero No 72, Colonia Centro Historico, CP 76000, Santiago de Queretaro, Queretaro, telefono: (442)2-27-18-00, ext 2302 y 2304, los dias del 09 al 16 de mayo del 2019 de las 9:00 a 14:00 horas.', 'Referencia del expediente: 51005002-001-19 QUERETARO, QUERETARO, A 9 DE MAYO DE 2019.', 'SUBSECRETARIO DE DESARROLLO URBANO Y ADMINISTRACION DE OBRAS PUBLICAS CP JORGE LUIS PEREZ TREJO RUBRICA.', '(R- 481067) Descripcion de la licitacion MODERNIZACION DE LA CARRETERA ESTATAL 413 DE LA CUEVA A PRESA DE BRAVO (TRAMO 4, DEL KM 12+445 AL 14+300).', 'Ubicacion CARRETERA ESTATAL 413 DE LA CUEVA A PRESA DE BRAVO (TRAMO 4, DEL KM 12+445 AL 14+300).', 'Volumen de licitacion Se detalla en la Convocatoria Fecha de publicacion en CompraNet 09/05/19 Visita al lugar de los trabajos 16/05/19 09:00 horas Junta de aclaraciones 16/05/19 12:00 horas Presentacion y apertura de proposiciones 24/05/19 12:00 horas']}}\n",
      "['SECRETARIA DE DESARROLLO URBANO Y OBRAS PUBLICAS DEL ESTADO DE QUERETARO DEPARTAMENTO DE CONCURSOS RESUMEN DE CONVOCATORIA 001 LICITACION PUBLICA NACIONAL De conformidad con la Ley de Obras Publicas y Servicios Relacionados con las Mismas del Sector Publico, se convoca a los interesados a participar en la licitacion publica nacional con numero 51005002-001-19 cuya Convocatoria contiene las bases de participacion disponible para consulta en Internet: [URL] o bien en: Francisco I. Madero No 72, Colonia Centro Historico, CP 76000, Santiago de Queretaro, Queretaro, telefono: (442)2-27-18-00, ext 2302 y 2304, los dias del 09 al 16 de mayo del 2019 de las 9:00 a 14:00 horas.', 'Referencia del expediente: 51005002-001-19 QUERETARO, QUERETARO, A 9 DE MAYO DE 2019.', 'SUBSECRETARIO DE DESARROLLO URBANO Y ADMINISTRACION DE OBRAS PUBLICAS CP JORGE LUIS PEREZ TREJO RUBRICA.', '(R- 481067) Descripcion de la licitacion MODERNIZACION DE LA CARRETERA ESTATAL 413 DE LA CUEVA A PRESA DE BRAVO (TRAMO 4, DEL KM 12+445 AL 14+300).', 'Ubicacion CARRETERA ESTATAL 413 DE LA CUEVA A PRESA DE BRAVO (TRAMO 4, DEL KM 12+445 AL 14+300).', 'Volumen de licitacion Se detalla en la Convocatoria Fecha de publicacion en CompraNet 09/05/19 Visita al lugar de los trabajos 16/05/19 09:00 horas Junta de aclaraciones 16/05/19 12:00 horas Presentacion y apertura de proposiciones 24/05/19 12:00 horas']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 677; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e31b995ec4aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Get all sentence documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_all_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_at_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_at_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Define queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a6479c2da028>\u001b[0m in \u001b[0;36mload_all_sentences\u001b[0;34m(language, s3, bucket_name, init_doc, end_doc)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlabeled_sentences_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_results_as_separate_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maws_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maws_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a6479c2da028>\u001b[0m in \u001b[0;36mlabeled_sentences_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0msentence_tags_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentence_tags_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dictionary update sequence element #0 has length 677; 2 is required"
     ]
    }
   ],
   "source": [
    "# Define params\n",
    "init_at_doc = 0\n",
    "end_at_doc = 1100\n",
    "\n",
    "similarity_threshold = 0\n",
    "search_results_limit = 500\n",
    "\n",
    "bucket_name = 'wri-nlp-policy'\n",
    "\n",
    "language, filename = adjust_for_language(\"Spanish\")\n",
    "\n",
    "transformer_name = 'xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "model = SentenceTransformer(transformer_name)\n",
    "\n",
    "\n",
    "# Get all sentence documents\n",
    "\n",
    "sentences = load_all_sentences(language, s3, bucket_name, init_at_doc, end_at_doc)\n",
    "\n",
    "# Define queries\n",
    "path = \"../../input/\"\n",
    "file = path + filename\n",
    "df = pd.read_excel(file, engine='openpyxl', sheet_name = \"Hoja1\", usecols = \"A:C\")\n",
    "\n",
    "queries = {}\n",
    "for index, row in df.iterrows():\n",
    "    queries[row['Query sentence']] = row['Policy instrument']\n",
    "\n",
    "\n",
    "\n",
    "# Calculate and store query embeddings\n",
    "query_embeddings = dict(zip(queries, [model.encode(query.lower(), show_progress_bar=False) for query in queries]))\n",
    "\n",
    "# For each sentence, calculate its embedding, and store the similarity\n",
    "query_similarities = defaultdict(list)\n",
    "\n",
    "i = 0\n",
    "for sentence_id, sentence in sentences.items():\n",
    "    sentence_embedding = model.encode(sentence['text'].lower(), show_progress_bar=False)\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "                print(i)\n",
    "    \n",
    "    for query_text, query_embedding in query_embeddings.items():\n",
    "        score = round(1 - distance.cosine(sentence_embedding, query_embedding), 4)\n",
    "        if score > similarity_threshold:\n",
    "            query_similarities[query_text].append([sentence_id, score, sentences[sentence_id]['text']])\n",
    "            \n",
    "# Sort results by similarity score\n",
    "for query in query_similarities:\n",
    "    query_similarities[query] = sorted(query_similarities[query], key = lambda x : x[1], reverse=True)\n",
    "    \n",
    "# Store results\n",
    "save_results_as_separate_csv(query_similarities, queries, init_at_doc, search_results_limit, aws_id, aws_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
