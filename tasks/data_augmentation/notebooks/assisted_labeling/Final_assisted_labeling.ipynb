{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import boto3\n",
    "import copy\n",
    "import csv\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import sentencepiece\n",
    "from scipy.spatial import distance\n",
    "from json import JSONEncoder\n",
    "import sys\n",
    "sys.path.append(\"/Users/dafirebanks/Projects/policy-data-analyzer/\")\n",
    "from tasks.data_loading.src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_credentials_from_file(f_name):\n",
    "    with open(f_name, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "    \n",
    "    return creds[\"aws\"][\"id\"], creds[\"aws\"][\"secret\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_file = '/Users/dafirebanks/Documents/credentials.json'\n",
    "aws_id, aws_secret = aws_credentials_from_file(credentials_file)\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = aws_id,\n",
    "    aws_secret_access_key = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load sentences from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_documents/sentences/\n",
      "english_documents/sentences/0002304f1f671ea916ae0a1f784484eb4874ceaa_sents.json\n",
      "english_documents/sentences/0002a815db93aaba959b04dbeaa17e87f8585734_sents.json\n",
      "english_documents/sentences/0005bd689cd9cc6ab99194b7bb5aed32fe0bbb47_sents.json\n",
      "english_documents/sentences/0007adab1d0acfc53274596b784bd85cdfbe502e_sents.json\n",
      "english_documents/sentences/000cc1ce72ee0d48230e5da68fe69c11ac428cfa_sents.json\n",
      "english_documents/sentences/0010b6d44c3d296ba1bfaa24c718ab16d9ae017b_sents.json\n",
      "english_documents/sentences/00175bb6c2a2b3368a9cfdfbd895ecac48920ccb_sents.json\n",
      "english_documents/sentences/00198951ca0fcb94619e41d5256c31e8ce57d70f_sents.json\n",
      "english_documents/sentences/0020eabfcbd08554df1bd58438dcfda7f454d8c7_sents.json\n",
      "english_documents/sentences/00274763715091c9c1186a4d13f61ef5b773b923_sents.json\n"
     ]
    }
   ],
   "source": [
    "policy_dict = {}\n",
    "objs = []\n",
    "language = \"english\"\n",
    "bucket_name = 'wri-nlp-policy'\n",
    "sents_folder = f\"{language}_documents/sentences\"\n",
    "\n",
    "for i, obj in enumerate(s3.Bucket(bucket_name).objects.all().filter(Prefix=sents_folder)):\n",
    "#     print(obj.key)\n",
    "    if not obj.key.endswith(\"/\"):\n",
    "        serializedObject = obj.get()['Body'].read()\n",
    "        policy_dict = {**policy_dict, **json.loads(serializedObject)}\n",
    "        \n",
    "        # Uncomment this for testing purposes\n",
    "#         if i == 10:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0002304f1f671ea916ae0a1f784484eb4874ceaa', '0002a815db93aaba959b04dbeaa17e87f8585734', '0005bd689cd9cc6ab99194b7bb5aed32fe0bbb47', '0007adab1d0acfc53274596b784bd85cdfbe502e', '000cc1ce72ee0d48230e5da68fe69c11ac428cfa', '0010b6d44c3d296ba1bfaa24c718ab16d9ae017b', '00175bb6c2a2b3368a9cfdfbd895ecac48920ccb', '00198951ca0fcb94619e41d5256c31e8ce57d70f', '0020eabfcbd08554df1bd58438dcfda7f454d8c7', '00274763715091c9c1186a4d13f61ef5b773b923'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Before, they were separated by document, now we join all sentences into a single JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeled_sentences_from_dataset(dataset):\n",
    "    sentence_tags_dict = {}\n",
    "\n",
    "    for document in dataset.values():\n",
    "        sentence_tags_dict.update(document['sentences'])\n",
    "\n",
    "    return sentence_tags_dict\n",
    "\n",
    "sentences = labeled_sentences_from_dataset(policy_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL: 2.2 Now, we'll randomly sample 10 sentences for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence_ids = random.sample(list(sentences), 10)\n",
    "sample_sentences = {}\n",
    "for s_id in sample_sentence_ids:\n",
    "    sample_sentences.update({s_id: sentences[s_id]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_embeddings(model, sentences_dict):\n",
    "    embeddings = {}\n",
    "    for sentence_id, sentence_map in sentences_dict.items():\n",
    "        embeddings[sentence_id] = model.encode(sentence_map['text'].lower(), show_progress_bar=False)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for creating the embeddings: 7.1771 seconds\n"
     ]
    }
   ],
   "source": [
    "Ti = time.perf_counter()\n",
    "\n",
    "# We will use only one transformer to compute embeddings\n",
    "transformer_name = 'xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "model = SentenceTransformer(transformer_name)\n",
    "embs = create_sentence_embeddings(model, sample_sentences)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"Time taken for creating the embeddings: {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity_search(model, queries, sentence_embeddings, sentences, similarity_limit, results_limit, filename):\n",
    "    results = {}\n",
    "    for query in queries:\n",
    "        Ti = time.perf_counter()\n",
    "        similarities = get_distance(model, sentence_embeddings, sentences, query, similarity_limit)\n",
    "        results[query] = similarities[0:results_limit]#results[transformer][query] = similarities[0:results_limit]\n",
    "        Tf = time.perf_counter()\n",
    "        print(f\"similarity search for query {query} has been done in {Tf - Ti:0.4f} seconds\")\n",
    "\n",
    "    path = \"../../output/\"\n",
    "    filename = filename + \".json\"\n",
    "    file = path + filename\n",
    "    with open(file, 'w') as fp:\n",
    "        json.dump(results, fp, indent=4)\n",
    "    return results\n",
    "\n",
    "def get_distance(model, sentence_emb, sentences_dict, query, similarity_treshold):\n",
    "    query_embedding = model.encode(query.lower(), show_progress_bar=False)\n",
    "    highlights = []\n",
    "    for sentence in sentences_dict:\n",
    "        sentence_embedding = sentence_emb[sentence]\n",
    "        score = round(1 - distance.cosine(sentence_embedding, query_embedding), 4)\n",
    "        if score > similarity_treshold:\n",
    "            highlights.append([sentence, score, sentences_dict[sentence]['text']])\n",
    "    highlights = sorted(highlights, key = lambda x : x[1], reverse = True)\n",
    "    return highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity search for query We will offer 10 dollars to each farmer that plants 1 seed per hour has been done in 0.0949 seconds\n",
      "similarity search for query A number of instrumental equipment for farming will be provided to every farmer in need has been done in 0.0960 seconds\n"
     ]
    }
   ],
   "source": [
    "transformer_name ='xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "model = SentenceTransformer(transformer_name)\n",
    "similarity_threshold = 0.2\n",
    "search_results_limit = 1000\n",
    "queries = [\"We will offer 10 dollars to each farmer that plants 1 seed per hour\", \"A number of instrumental equipment for farming will be provided to every farmer in need\"]\n",
    "out_fname = \"test_similarity_file\"\n",
    "\n",
    "results_dict = sentence_similarity_search(model, queries, embs, sample_sentences, similarity_threshold, search_results_limit, out_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'We will offer 10 dollars to each farmer that plants 1 seed per hour': [['00274763715091c9c1186a4d13f61ef5b773b923_sent_4310',\n",
       "   0.2974,\n",
       "   '(i) In units Oahu--Coastal--Unit 9, Oahu--Coastal--Unit 11, and Oahu--Coastal--Unit 12, the physical and biological features of critical habitat are: (A) Elevation: Less than 980 ft (300 m).'],\n",
       "  ['00274763715091c9c1186a4d13f61ef5b773b923_sent_4239',\n",
       "   0.2152,\n",
       "   '(B) Annual precipitation: 50 to 75 in (130 to 190 cm).'],\n",
       "  ['00274763715091c9c1186a4d13f61ef5b773b923_sent_1240',\n",
       "   0.2113,\n",
       "   'The following activities could potentially result in a violation of section 9 of the Act; this list is not comprehensive: (1) Unauthorized collecting, handling, possessing, selling, delivering, carrying, or transporting of the species, including import or export across State lines and international boundaries, except for properly documented antique specimens of these taxa at least 100 years old, as defined by section 10(h)(1) of the Act.']],\n",
       " 'A number of instrumental equipment for farming will be provided to every farmer in need': [['00274763715091c9c1186a4d13f61ef5b773b923_sent_1301',\n",
       "   0.2417,\n",
       "   'Section 7(a)(2) requires consultation on Federal actions that may affect critical habitat.'],\n",
       "  ['00274763715091c9c1186a4d13f61ef5b773b923_sent_4837',\n",
       "   0.2228,\n",
       "   '(C) Substrate: Shallow soils, little to no herbaceous layer.']]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Output results in CSV format to S3 for labeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_as_separate_csv(results_dictionary, aws_id, aws_secret):\n",
    "    path = \"s3://wri-nlp-policy/english_documents/assisted_labeling\"\n",
    "    col_headers = [\"sentence_id\", \"similarity_score\", \"text\"]\n",
    "    for i, query in enumerate(results_dict.keys()):\n",
    "        filename = f\"{path}/query_{i}_results.csv\"\n",
    "        pd.DataFrame(results_dict[query], columns=col_headers).to_csv(filename, storage_options={\"key\": aws_id, \"secret\": aws_secret})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_as_separate_csv(results_dict, aws_id, aws_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Optimized full loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_credentials_from_file(f_name):\n",
    "    with open(f_name, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "    \n",
    "    return creds[\"aws\"][\"id\"], creds[\"aws\"][\"secret\"]\n",
    "\n",
    "def load_all_sentences(language, s3, bucket_name):\n",
    "    policy_dict = {}\n",
    "    sents_folder = f\"{language}_documents/sentences\"\n",
    "\n",
    "    for i, obj in enumerate(s3.Bucket(bucket_name).objects.all().filter(Prefix=sents_folder)):\n",
    "        if not obj.key.endswith(\"/\"):\n",
    "            serializedObject = obj.get()['Body'].read()\n",
    "            policy_dict = {**policy_dict, **json.loads(serializedObject)}\n",
    "            \n",
    "    return labeled_sentences_from_dataset(policy_dict)\n",
    "\n",
    "def save_results_as_separate_csv(results_dictionary, aws_id, aws_secret):\n",
    "    path = \"s3://wri-nlp-policy/english_documents/assisted_labeling\"\n",
    "    col_headers = [\"sentence_id\", \"similarity_score\", \"text\"]\n",
    "    for i, query in enumerate(results_dict.keys()):\n",
    "        filename = f\"{path}/query_{i}_results.csv\"\n",
    "        pd.DataFrame(results_dict[query], columns=col_headers).to_csv(filename, storage_options={\"key\": aws_id, \"secret\": aws_secret})\n",
    "\n",
    "def labeled_sentences_from_dataset(dataset):\n",
    "    sentence_tags_dict = {}\n",
    "\n",
    "    for document in dataset.values():\n",
    "        sentence_tags_dict.update(document['sentences'])\n",
    "\n",
    "    return sentence_tags_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up AWS\n",
    "credentials_file = '/Users/dafirebanks/Documents/credentials.json'\n",
    "aws_id, aws_secret = aws_credentials_from_file(credentials_file)\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = aws_id,\n",
    "    aws_secret_access_key = aws_secret\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all sentence documents\n",
    "language = \"english\"\n",
    "bucket_name = 'wri-nlp-policy'\n",
    "sentences = load_all_sentences(language, s3, bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define queries\n",
    "queries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Define params\n",
    "transformer_name = 'xlm-r-bert-base-nli-stsb-mean-tokens'\n",
    "model = SentenceTransformer(transformer_name)\n",
    "\n",
    "similarity_threshold = 0.2\n",
    "search_results_limit = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate and store query embeddings\n",
    "query_embeddings = dict(zip(queries, [model.encode(query.lower(), show_progress_bar=False) for query in queries]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. For each sentence, calculate its embedding, and store the similarity\n",
    "query_similarities = defaultdict(list)\n",
    "\n",
    "for sentence_id, sentence in sentences.items():\n",
    "    sentence_embedding = model.encode(sentence['text'].lower(), show_progress_bar=False)\n",
    "    \n",
    "    for query_text, query_embedding in query_embeddings.items():\n",
    "        score = round(1 - distance.cosine(sentence_embedding, query_embedding), 4)\n",
    "        if score > similarity_threshold:\n",
    "            query_similarities[query_text].append([sentence_id, score, sentences[sentence_id]['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sort results by similarity score\n",
    "for query in query_similarities:\n",
    "    query_similarities[query] = sorted(query_similarities[query], key = lambda x : x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Store results\n",
    "save_results_as_separate_csv(query_similarities, aws_id, aws_secret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wri-env",
   "language": "python",
   "name": "wri-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
