{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "GoogleColabFineTuneEmbeddingModel.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "P4l8oGqlvDXs",
        "8KxOPUcovDXu"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk0XWLfMvDXU"
      },
      "source": [
        "# Note\n",
        "This notebook can be run on google colab for improved performance. The code changes necessary for running on this system are commented over the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edvxyOpnvDXj"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml7Gd4VgxW1d"
      },
      "source": [
        "!pip install scprep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vfID0vcx6pm"
      },
      "source": [
        "!pip install spacy==2.3.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkzjENKB0is4"
      },
      "source": [
        "! python -m spacy download es_core_news_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i1BZKUvvDXk"
      },
      "source": [
        "! pip install sentence_transformers==0.4.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4UNMkgIvDXl",
        "outputId": "8c6d88ed-34d3-46e8-827f-9a2f6c7026b9"
      },
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sentence_transformers import SentencesDataset, SentenceTransformer, InputExample, losses\n",
        "from sentence_transformers.evaluation import LabelAccuracyEvaluator\n",
        "from torch import nn, Tensor\n",
        "from typing import Iterable, Dict\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5qTIDgpxm3L"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def load_file(file_name):\n",
        "    with open(file_name, \"r\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def labeled_sentences_from_dataset(dataset):\n",
        "    sentence_tags_dict = {}\n",
        "\n",
        "    for document in dataset.values():\n",
        "        for section in document.values():\n",
        "            sentence_tags_dict.update(section['sentences'])\n",
        "\n",
        "    return sentence_tags_dict\n",
        "\n",
        "\n",
        "def numeric_labels_from_dataset(dataset):\n",
        "    \"\"\"\n",
        "    TEMPORARY: We're getting the set of unique labels from the data, but in reality we should standardize this and define it in a separate env file.\n",
        "    \"\"\"\n",
        "    dataset_labels = labels_from_dataset(dataset)\n",
        "    label_names = [label for label in set(dataset_labels)]\n",
        "    label_names.sort()\n",
        "    label_map = dict(zip(label_names, range(len(label_names))))\n",
        "    num_dataset_labels = [label_map[label] for label in dataset_labels]\n",
        "    return num_dataset_labels\n",
        "\n",
        "\n",
        "def labels2numeric(labels, label_names):\n",
        "    label_map = dict(zip(label_names, range(len(label_names))))\n",
        "    num_dataset_labels = [label_map[label] for label in labels]\n",
        "    return num_dataset_labels\n",
        "\n",
        "\n",
        "def unique_labels(all_labels):\n",
        "    return list(set(all_labels))\n",
        "\n",
        "\n",
        "def filter_out_labeled_sents(sents, labels_to_filter):\n",
        "    return [sent for sent in sents.values() if sent['labels'][0] not in labels_to_filter]\n",
        "\n",
        "\n",
        "def sort_model_preds(dataset, model_preds):\n",
        "    \"\"\"\n",
        "    Sorts the model predictions in the order that the input dataset is in.\n",
        "    \"\"\"\n",
        "    # Dictionaries are insertion ordered since Python 3.6+,\n",
        "    ordered_preds = {}\n",
        "\n",
        "    for sentence_id in dataset:\n",
        "        ordered_preds[sentence_id] = model_preds[sentence_id]\n",
        "\n",
        "    return ordered_preds\n",
        "\n",
        "\n",
        "def sentences_from_dataset(dataset):\n",
        "    sentences = []\n",
        "\n",
        "    for document in dataset.values():\n",
        "        for section in document.values():\n",
        "            for sentence in section['sentences'].values():\n",
        "                sentences.append(sentence['text'])\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def labels_from_dataset(dataset):\n",
        "    labels = []\n",
        "\n",
        "    for document in dataset.values():\n",
        "        for section in document.values():\n",
        "            for sentence in section['sentences'].values():\n",
        "                labels.append(sentence['labels'][0])\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "def country_labeled_sentences(excel_map):\n",
        "    result = {}\n",
        "    sent_num = 0\n",
        "\n",
        "    for country, dataframe in excel_map.items():\n",
        "\n",
        "        new_sents_col = dataframe[\"Sentence\"].dropna()\n",
        "        new_labels_col = dataframe[\"Primary Instrument\"].dropna()\n",
        "\n",
        "        sentences = list(new_sents_col.apply(lambda x: x.replace(\"\\n\", \"\").strip()))\n",
        "        label_col = new_labels_col.apply(lambda x: x.replace(\"(PES)\", \"\").replace(\"(Bond)\", \"\").strip())\n",
        "        labels = [[string.strip() for string in label.split(\", \")][0] for label in label_col]\n",
        "        result[country] = {}\n",
        "\n",
        "        for sent, label in zip(sentences, labels):\n",
        "            if sent_num not in result[country]:\n",
        "                result[country][sent_num] = {\"text\": sent, \"labels\": [label]}\n",
        "            else:\n",
        "                result[country][sent_num][\"text\"] = sent\n",
        "                result[country][sent_num][\"labels\"] = [label]\n",
        "\n",
        "            sent_num += 1\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def labeled_sentences_from_excel(excel_map):\n",
        "    country2labeledsents = country_labeled_sentences(excel_map)\n",
        "    labeled_sents = dict()\n",
        "    for sents in country2labeledsents.values():\n",
        "        labeled_sents.update(sents)\n",
        "\n",
        "    return labeled_sents\n",
        "\n",
        "\n",
        "def sentences_from_model_output(model_preds):\n",
        "    return [preds[\"text\"] for preds in model_preds.values()]\n",
        "\n",
        "\n",
        "def labels_from_model_output(model_preds):\n",
        "    return [preds[\"labels\"][0] for preds in model_preds.values()]\n",
        "\n",
        "\n",
        "def get_counts_per_label(y_true, n_classes):\n",
        "    \"\"\"\n",
        "    Return a map of {label: number of data points with that label} for the given list of labels\n",
        "\n",
        "    Parameters:\n",
        "        - y_true: (integer) a list of labels\n",
        "        - n_classes: (integer) the number of classes\n",
        "    \"\"\"\n",
        "    label_counts = [0] * n_classes\n",
        "    for label in y_true:\n",
        "        label_counts[label] += 1\n",
        "    return label_counts\n",
        "\n",
        "\n",
        "def merge_labels(all_labels, labels_to_merge):\n",
        "    return [f\"{labels_to_merge[0]} & {labels_to_merge[1]}\" if label in labels_to_merge else label for label in all_labels]\n",
        "\n",
        "\n",
        "def plot_data_distribution(data, label_names, normalize=True):\n",
        "    weights = np.array(get_counts_per_label(data, len(label_names)))\n",
        "    if normalize:\n",
        "        weights = weights / sum(weights)\n",
        "\n",
        "    plt.bar(label_names, weights)\n",
        "    plt.xticks(label_names, rotation=90)\n",
        "    plt.title(\"Data Distribution\")\n",
        "    plt.xlabel(\"Label\")\n",
        "    plt.ylabel(\"Percentage of label in data\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Label counts:\")\n",
        "    print(dict(zip(label_names, weights)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8AivuobvvAR"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, f1_score, accuracy_score, precision_score, \\\n",
        "    recall_score, average_precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from itertools import cycle\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "import sys\n",
        "\n",
        "sys.path.append(\"../../\")\n",
        "\n",
        "METRICS = [\"Precision\", \"Recall (Sensitivity)\", \"True negative rate (Specificity)\", \"F1-score\"]\n",
        "\n",
        "\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, label_names, output_path=\"../output/\", y_true=None, y_pred=None):\n",
        "        self.label_names = label_names\n",
        "        self.df_indices = label_names + [\"Macro avg\", \"Weighted avg\"]\n",
        "        self.output_path = output_path\n",
        "        self.n_classes = len(label_names)\n",
        "\n",
        "        if y_true is not None and y_pred is not None:\n",
        "            self.update(y_true, y_pred)\n",
        "\n",
        "    def update(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Given a set of true labels and model predictions, calculate and store the following metrics:\n",
        "            - Confusion matrix\n",
        "            - FP: Number of false positives\n",
        "            - FN: Number of false negatives\n",
        "            - TP: Number of true positives\n",
        "            - TN: Number of true negatives\n",
        "            - Recall\n",
        "            - Specificity\n",
        "            - Precision\n",
        "            - F1-score\n",
        "            - Accuracy\n",
        "            - FDR: False discovery rate\n",
        "            - NPV: Negative predictive value\n",
        "            - FPR: False positive rate\n",
        "            - False negative rate\n",
        "        \"\"\"\n",
        "        # Ignore division by 0 errors\n",
        "        settings = np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "        # ---- Set up raw components ----\n",
        "        self.cm = confusion_matrix(y_true, y_pred)\n",
        "        self.FP = self.cm.sum(axis=0) - np.diag(self.cm)\n",
        "        self.FN = self.cm.sum(axis=1) - np.diag(self.cm)\n",
        "        self.TP = np.diag(self.cm)\n",
        "        self.TN = self.cm.sum() - (self.FP + self.FN + self.TP)\n",
        "\n",
        "        # ---- Useful metrics at the class level ----\n",
        "        self.recall = self.TP / (self.TP + self.FN)\n",
        "        self.specificity = self.TN / (self.TN + self.FP)\n",
        "        self.precision = self.TP / (self.TP + self.FP)\n",
        "        self.f1 = (2 * self.precision * self.recall) / (self.precision + self.recall)\n",
        "\n",
        "        # ---- Useful metrics across all classes ----\n",
        "        self.avg_precision = np.array(\n",
        "            [precision_score(y_true, y_pred, average='macro'), precision_score(y_true, y_pred, average='weighted')])\n",
        "        self.avg_recall = np.array(\n",
        "            [recall_score(y_true, y_pred, average='macro'), recall_score(y_true, y_pred, average='weighted')])\n",
        "        self.avg_specificity = np.array([np.mean(self.specificity), self.weighted_avg(self.specificity, y_true, )])\n",
        "        self.avg_f1 = np.array(\n",
        "            [f1_score(y_true, y_pred, average='macro'), f1_score(y_true, y_pred, average='weighted')])\n",
        "        self.accuracy = accuracy_score(y_true, y_pred)\n",
        "        self.acc = np.array([\"-----\", \"-----\", \"-----\", self.accuracy])\n",
        "\n",
        "        # ---- Extra metrics at the class level ----\n",
        "        self.FDR = self.FP / (self.TP + self.FP)  # False discovery rate\n",
        "        self.NPV = self.TN / (self.TN + self.FN)  # Negative predictive value\n",
        "        self.FPR = self.FP / (self.FP + self.TN)  # Fall out or false positive rate\n",
        "        self.FNR = self.FN / (self.TP + self.FN)  # False negative rate\n",
        "\n",
        "\n",
        "    def evaluate(self, y_true, y_pred, plot_cm=False, normalize=False, exp_name=None):\n",
        "        \"\"\"\n",
        "        Given a set of true labels and model predictions, runs a series of selected evaluation metrics:\n",
        "            - Precision\n",
        "            - Recall (Sensitivity)\n",
        "            - Accuracy\n",
        "            - Specificity\n",
        "            - Confusion matrix\n",
        "            - Precision-Recall curve\n",
        "\n",
        "        Parameters:\n",
        "            `plot_cm`: (boolean) Plot confusion matrix\n",
        "            `plot_prc`: (boolean) Plot precision-recall curve (averaged for all classes)\n",
        "            `plot_prc_multi`: (boolean) Plot the multi-class version of the precision-recall curve (`plot_prc` MUST be `True` if this is set to `True`)\n",
        "            `normalize`: (boolean) Normalize the confusion matrix content\n",
        "            `store`: (boolean) Store the plots and the results dataframe. If this is set to `True`, then `exp_name` MUST have a value and it can't be None. The files will be stored in the `evaluate_model/output/` folder.\n",
        "            `exp_name`: (str) The name of the model or the experiment, useful if we will want to store files (e.g `test_BETO_1`).\n",
        "        \"\"\"\n",
        "\n",
        "        self.update(y_true, y_pred)\n",
        "\n",
        "        data = np.stack((self.precision, self.recall, self.specificity, self.f1)).T\n",
        "        avgs = np.array([self.avg_precision, self.avg_recall, self.avg_specificity, self.avg_f1]).T\n",
        "        data_with_avgs = np.concatenate((data, avgs))\n",
        "\n",
        "        metrics_df = pd.DataFrame(data_with_avgs, index=self.df_indices, columns=METRICS)\n",
        "        metrics_df = metrics_df.applymap(lambda x: round(x, 2))\n",
        "        metrics_df.loc['Accuracy'] = self.acc\n",
        "\n",
        "        line = pd.DataFrame(dict(zip(METRICS, [\"-----\"] * len(METRICS))), index=[\"-----\"])\n",
        "        metrics_df = pd.concat([metrics_df.iloc[:self.n_classes], line, metrics_df.iloc[self.n_classes:]])\n",
        "        self.metrics_df = metrics_df.fillna(0)\n",
        "\n",
        "        if plot_cm:\n",
        "            self.plot_confusion_matrix(color_map=\"Blues\",\n",
        "                                       normalize=normalize,\n",
        "                                       exp_name=f\"{self.output_path}{exp_name}\")\n",
        "\n",
        "        if exp_name:\n",
        "            fname = f\"{self.output_path + exp_name}_results.csv\"\n",
        "            self.metrics_df.to_csv(fname)\n",
        "            print(f\"Stored results: {fname}\")\n",
        "\n",
        "        return self.metrics_df\n",
        "\n",
        "    def get_counts_per_label(self, y_true):\n",
        "        \"\"\"\n",
        "        Return a map of {label: number of data points with that label} for the given list of labels\n",
        "\n",
        "        Parameters:\n",
        "            - y_true: a list of labels (integers)\n",
        "        \"\"\"\n",
        "        label_counts = [0] * self.n_classes\n",
        "        for label in y_true:\n",
        "            label_counts[label] += 1\n",
        "        return label_counts\n",
        "\n",
        "    def weighted_avg(self, metric_array, y_true):\n",
        "        \"\"\"\n",
        "        Given a numpy array of a particular metric for all classes (i.e precision for all classes),\n",
        "        return a weighted average of the metric, where the weights are the number of data points that\n",
        "        have a given label.\n",
        "\n",
        "        Parameters:\n",
        "            - metric array: a 1D-numpy array of floats representing metrics\n",
        "            - y_true: a list of labels (integers)\n",
        "        \"\"\"\n",
        "        weights = self.get_counts_per_label(y_true)\n",
        "        weighted_metrics = sum(metric_array * weights)\n",
        "        return weighted_metrics / len(y_true)\n",
        "\n",
        "\n",
        "    def plot_confusion_matrix(self, title='Confusion matrix',\n",
        "                              color_map=None,\n",
        "                              normalize=True,\n",
        "                              exp_name=None):\n",
        "        \"\"\"\n",
        "        Adapted from: https://stackoverflow.com/questions/19233771/sklearn-plot-confusion-matrix-with-labels\n",
        "        \"\"\"\n",
        "        if color_map is None:\n",
        "            color_map = plt.get_cmap('Blues')\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(self.cm, interpolation='nearest', cmap=color_map)\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.colorbar()\n",
        "        plt.style.use('seaborn-white')\n",
        "\n",
        "        if self.label_names:\n",
        "            tick_marks = np.arange(len(self.label_names))\n",
        "            plt.xticks(tick_marks, self.label_names, rotation=45)\n",
        "            plt.yticks(tick_marks, self.label_names)\n",
        "\n",
        "        if normalize:\n",
        "            self.cm = self.cm.astype('float') / self.cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        thresh = self.cm.max() / 1.5 if normalize else self.cm.max() / 2\n",
        "        for i, j in itertools.product(range(self.cm.shape[0]), range(self.cm.shape[1])):\n",
        "            if normalize:\n",
        "                plt.text(j, i, \"{:0.2f}\".format(self.cm[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         color=\"white\" if self.cm[i, j] > thresh else \"black\")\n",
        "            else:\n",
        "                plt.text(j, i, \"{:,}\".format(self.cm[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         color=\"white\" if self.cm[i, j] > thresh else \"black\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.ylabel('True label')\n",
        "\n",
        "        if exp_name:\n",
        "            fname = f\"{exp_name}_cm.png\"\n",
        "            plt.savefig(fname)\n",
        "            print(f\"Stored confusion matrix: {fname}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def plot_precision_recall_curve(self, y_true, y_pred, bin_class=False, all_classes=False,\n",
        "                                    exp_name=None):\n",
        "        \"\"\"\n",
        "        Plots the precision-recall curve for either a binary or multi-class classification model.\n",
        "\n",
        "        Parameters:\n",
        "            y_true: (np.array[int]) The true labels for the dataset\n",
        "            y_pred: If binary_class is True, y_pred should be a numpy array of floats holding the prediction probabilities,\n",
        "                else y_pred should be a numpy array of ints holding the predictions themselves\n",
        "            bin_class: (boolean) Whether we should plot the curve for a binary classification setting\n",
        "            all_classes: (boolean) In a multi-class classification problem, whether we should plot the curve for all classes or just the average\n",
        "            store: (boolean) Whether we want to store this plot or not\n",
        "            exp_name: (str) The name of the experiment, used to name the file to store the plot. Requires store=True\n",
        "        \"\"\"\n",
        "\n",
        "        if bin_class:\n",
        "            if not isinstance(y_pred.flat[0], np.floating):\n",
        "                print(\"Error: Array of predictions should contain probabilities [0.3, 0.75] instead of labels [0, 1] for binary classification problems.\")\n",
        "                return\n",
        "\n",
        "            random_pred_precision = y_true.mean()\n",
        "\n",
        "            precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
        "            average_precision = average_precision_score(y_true, y_pred)\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot([0, 1], [random_pred_precision, random_pred_precision], linestyle='--', label='Random Prediction')\n",
        "            plt.step(recall, precision, where='post')\n",
        "\n",
        "            plt.xlabel('Recall')\n",
        "            plt.ylabel('Precision')\n",
        "\n",
        "            plt.ylim([0.0, 1.05])\n",
        "            plt.xlim([0.0, 1.0])\n",
        "            plt.title('Precision-Recall Curve. Avg Precision=' + str(round(average_precision, 2)))\n",
        "\n",
        "            if exp_name:\n",
        "                fname = f\"{self.output_path}{exp_name}_prc.png\"\n",
        "                plt.savefig(fname)\n",
        "                print(f\"Stored Precision-Recall Curve: {fname}\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        else:\n",
        "            y_true_bin = label_binarize(y_true, classes=range(self.n_classes))\n",
        "            y_pred_bin = label_binarize(y_pred, classes=range(self.n_classes))\n",
        "\n",
        "            precision = dict()\n",
        "            recall = dict()\n",
        "            average_precision = dict()\n",
        "\n",
        "            for i in range(self.n_classes):\n",
        "                precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i],\n",
        "                                                                    y_pred_bin[:, i])\n",
        "                average_precision[i] = average_precision_score(y_true_bin[:, i], y_pred_bin[:, i])\n",
        "\n",
        "            # A \"micro-average\": quantifying score on all classes jointly\n",
        "            precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_true_bin.ravel(),\n",
        "                                                                            y_pred_bin.ravel())\n",
        "            average_precision[\"micro\"] = average_precision_score(y_true_bin, y_pred_bin,\n",
        "                                                                 average=\"micro\")\n",
        "\n",
        "            random_pred_precision = y_true_bin.mean()\n",
        "\n",
        "            if all_classes:\n",
        "\n",
        "                # Setup plot details\n",
        "                colors = cycle(list(mcolors.TABLEAU_COLORS.keys()))\n",
        "                plt.figure(figsize=(7, 8))\n",
        "                plt.style.use('seaborn-white')\n",
        "\n",
        "                # Plot f1 score lines\n",
        "                f_scores = np.linspace(0.2, 0.8, num=4)\n",
        "                lines = []\n",
        "                labels = []\n",
        "                for f_score in f_scores:\n",
        "                    x = np.linspace(0.01, 1)\n",
        "                    y = f_score * x / (2 * x - f_score)\n",
        "                    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
        "                    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
        "\n",
        "                # Plot precision-recall lines\n",
        "                lines.append(l)\n",
        "                labels.append('iso-f1 curves')\n",
        "                l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
        "                lines.append(l)\n",
        "                labels.append('Micro-average Precision-Recall (area = {0:0.2f})'\n",
        "                              ''.format(average_precision[\"micro\"]))\n",
        "\n",
        "                for i, color in zip(range(self.n_classes), colors):\n",
        "                    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
        "                    lines.append(l)\n",
        "                    labels.append('Precision-Recall for class {0} (area = {1:0.2f})'\n",
        "                                  ''.format(i, average_precision[i]))\n",
        "\n",
        "                rand_l, = plt.plot([0, 1], [random_pred_precision, random_pred_precision], linestyle='--')\n",
        "                lines.append(rand_l)\n",
        "                labels.append(\"Precision-Recall for Random Classifier\")\n",
        "\n",
        "                # Final touches on plot\n",
        "                fig = plt.gcf()\n",
        "                fig.subplots_adjust(bottom=0.25)\n",
        "                plt.xlim([0.0, 1.0])\n",
        "                plt.ylim([0.0, 1.05])\n",
        "                plt.xlabel('Recall')\n",
        "                plt.ylabel('Precision')\n",
        "                plt.title('Multiclass Precision-Recall Curve')\n",
        "                plt.legend(lines, labels, loc=(0, -.68), prop=dict(size=14))\n",
        "\n",
        "                if exp_name:\n",
        "                    fname = f\"{self.output_path}{exp_name}_prc.png\"\n",
        "                    plt.savefig(fname)\n",
        "                    print(f\"Stored Precision-Recall Curve: {fname}\")\n",
        "                plt.show()\n",
        "\n",
        "            else:\n",
        "                plt.figure()\n",
        "                plt.plot([0, 1], [random_pred_precision, random_pred_precision], linestyle='--', label='Random Prediction')\n",
        "                plt.step(recall[\"micro\"], precision[\"micro\"], where='post')\n",
        "\n",
        "                plt.xlabel('Recall')\n",
        "                plt.ylabel('Precision')\n",
        "\n",
        "                plt.ylim([0.0, 1.05])\n",
        "                plt.xlim([0.0, 1.0])\n",
        "                plt.title('Averaged Precision-Recall Curve')\n",
        "\n",
        "                if exp_name:\n",
        "                    fname = f\"{self.output_path}{exp_name}_prc.png\"\n",
        "                    plt.savefig(fname)\n",
        "                    print(f\"Stored Precision-Recall Curve: {fname}\")\n",
        "\n",
        "                plt.show()\n",
        "\n",
        "    def plot_data_distribution(self, data, normalize=True):\n",
        "        weights = np.array(self.get_counts_per_label(data))\n",
        "        if normalize:\n",
        "            weights = weights / sum(weights)\n",
        "\n",
        "        plt.bar(self.label_names, weights)\n",
        "        plt.xticks(self.label_names, rotation=90)\n",
        "        plt.title(\"Data Distribution\")\n",
        "        plt.xlabel(\"Label\")\n",
        "        plt.ylabel(\"Percentage of label in data\")\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Label counts:\")\n",
        "        print(dict(zip(self.label_names, weights)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjWpaTMcvDXr"
      },
      "source": [
        "def country_labeled_sentences(excel_map):\n",
        "    result = {}\n",
        "    sent_num = 0\n",
        "    \n",
        "    for country, dataframe in excel_map.items():\n",
        "\n",
        "        new_sents_col = dataframe[\"Sentence\"].dropna()\n",
        "        new_labels_col= dataframe[\"Primary Instrument\"].dropna()\n",
        "        \n",
        "        sentences = list(new_sents_col.apply(lambda x: x.replace(\"\\n\", \"\").strip()))\n",
        "        label_col = new_labels_col.apply(lambda x: x.replace(\"(PES)\", \"\").replace(\"(Bond)\", \"\").strip())\n",
        "        labels = [[string.strip() for string in label.split(\", \")][0] for label in label_col]\n",
        "        result[country] = {}\n",
        "\n",
        "        for sent, label in zip(sentences, labels):\n",
        "            if sent_num not in result[country]:\n",
        "                result[country][sent_num] = {\"text\": sent, \"labels\": [label]}\n",
        "            else:\n",
        "                result[country][sent_num][\"text\"] = sent\n",
        "                result[country][sent_num][\"labels\"] = [label]\n",
        "            \n",
        "            sent_num += 1\n",
        "            \n",
        "    return result\n",
        "\n",
        "def merge_labels(all_labels, labels_to_merge):\n",
        "    return [f\"{labels_to_merge[0]} & {labels_to_merge[1]}\" if label in labels_to_merge else label for label in all_labels]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoOH5t40xG_L"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def top_k_words(k, document, spacy_model, include_labels=None):\n",
        "    doc = spacy_model(document)\n",
        "\n",
        "    # all tokens that arent stop words or punctuations and are longer than 3 letters\n",
        "    words = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct and len(token.text) > 3]\n",
        "\n",
        "    # k most common tokens\n",
        "    word_freq = Counter(words)\n",
        "    common_words = word_freq.most_common(k)\n",
        "\n",
        "    result = list(list(zip(*common_words))[0])\n",
        "\n",
        "    if include_labels:\n",
        "        result.extend(include_labels)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def top_k_word_embeddings(top_k_words, spacy_model):\n",
        "    word_embeddings = []\n",
        "\n",
        "    for word in top_k_words:\n",
        "        doc = spacy_model(word)\n",
        "        vector = doc.vector\n",
        "        word_embeddings.append(vector.reshape(1, vector.shape[0]))\n",
        "\n",
        "    return word_embeddings\n",
        "\n",
        "\n",
        "def top_k_sbert_embeddings(top_k_words, sbert_model):\n",
        "    sbert_embeddings = []\n",
        "\n",
        "    for word in top_k_words:\n",
        "        vector = sbert_model.encode([word], convert_to_numpy=True)\n",
        "        sbert_embeddings.append(vector)\n",
        "\n",
        "    return sbert_embeddings\n",
        "\n",
        "\n",
        "def least_squares_with_reg(X, y, lamda=0.01):\n",
        "    # Help from: https://stackoverflow.com/questions/27476933/numpy-linear-regression-with-regularization and https://www.kdnuggets.com/2016/11/linear-regression-least-squares-matrix-multiplication-concise-technical-overview.html\n",
        "    # Multiple Linear Regression with OLS parameter estimation with L2 regularization term. lambda = 0 is equivalent to OLS estimation without regularization\n",
        "\n",
        "    X_inv = cupy.array(np.linalg.inv(X.T.dot(X) + lamda * np.eye(X.shape[1])).dot(X.T))\n",
        "\n",
        "    # return np.linalg.inv(X.T.dot(X) + lamda * np.eye(X.shape[1])).dot(X.T).dot(y)\n",
        "    return X_inv.dot(y)\n",
        "\n",
        "\n",
        "def calc_proj_matrix(sentences, k, spacy_model, sbert_model, lamda=0.01, include_labels=None):\n",
        "    sents_as_str = \". \".join(sentences)\n",
        "    top_words = top_k_words(k, sents_as_str, spacy_model, include_labels)\n",
        "    word_emb = np.vstack(top_k_word_embeddings(top_words, spacy_model))\n",
        "    sent_emb = np.vstack(top_k_sbert_embeddings(top_words, sbert_model))\n",
        "    proj_matrix = least_squares_with_reg(sent_emb, word_emb, lamda)\n",
        "\n",
        "    return proj_matrix\n",
        "\n",
        "\n",
        "def encode_sentence(sentence, model, Z):\n",
        "    sentence_rep = torch.from_numpy(np.matmul(model.encode(sentence), Z))\n",
        "    sentence_rep = sentence_rep.reshape((1, sentence_rep.shape[0]))\n",
        "    return sentence_rep\n",
        "\n",
        "\n",
        "def encode_labels(labels, model, Z):\n",
        "    return torch.from_numpy(np.matmul(model.encode(labels), Z))\n",
        "\n",
        "\n",
        "def classify_sentence(sentence, label_names, model, Z):\n",
        "    sentence_rep = encode_sentence(sentence, model, Z)\n",
        "    label_reps = encode_labels(label_names, model, Z)\n",
        "\n",
        "    return calc_cos_similarity(sentence_rep, label_reps, label_names)\n",
        "\n",
        "\n",
        "def calc_cos_similarity(sentence_rep, label_reps, label_names):\n",
        "    similarities = F.cosine_similarity(sentence_rep, label_reps)\n",
        "    closest = similarities.argsort(descending=True)\n",
        "\n",
        "    top_index = closest[0]\n",
        "    return label_names[top_index], similarities[top_index]\n",
        "\n",
        "\n",
        "def classify_sentence_given_label_reps(sentence, label_names, label_reps, model, Z):\n",
        "    sentence_rep = encode_sentence(sentence, model, Z)\n",
        "\n",
        "    return calc_cos_similarity(sentence_rep, label_reps, label_names)\n",
        "\n",
        "\n",
        "def calc_all_cos_similarity(all_sents_reps, label_reps, label_names):\n",
        "    model_preds, model_scores = [], []\n",
        "    for sent_rep in tqdm(all_sents_reps):\n",
        "        pred, score = calc_cos_similarity(sent_rep, label_reps, label_names)\n",
        "        model_preds.append(pred)\n",
        "        model_scores.append(score)\n",
        "\n",
        "    return model_preds, model_scores\n",
        "\n",
        "\n",
        "def classify_all_sentences(all_sents, label_names, sbert_model, proj_matrix):\n",
        "    model_preds, model_scores = [], []\n",
        "    label_reps = encode_labels(label_names, sbert_model, proj_matrix)\n",
        "\n",
        "    for sent in tqdm(all_sents):\n",
        "        pred, score = classify_sentence_given_label_reps(sent, label_names, label_reps, sbert_model, proj_matrix)\n",
        "        model_preds.append(pred)\n",
        "        model_scores.append(score)\n",
        "\n",
        "    return model_preds, model_scores\n",
        "\n",
        "\n",
        "def encode_all_sents(all_sents, sbert_model, proj_matrix=None):\n",
        "    if proj_matrix is None:\n",
        "        stacked = np.vstack([sbert_model.encode(sent) for sent in tqdm(all_sents)])\n",
        "    else:\n",
        "        stacked = np.vstack([encode_sentence(sent, sbert_model, proj_matrix) for sent in tqdm(all_sents)])\n",
        "    return [torch.from_numpy(element).reshape((1, element.shape[0])) for element in stacked]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYCwi6aoxNea"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scprep\n",
        "\n",
        "\n",
        "def visualize_embeddings_2D(embs, numeric_labels, tsne_perplexity, pca_k_n_comps=None, seed=69420, store_name=None):\n",
        "    df = pd.DataFrame()\n",
        "    df[\"y\"] = np.array(numeric_labels)\n",
        "\n",
        "    # Data for plot 1\n",
        "    pca = PCA(n_components=2, random_state=seed)\n",
        "    pca_result = pca.fit_transform(embs)\n",
        "    df['pca-1'] = pca_result[:, 0]\n",
        "    df['pca-2'] = pca_result[:, 1]\n",
        "\n",
        "    # Data for plot 2\n",
        "    tsne = TSNE(n_components=2, verbose=1, perplexity=tsne_perplexity, n_iter=1000, random_state=seed)\n",
        "    tsne_results = tsne.fit_transform(embs)\n",
        "    df[\"tsne-1\"] = tsne_results[:, 0]\n",
        "    df[\"tsne-2\"] = tsne_results[:, 1]\n",
        "\n",
        "    # Actual plotting\n",
        "    plt.figure(figsize=(24, 4))\n",
        "    ax1 = plt.subplot(1, 3, 1)\n",
        "    sns.scatterplot(\n",
        "        x=\"pca-1\", y=\"pca-2\",\n",
        "        hue=df.y.tolist(),\n",
        "        palette=\"bright\",\n",
        "        data=df,\n",
        "        legend=False,\n",
        "        ax=ax1\n",
        "    ).set(title=\"PCA projection\")\n",
        "\n",
        "    ax2 = plt.subplot(1, 3, 2)\n",
        "    sns.scatterplot(\n",
        "        x=\"tsne-1\", y=\"tsne-2\",\n",
        "        hue=df.y.tolist(),\n",
        "        palette=\"bright\",\n",
        "        data=df,\n",
        "        legend=False if pca_k_n_comps else \"auto\",\n",
        "        ax=ax2\n",
        "    ).set(title=\"t-SNE projection\")\n",
        "\n",
        "    if pca_k_n_comps:\n",
        "        # Data for plot 3\n",
        "        pca_k = PCA(n_components=pca_k_n_comps, random_state=seed)\n",
        "        pca_k_result = pca_k.fit_transform(embs)\n",
        "        tsne = TSNE(n_components=2, verbose=1, perplexity=tsne_perplexity, n_iter=1000, random_state=seed)\n",
        "        tsne_pca_results = tsne.fit_transform(pca_k_result)\n",
        "        df[f\"tsne-pca-{pca_k_n_comps}-1\"] = tsne_pca_results[:, 0]\n",
        "        df[f\"tsne-pca-{pca_k_n_comps}-2\"] = tsne_pca_results[:, 1]\n",
        "\n",
        "        # Actual plotting\n",
        "        ax3 = plt.subplot(1, 3, 3)\n",
        "        sns.scatterplot(\n",
        "            x=f\"tsne-pca-{pca_k_n_comps}-1\", y=f\"tsne-pca-{pca_k_n_comps}-2\",\n",
        "            hue=df.y.tolist(),\n",
        "            palette=\"bright\",\n",
        "            data=df,\n",
        "            ax=ax3\n",
        "        ).set(title=\"t-SNE on PCA projection\")\n",
        "\n",
        "    plt.legend(bbox_to_anchor=(1.01, 1), borderaxespad=0)\n",
        "    \n",
        "    if store_name:\n",
        "        plt.savefig(store_name + \"_viz.png\")\n",
        "\n",
        "\n",
        "def visualize_PCA_embeddings_3D(embs, labels, fname=None, seed=69420):\n",
        "    pca = PCA(n_components=3, random_state=seed)\n",
        "    pca_result = pca.fit_transform(embs)\n",
        "    data = np.vstack([pca_result[:, 0], pca_result[:, 1], pca_result[:, 2]]).T\n",
        "    colors = np.array(labels)\n",
        "\n",
        "    return scprep.plot.rotate_scatter3d(data, c=colors, figsize=(10, 8), title=f\"PCA 3 components\",\n",
        "                                        legend_anchor=(1.01, 1), filename=fname)\n",
        "\n",
        "\n",
        "def visualize_tSNE_embeddings_3D(embs, labels, perplexity=50, fname=None, seed=69420):\n",
        "    tsne = TSNE(n_components=3, verbose=1, perplexity=perplexity, n_iter=1000, random_state=seed)\n",
        "    tsne_result = tsne.fit_transform(embs)\n",
        "    data = np.vstack([tsne_result[:, 0], tsne_result[:, 1], tsne_result[:, 2]]).T\n",
        "    colors = np.array(labels)\n",
        "\n",
        "    return scprep.plot.rotate_scatter3d(data, c=colors, figsize=(10, 8), title=f\"t-SNE {perplexity} perplexity\",\n",
        "                                        legend_anchor=(1.01, 1), filename=fname)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4l8oGqlvDXs"
      },
      "source": [
        "## Fine-tuning the embedding model on the labeled data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KxOPUcovDXu"
      },
      "source": [
        "### Something we can try out:\n",
        "https://www.sbert.net/examples/training/data_augmentation/README.html#extend-to-your-own-datasets\n",
        "\n",
        "### Links:\n",
        "https://github.com/UKPLab/sentence-transformers/issues/350\n",
        "\n",
        "https://omoindrot.github.io/triplet-loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_chNEXckvDXu"
      },
      "source": [
        "### Possible tasks for fine-tuning:\n",
        "1) Given a pair of sentence embeddings, do they belong to the same category (binary)?\n",
        "\n",
        "2) Given a sentence and a category embedding, does the sentence belong to the category (binary)?\n",
        "\n",
        "3) Given a sentence embedding, use a classifier to predict its category (multiclass) [https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/nli/training_nli.py)\n",
        "\n",
        "4) Use a triplet loss approach such that sentences (texts) that have the same labels will become close in vector space, while sentences with a different label will be further away [https://github.com/UKPLab/sentencetransformers/blob/master/examples/training/other/training_batch_hard_trec_continue_training.py](https://github.com/UKPLab/sentencetransformers/blob/master/examples/training/other/training_batch_hard_trec_continue_training.py)\n",
        "   \n",
        "#### In this notebook **task number 3** is used to fine-tune the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEhL0mw8vDXv"
      },
      "source": [
        "# Train test split stratified\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=0.15, stratify=all_labels, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swPfKIanvDXv"
      },
      "source": [
        "# Define model to fine-tune\n",
        "model = SentenceTransformer('stsb-xlm-r-multilingual')\n",
        "# model = SentenceTransformer('xlm-r-100langs-bert-base-nli-stsb-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ5dP4S8vDXw"
      },
      "source": [
        "class SoftmaxClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    This loss adds a softmax classifier on top of the output of the transformer network. \n",
        "    It takes a sentence embedding and learns a mapping between it and the corresponding category.\n",
        "    :param model: SentenceTransformer model\n",
        "    :param sentence_embedding_dimension: Dimension of your sentence embeddings\n",
        "    :param num_labels: Number of different labels\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 model: SentenceTransformer,\n",
        "                 sentence_embedding_dimension: int,\n",
        "                 num_labels: int):\n",
        "        super(SoftmaxClassifier, self).__init__()\n",
        "        self.model = model\n",
        "        self.num_labels = num_labels\n",
        "        self.classifier = nn.Linear(sentence_embedding_dimension, num_labels)\n",
        "\n",
        "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
        "        # Get batch sentence embeddings\n",
        "        features = self.model(sentence_features[0])['sentence_embedding']\n",
        "        \n",
        "        # Get batch loss\n",
        "        output = self.classifier(features)\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = loss_fct(output, labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return features, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWsN0NMTvDXx"
      },
      "source": [
        "# Load data samples into batches\n",
        "train_batch_size = 16\n",
        "label2int = dict(zip(label_names, range(len(label_names))))\n",
        "train_samples = []\n",
        "for sent, label in zip(X_train, y_train):\n",
        "    label_id = label2int[label]\n",
        "    train_samples.append(InputExample(texts=[sent], label=label_id))\n",
        "train_dataset = SentencesDataset(train_samples, model=model)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
        "\n",
        "# Define the way the loss is computed\n",
        "classifier = SoftmaxClassifier(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=len(label2int))\n",
        "\n",
        "# Configure the dev set evaluator - still need to test whether this works\n",
        "dev_samples = []\n",
        "for sent, label in zip(X_test, y_test):\n",
        "    label_id = label2int[label]\n",
        "    dev_samples.append(InputExample(texts=[sent], label=label_id))\n",
        "dev_dataset = SentencesDataset(dev_samples, model=model)\n",
        "dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=train_batch_size)\n",
        "dev_evaluator = LabelAccuracyEvaluator(dataloader=dev_dataloader, softmax_model=classifier, name='lae-dev')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wog9dImLvDXy"
      },
      "source": [
        "# Configure the training\n",
        "num_epochs = 1\n",
        "warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1)  # 10% of train data for warm-up\n",
        "model_save_path = \"../../output/FineTuning\"\n",
        "# model_save_path = \"/content/drive/MyDrive/WRI-LatinAmerica-Talent/Modeling/FineTuning\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awzBPX5IvDXz"
      },
      "source": [
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, classifier)],\n",
        "          evaluator=dev_evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVsBh96ivDX0"
      },
      "source": [
        "# Load the saved model and obtain random sentence embedding\n",
        "load_model = SentenceTransformer(model_save_path)\n",
        "load_model.encode(all_sents[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhqZctr8xtAA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6aYlVsoxtKQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG2p7TsoxtNn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot37jC2mxuGK"
      },
      "source": [
        "## Run fine tuning experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WhMprt8l6PO"
      },
      "source": [
        "class SoftmaxClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    This loss adds a softmax classifier on top of the output of the transformer network. \n",
        "    It takes a sentence embedding and learns a mapping between it and the corresponding category.\n",
        "    :param model: SentenceTransformer model\n",
        "    :param sentence_embedding_dimension: Dimension of your sentence embeddings\n",
        "    :param num_labels: Number of different labels\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 model: SentenceTransformer,\n",
        "                 sentence_embedding_dimension: int,\n",
        "                 num_labels: int):\n",
        "        super(SoftmaxClassifier, self).__init__()\n",
        "        self.model = model\n",
        "        self.num_labels = num_labels\n",
        "        self.classifier = nn.Linear(sentence_embedding_dimension, num_labels)\n",
        "\n",
        "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
        "        # Get batch sentence embeddings\n",
        "        features = self.model(sentence_features[0])['sentence_embedding']\n",
        "        \n",
        "        # Get batch loss\n",
        "        output = self.classifier(features)\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = loss_fct(output, labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return features, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F75W2lmqxtSX"
      },
      "source": [
        "# Reading data from excel\n",
        "data_excel = pd.read_excel(\"/content/drive/MyDrive/WRI-LatinAmerica-Talent/Cristina_Policy_Files/WRI_Policy_Tags.xlsx\", engine=\"openpyxl\", sheet_name=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIhBjg01x3z0",
        "outputId": "40827538-c704-4a81-81f3-06d464b9a2d5"
      },
      "source": [
        "# Formatting the data\n",
        "all_labeled_sentences = country_labeled_sentences(data_excel)\n",
        "labeled_sents = dict()\n",
        "for sents in all_labeled_sentences.values():\n",
        "    labeled_sents.update(sents)\n",
        "\n",
        "# Fitlering out General Incentive and Unknown sentences\n",
        "filtered_sents_maps = [sent for sent in labeled_sents.values() if sent['labels'][0] not in [\"General incentive\", \"Unknown\", \"Other\"]]\n",
        "all_sents = [sent['text'] for sent in filtered_sents_maps]\n",
        "all_labels = [sent['labels'][0] for sent in filtered_sents_maps]\n",
        "all_labels = merge_labels(all_labels, [\"Credit\", \"Guarantee\"])\n",
        "label_names = list(set(all_labels))\n",
        "numeric_labels = labels2numeric(all_labels, label_names)\n",
        "label_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Credit & Guarantee',\n",
              " 'Fine',\n",
              " 'Direct payment',\n",
              " 'Technical assistance',\n",
              " 'Supplies',\n",
              " 'Tax deduction']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHLfOdQXnUhn"
      },
      "source": [
        "Reading files from JSON"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdD2Da9fnRqb"
      },
      "source": [
        "dataset_fname = \"\"\n",
        "\n",
        "# If the json is in the format that includes headers and other titles:\n",
        "dataset = load_file(dataset_fname)\n",
        "dataset_map = labeled_sentences_from_dataset(dataset) # Labels AND sentences\n",
        "all_sents = sentences_from_dataset(dataset) # Just sentences\n",
        "all_labels = labels_from_dataset(dataset) # Just labels\n",
        "\n",
        "# If the json is in the format where it only contains sentences and labels\n",
        "dataset = load_file(dataset_fname) \n",
        "dataset_map = labels_from_model_output(dataset) # Labels AND sentences\n",
        "all_sents = sentences_from_model_output(dataset) # Just sentences\n",
        "all_labels = labels_from_model_output(dataset) # Just labels\n",
        "\n",
        "# The rest:\n",
        "label_names = unique_labels(all_labels)\n",
        "numeric_labels = labels2numeric(all_labels, label_names)\n",
        "label_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIQpblTXnR3p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sNGrOvI2zF0"
      },
      "source": [
        "import time\n",
        "import cupy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1eWkGLcx_yi"
      },
      "source": [
        "import spacy\n",
        "spacy.prefer_gpu()\n",
        "es_nlp = spacy.load('es_core_news_lg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9ycZoZ5yU0d"
      },
      "source": [
        "model_names = ['distiluse-base-multilingual-cased-v2', 'stsb-xlm-r-multilingual', 'paraphrase-xlm-r-multilingual-v1', 'quora-distilbert-multilingual']\n",
        "\n",
        "# Train test split stratified\n",
        "all_test_perc = [0.15, 0.2, 0.25, 0.3, 0.4]\n",
        "\n",
        "# Output setup\n",
        "output = {}\n",
        "\n",
        "for test_perc in all_test_perc:\n",
        "  output[f\"test_perc={test_perc}\"] = {}\n",
        "  X_train, X_test, y_train, y_test = train_test_split(all_sents, all_labels, test_size=test_perc, stratify=all_labels, random_state=69420)\n",
        "\n",
        "  # Load data samples into batches\n",
        "  train_batch_size = 16\n",
        "  label2int = dict(zip(label_names, range(len(label_names))))\n",
        "  train_samples = []\n",
        "  for sent, label in zip(X_train, y_train):\n",
        "      label_id = label2int[label]\n",
        "      train_samples.append(InputExample(texts=[sent], label=label_id))\n",
        "\n",
        "  # Configure the dev set evaluator - still need to test whether this works\n",
        "  dev_samples = []\n",
        "  for sent, label in zip(X_test, y_test):\n",
        "      label_id = label2int[label]\n",
        "      dev_samples.append(InputExample(texts=[sent], label=label_id))\n",
        "  \n",
        "  for model_name in model_names:\n",
        "    # Setup\n",
        "    model_preds = []\n",
        "    model_scores = []\n",
        "    output[f\"test_perc={test_perc}\"][model_name] = []\n",
        "    \n",
        "    # Train set config\n",
        "    model = SentenceTransformer(model_name)\n",
        "    train_dataset = SentencesDataset(train_samples, model=model)\n",
        "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size)\n",
        "    \n",
        "    # Define the way the loss is computed\n",
        "    classifier = SoftmaxClassifier(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=len(label2int))\n",
        "    \n",
        "    # Dev set config\n",
        "    dev_dataset = SentencesDataset(dev_samples, model=model)\n",
        "    dev_dataloader = DataLoader(dev_dataset, shuffle=True, batch_size=train_batch_size)\n",
        "    dev_evaluator = LabelAccuracyEvaluator(dataloader=dev_dataloader, softmax_model=classifier, name='lae-dev')\n",
        "\n",
        "    # Configure the training\n",
        "    max_num_epochs = 10\n",
        "        \n",
        "    for num_epochs in range(2, max_num_epochs + 2, 2):\n",
        "        print(\"Num epochs:\", num_epochs)\n",
        "        \n",
        "        warmup_steps = math.ceil(len(train_dataset) * num_epochs / train_batch_size * 0.1)  # 10% of train data for warm-up\n",
        "        model_deets = f\"model={model_name}_test-perc={test_perc}_n-epoch={num_epochs}\"\n",
        "        model_save_path = f\"/content/drive/MyDrive/WRI-LatinAmerica-Talent/Modeling/FineTuning_{model_deets}\"\n",
        "        \n",
        "\n",
        "        # Train the model\n",
        "        start = time.time()\n",
        "        model.fit(train_objectives=[(train_dataloader, classifier)],\n",
        "                  evaluator=dev_evaluator,\n",
        "                  epochs=2, # We always tune on an extra epoch to see the performance gain\n",
        "                  evaluation_steps=1000,\n",
        "                  warmup_steps=warmup_steps,\n",
        "                  output_path=model_save_path\n",
        "                  )\n",
        "        \n",
        "        end = time.time()\n",
        "        hours, rem = divmod(end-start, 3600)\n",
        "        minutes, seconds = divmod(rem, 60)\n",
        "        print(\"Time taken for fine-tuning:\", \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))\n",
        "        \n",
        "        ### Classify sentences\n",
        "        # Projection matrix Z low-dim projection\n",
        "        print(\"Classifying sentences...\")\n",
        "        proj_matrix = cupy.asnumpy(calc_proj_matrix(all_sents, 50, es_nlp, model, 0.01))\n",
        "        all_sent_embs = encode_all_sents(all_sents, model, proj_matrix)\n",
        "        all_label_embs = encode_labels(label_names, model, proj_matrix)\n",
        "        visualize_embeddings_2D(np.vstack(all_sent_embs), all_labels, tsne_perplexity=50, store_name=f\"{model_save_path}/{model_deets}\")\n",
        "        model_preds, model_scores = calc_all_cos_similarity(all_sent_embs, all_label_embs, label_names)\n",
        "        \n",
        "        ### Evaluate the model\n",
        "        numeric_preds = labels2numeric(model_preds, label_names)\n",
        "        evaluator = ModelEvaluator(label_names, y_true=numeric_labels, y_pred=numeric_preds)\n",
        "        \n",
        "        output[f\"test_perc={test_perc}\"][model_name].append({\"num_epochs\": num_epochs, \"avg_f1\": evaluator.avg_f1.tolist()})\n",
        "        \n",
        "        evaluator.plot_confusion_matrix(color_map='Blues', exp_name=f\"{model_save_path}/{model_deets}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyJSG6SIaECl",
        "outputId": "6719cb8c-1aec-412d-ed84-7e117d925000"
      },
      "source": [
        "output.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['distiluse-base-multilingual-cased-v2', 'stsb-xlm-r-multilingual', 'paraphrase-xlm-r-multilingual-v1', 'quora-distilbert-multilingual'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e65BceczaDwI"
      },
      "source": [
        "new_json = {}\n",
        "\n",
        "for key in output.keys():\n",
        "  new_json[key] = {}\n",
        "  for subkey in output[key].keys():\n",
        "    new_json[key][subkey] = []\n",
        "    for element in output[key][subkey]:\n",
        "      el_copy = {\"avg_f1\": element[\"avg_f1\"].tolist(), \"num_epochs\": element[\"num_epochs\"]}\n",
        "      new_json[key][subkey].append(el_copy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40gn7pZUHO4c"
      },
      "source": [
        "import json\n",
        "with open(\"/content/drive/MyDrive/WRI-LatinAmerica-Talent/Modeling/FineTuningResults.json\", \"w\") as f:\n",
        "  json.dump(new_json, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uwxoji8ka-7N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}