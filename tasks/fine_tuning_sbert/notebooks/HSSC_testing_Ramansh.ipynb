{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine_tuning_SBERT_HSSC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk0XWLfMvDXU"
      },
      "source": [
        "# Fine tuning with S-BERT\n",
        "\n",
        "### Instructions to run this notebook\n",
        "\n",
        "#### These are the sections in this notebook. Please read the notes below to avoid any errors.\n",
        "\n",
        "1. [Setup](#setup) - Install the necessary libraries, the GitHub repository, and import the code from the repository.\n",
        "2. [Data Loading](#data-loading) - Define your **GLOBAL_EXPERIMENT_NUMBER** carefully so that it doesn't overwrite another folder on Google Drive. Running this section will produce the data distriibution for the binary/multiclass classification dataset. A good visual check to see if the number of examples in the classes are roughly equal and well distributes in the training and test datasets. This section also makes sure that the training sentences and labels are available to the Python scripts from the GitHub repository to fine-tune the model.\n",
        "3. [Configuring W&B credentials and Run](#wandb-config) - The most importany part of this section is to insert your WANDB_API_KEY in the key variable. This will make sure that you can submit your run to the common Weights&Biases project. The WANDB_RUN_GROUP and WANDB_JOB_TYPE variables can be set as well. These 2 variables help in filtering multiple runs of the same or similar hyperparameter configuring for better readability on the Weights&Biases dashboard.\n",
        "4. [Grid Search Fine Tuning - W&B sweeps](#wandb-sweep) - In this section we define the Weights&Biases hyperparameter sweep. The method of the sweep defines how the automatic tuning will happen (random is the easiest way). If we use the random method, we have to specify the ``count`` parameter later on to tell wandb how many random subsets are to be taken when running the sweep. You can also specify the name of the sweep in this sweep_config. Another important part is to define the maximizing factor, right now in our case it is the ``Weighted F1 validation`` score. **Note**, the maximizing factor in the metrics needs to be a value that we are loggings with wandb (wandb.log), the string needs to match exactly. In the hyperparameter dictionary of the sweep is where we list the multiple values of each hyperparameter that we wish to fine-tune. If the key ``values`` is used, wandb expects a list of values for that hyperparameter, if the key is ``value`` there should only be one value. In this way we can specify constant hyperparameters as well. \n",
        "5. [Running training function for only one run](#single-run) - In the scenario where we push to only submit a single run to Weights&Biases, we can use this section to run another training function with **one set of hyperparameter values**. **Note** that this section still requires the WANDB_API_KEY to be set in the [Configuring W&B credentials and Run](#wandb-config) section as well as the run group if it is required.\n",
        "6. [Removing the saving directory from Google Drive](#delete-folder) - While fine-tuning we save the model in the Google Drive in the GLOBAL_EXPERIMENT_NUMBER folder. After the model is successfully saved onto the Weights&Biases run, we can safely delete the folder from Google Drive to save storage space.\n",
        "7. [Loading saved model](#load-model) - We can load the model saved in the single run fine-tuning section. The run id is already saved in a variable which is how wandb finds the ``saved_model.pt`` file. If you wish to retrieve a model from a sweep, you need to find the run id from the online dashboard of the best run and use it in this section.\n",
        "8. [Testing model on test set](#test-set) - In this section we get a realistic performance of the saved model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW6NlcIEvXbG"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Ml7Gd4VgxW1d",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e87d8d-c2cd-49e3-b1e2-987bf26c4f5b"
      },
      "source": [
        "# Install necessary libraries\n",
        "! pip install --quiet scprep \\\n",
        "  wandb \\\n",
        "  sentence_transformers==1.0.2 \\\n",
        "  phate==1.0.7 \\\n",
        "  boto3\n",
        "  # Restarting the runtime is required for the libraries to be active in the notebook\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 112kB 26.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 40.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 55.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 54.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 11.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.5MB 46.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 49.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 419kB 50.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 25.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 41.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 35.3MB/s \n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfOtuJGD-XaT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "340d4605-b87e-4cca-9005-ce31bed28f37"
      },
      "source": [
        "# Setup connection with your own google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVeFAuzRLxy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b7e94e-456d-4a29-949b-3a2e1c053725"
      },
      "source": [
        "# Clone branch from github\n",
        "!rm -rf policy-data-analyzer/\n",
        "!branch_name='hssc' && \\\n",
        "  git clone --branch $branch_name https://github.com/wri-dssg/policy-data-analyzer.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'policy-data-analyzer'...\n",
            "remote: Enumerating objects: 6654, done.\u001b[K\n",
            "remote: Counting objects: 100% (720/720), done.\u001b[K\n",
            "remote: Compressing objects: 100% (479/479), done.\u001b[K\n",
            "remote: Total 6654 (delta 474), reused 446 (delta 239), pack-reused 5934\u001b[K\n",
            "Receiving objects: 100% (6654/6654), 209.79 MiB | 30.91 MiB/s, done.\n",
            "Resolving deltas: 100% (3702/3702), done.\n",
            "Checking out files: 100% (1008/1008), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDjA5e9xd3Wk"
      },
      "source": [
        "#If you run this cell more than once, comment out this line because you are already in this folder and you will get an error\n",
        "import os\n",
        "os.chdir(\"policy-data-analyzer\") \n",
        "\n",
        "# from tasks.fine_tuning_sbert.src.loops import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlL8L6HzjeWr"
      },
      "source": [
        "<a name=\"data-loading\"></a>\n",
        "## Experiment setup and run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tgGMqalNBJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496bed08-44a2-4428-c59f-8e9c93269e1a"
      },
      "source": [
        "'''\n",
        "PASTE YOUR WEIGHTS & BIASES KEY HERE\n",
        "Please do not forget to delete the key after finishing using the notebook. Or simply don't save the notebook to GitHub or Google Drive :)\n",
        "If the key is compromised you can always make a new one in your W&B settings and remove the old one :)\n",
        "'''\n",
        "import os\n",
        "import wandb\n",
        "import time\n",
        "from tasks.data_loading.src.utils import *\n",
        "from tasks.fine_tuning_sbert.src.loops import *\n",
        "\n",
        "wandb_key = ''\n",
        "group_desc = 'testing-error'\n",
        "job_type = ''\n",
        "\n",
        "os.environ['WANDB_JOB_TYPE'] = job_type\n",
        "os.environ['WANDB_RUN_GROUP'] = group_desc\n",
        "os.environ['WANDB_API_KEY'] = wandb_key"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using the GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MbrGYQv4ykiW",
        "outputId": "75c3fa41-3c62-4d16-ed8a-c6e318633d53"
      },
      "source": [
        "base_path = \"/content/drive/MyDrive/Official Folder of WRI Latin America Project/WRI-LatinAmerica-Talent\"\n",
        "data_path = f\"{base_path}/Modeling/Labeled data\"\n",
        "results_save_path = f\"{base_path}/Modeling/HSSC/Results/\"\n",
        "\n",
        "project_name = 'HSSC'\n",
        "languages = [\"spanish\", \"english\"]\n",
        "classification = [\"binary\", \"multiclass\"]\n",
        "labeling = [\"handpicked\", \"assisted\", \"merged\"]\n",
        "\n",
        "\n",
        "languages = [\"spanish\"]\n",
        "classification = [\"binary\"]\n",
        "labeling = [\"handpicked\"]\n",
        "\n",
        "for language in languages:\n",
        "\n",
        "  for classif_type in classification:\n",
        "\n",
        "    for training in labeling:\n",
        "        # Setup the WandB group\n",
        "        group_name = language + \"_\" + classif_type + \"_\" + training + '_testingerror'\n",
        "        \n",
        "        # Load training dataset\n",
        "        train_sents, train_labels = load_training_dataset_HSSC(data_path, language, classif_type, training, \"train\")\n",
        "        label_names = unique_labels(train_labels)\n",
        "\n",
        "        #Model training\n",
        "        single_run_params = {\n",
        "            \"all_dev_perc\": 0.20,\n",
        "            \"model_names\": 'paraphrase-xlm-r-multilingual-v1',\n",
        "            \"output_path\": None,\n",
        "            \"max_num_epochs\": 10,\n",
        "            \"group_name\": group_name\n",
        "        }\n",
        "        \n",
        "        model = single_run_fine_tune_HSSC(single_run_params, train_sents, train_labels, label_names)\n",
        "\n",
        "        for testing in labeling:\n",
        "            start = time.time()\n",
        "\n",
        "            wandb.run.name = group_name + \"_\" + testing\n",
        "            #Loading testing dataset\n",
        "            test_sents, test_labels = load_training_dataset_HSSC(data_path, language, classif_type, testing, \"test\")\n",
        "            # class balance/imbalance for test set\n",
        "            label_names_test = unique_labels(test_labels)\n",
        "            numeric_train_labels_test = labels2numeric(test_labels, label_names_test)\n",
        "            print(\"\\n*****\", group_name, \" -- \", testing, \"*****\\n\")\n",
        "            class_distribution = plot_data_distribution_HSSC(numeric_train_labels_test, label_names_test)\n",
        "            wandb.log({\"label class distribution\": wandb.Image(class_distribution)})\n",
        "            #Classification and evaluation\n",
        "            clf = RandomForestClassifier(n_estimators=500,\n",
        "                                max_features=0.06,\n",
        "                                n_jobs=6,\n",
        "                                random_state=69420)\n",
        "            F1 = evaluate_using_sklearn(clf, model, train_sents, train_labels, test_sents, test_labels,\n",
        "                            label_names)\n",
        "            wandb.log({\"F1\": F1})\n",
        "            print(\"\\######\", F1, \"######\\n\")\n",
        "            end = time.time()\n",
        "            hours, rem = divmod(end - start, 3600)\n",
        "            minutes, seconds = divmod(rem, 60)\n",
        "            print(\"Time taken for fine-tuning:\",\n",
        "                \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mramanshsharma\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Problem at: <ipython-input-4-3383b75456b6> 23 <module>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 757, in init\n",
            "    run = wi.init()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\", line 520, in init\n",
            "    backend.cleanup()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/backend/backend.py\", line 167, in cleanup\n",
            "    self.interface.join()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\", line 836, in join\n",
            "    _ = self._communicate(record)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\", line 545, in _communicate\n",
            "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\", line 550, in _communicate_async\n",
            "    raise Exception(\"The wandb backend process has shutdown\")\n",
            "Exception: The wandb backend process has shutdown\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m             \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0;31m# we don't need to do console cleanup at this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m                 \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/backend/backend.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwandb_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, rec, timeout, local)\u001b[0m\n\u001b[1;32m    544\u001b[0m     ) -> Optional[pb.Result]:\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36m_communicate_async\u001b[0;34m(self, rec, local)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_router\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_and_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3383b75456b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mgroup_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclassif_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_testingerror'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Init WandB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jordi_planas'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Load training dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtrain_sents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_training_dataset_HSSC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassif_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexcept_exit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"problem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_seen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: problem"
          ]
        }
      ]
    }
  ]
}