{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to split sentences and store them in S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Methods for sentence splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import nltk.data\n",
    "import spacy \n",
    "import string\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import uuid\n",
    "import json\n",
    "import boto3\n",
    "import csv\n",
    "import s3fs\n",
    "import codecs\n",
    "\n",
    "en_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "es_tokenizer = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    return re.sub(re.compile('<.*?>'), '', text)\n",
    "\n",
    "def replace_links(text):\n",
    "    text = re.sub(r'http\\S+', '[URL]', text)\n",
    "    return re.sub(r'www\\S+', '[URL]', text)\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "# abreviations = {\"No.\", \"Sec.\", \"Cong.\", \"Dist.\", \"Doc.\"}\n",
    "# acronyms = {\"W.D.\", \"U.S.\", \"H.R.\", \"U.S.C.\", \"p.m.\", \"a.m.\"}\n",
    "\n",
    "def parse_emails(text):\n",
    "    \"\"\" \n",
    "    Remove the periods from emails in text, except the last one\n",
    "    \"\"\"\n",
    "    emails = [email if email[-1] != \".\" else email[:-1] for email in re.findall(r\"\\S*@\\S*\\s?\", text)]\n",
    "    \n",
    "    for email in emails:\n",
    "        new_email = email.replace(\".\", \"\")\n",
    "        text = text.replace(email, new_email)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def parse_acronyms(text):\n",
    "    \"\"\" \n",
    "    Remove the periods from acronyms in the text (i.e \"U.S.\" becomes \"US\") \n",
    "    \"\"\"\n",
    "\n",
    "    acronyms = re.findall(r\"\\b(?:[a-zA-Z]\\.){2,}\", text)\n",
    "         \n",
    "    for acronym in acronyms:\n",
    "        new_acronym = acronym.replace(\".\", \"\")\n",
    "        text = text.replace(acronym, new_acronym)\n",
    "        \n",
    "    return text\n",
    "\n",
    "def english_preprocessing(txt):\n",
    "    \"\"\"\n",
    "    Steps in the preprocessing of text:\n",
    "        1. Remove HTML tags\n",
    "        2. Replace URLS by a tag [URL]\n",
    "        3. Replace new lines and tabs by normal spaces - sometimes sentences have new lines in the middle\n",
    "        4. Remove excessive spaces (more than 1 occurrence)\n",
    "        5. Parse abreviations and acronyms\n",
    "    \"\"\"\n",
    "    txt = replace_links(remove_html_tags(txt)).strip()#.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "    txt = remove_multiple_spaces(txt)\n",
    "    txt = parse_emails(txt)\n",
    "    txt = parse_acronyms(txt)\n",
    "    \n",
    "    new_txt = \"\"\n",
    "    all_period_idx = set([indices.start() for indices in re.finditer(\"\\.\", txt)])\n",
    "    \n",
    "    for i, char in enumerate(txt):\n",
    "        if i in all_period_idx:\n",
    "            # Any char following a period that is NOT a space means that we should not add that period\n",
    "            if i + 1 < len(txt) and txt[i + 1] != \" \":\n",
    "                continue\n",
    "            \n",
    "            # Any char that is a number following a period will not count. \n",
    "            # For enumerations, we're counting on docs being enumerated as \"(a)\" or \"(ii)\", and if not, they will be separated by the . after the number (\"3. Something\" will just be \"Something\" as a sentence)\n",
    "            if i + 2 < len(txt) and txt[i + 2].isnumeric(): \n",
    "                continue\n",
    "            \n",
    "            # If we wanted to have all numbered lists together, uncomment this, and comment out the previous condition\n",
    "#             if i + 2 < len(txt) and not txt[i + 2].isalpha(): \n",
    "#                 continue\n",
    "            \n",
    "        new_txt += char\n",
    "\n",
    "    return new_txt\n",
    "\n",
    "def english_postprocessing(sents, min_num_words=4):\n",
    "    \"\"\"\n",
    "    Remove sentences that are made of less than a given number of words. Default is 4\n",
    "    \"\"\"\n",
    "    \n",
    "    return [sent for sent in sents if len(sent.split()) >= min_num_words]\n",
    "\n",
    "def get_nltk_sents(txt, tokenizer, extra_abbreviations=None):\n",
    "    if extra_abbreviations:\n",
    "        tokenizer._params.abbrev_types.update(extra_abbreviations)\n",
    "        \n",
    "    sents = tokenizer.tokenize(txt)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_credentials_from_file(f_name):\n",
    "    with open(f_name, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "    \n",
    "    return creds[\"aws\"][\"id\"], creds[\"aws\"][\"secret\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sents_for_output(sents, doc_id):\n",
    "    formatted_sents = {}\n",
    "\n",
    "    for i, sent in enumerate(sents):\n",
    "        formatted_sents.update({f\"{doc_id}_sent_{i}\": {\"text\": sent, \"label\": []}})\n",
    "\n",
    "    return formatted_sents\n",
    "\n",
    "\n",
    "def output_sents(sents, f_name, f_uuid, country, bucket, output_dir):\n",
    "\n",
    "    sents_json = {}\n",
    "    fformat = f_name.split(\".\")[-1]\n",
    "    sents_json[f_uuid] = {\"metadata\":\n",
    "                              {\"n_sentences\": len(sents),\n",
    "                               \"file_name\": f_name,\n",
    "                               \"file_format\": fformat,\n",
    "                               \"country\": country},\n",
    "                          \"sentences\": format_sents_for_output(sents, f_uuid)}\n",
    "    \n",
    "    s3.Object(bucket, f\"{output_dir}/{f_uuid}_sents.json\").put(Body=(json.dumps(sents_json, indent=4)))\n",
    "    \n",
    "    \n",
    "def filenames_for_country(country, s3, bucket):\n",
    "    metadata_fname = f\"metadata/{country}_metadata.csv\"\n",
    "    obj = s3.Object(bucket_name = bucket, key = metadata_fname)\n",
    "    \n",
    "    filenames = []\n",
    "    i = 0\n",
    "    for row in csv.reader(codecs.getreader(\"utf-8\")(obj.get()['Body'])):\n",
    "        # Add original file ID without the file format\n",
    "        filenames.append(row[3][:-4])\n",
    "    \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actual pipeline\n",
    "\n",
    "Reads from the S3 Bucket english documents, outputs to test_sentences\n",
    "\n",
    "\n",
    "Format of credentials JSON file:\n",
    "```\n",
    "{\n",
    "    \"aws\": {\n",
    "        \"id\": \"AWS ID\",\n",
    "        \"secret\": \"AWS SECRET\"\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_file = '/Users/dafirebanks/Documents/credentials.json'\n",
    "aws_id, aws_secret = aws_credentials_from_file(credentials_file)\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = aws_id,\n",
    "    aws_secret_access_key = aws_secret\n",
    ")\n",
    "\n",
    "bucket = 'wri-nlp-policy'\n",
    "countries = ['India', 'USA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline is ready to be ran on the India documents only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define abbreviations per country - this can work for all english documents\n",
    "usa_abrevs = {\"no\", \"sec\", \"cong\", \"dist\", \"doc\"}\n",
    "india_abrevs = {\"sub\", \"subs\", \"ins\", \"govt\", \"dy\", \"dept\", \"deptt\", \"ptg\"} \n",
    "\n",
    "# Define country and output directory in S3 bucket\n",
    "country = \"India\"\n",
    "out_dir = \"english_documents/test_sentences\"\n",
    "select_filenames = filenames_for_country(country, s3, bucket)\n",
    "\n",
    "i = 0\n",
    "for obj in s3.Bucket(bucket).objects.all().filter(Prefix=\"english_documents/text_files/\"):\n",
    "    \n",
    "    # Don't get the directory itself\n",
    "    if not obj.key.endswith(\"/\"):\n",
    "        file_id = obj.key.replace(\"english_documents/text_files/\", \"\").replace(\".txt\", \"\")\n",
    "        if file_id in select_filenames:\n",
    "            text = obj.get()['Body'].read().decode('utf-8')\n",
    "            preprocessed = english_preprocessing(text)\n",
    "            # NOTE: Change abreviations in the future specific to country\n",
    "            sents = get_nltk_sents(preprocessed, en_tokenizer, india_abrevs)\n",
    "            post_processed_sents = english_postprocessing(sents, min_num_words=5)\n",
    "            output_sents(post_processed_sents, obj.key, file_id, country, bucket, out_dir)\n",
    "\n",
    "    i += 1\n",
    "    if i == 2:\n",
    "        break\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i} documents\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wri-env",
   "language": "python",
   "name": "wri-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
