{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Headings Curation and sentences spliter of Chilean Policies\n",
    "\n",
    "In this notebook there are a series of dictionaries and methods to curate section headings of El Salvador policies. Policies from El Salvador have a rather definite structure, so that the law text is organized under section headings. There are two kinds of sections, the ones that are general and that can be often found in many policies, and the ones which are specific. The sections headings which are more general often come with a whole range of name variants which makes the task of machine recognition difficult.\n",
    "\n",
    "The goal of this notebook is to group all pretreatment methods that would harmonize sections heading to make the further processing machine friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import boto3, json, operator, os, re, string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries of particular vocabularies to help in the curation of section headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most policies come with the final signatures. This is a piece of text that we want to be able to recognize. To make the\n",
    "# detection of signatures easier, this dictionary contain the most common terms that can be found in these lines of text.\n",
    "official_positions = {\"ALCALDE\" : 0,\n",
    "\"Alcalde\" : 0,\n",
    "\"MINISTRA\" : 0,\n",
    "\"Ministra\" : 0,\n",
    "\"MINISTRO\" : 0,\n",
    "\"Ministro\" : 0,\n",
    "\"PRESIDENTA\" : 0,\n",
    "\"Presidenta\" : 0,\n",
    "\"PRESIDENTE\" : 0,\n",
    "\"Presidente\" : 0,\n",
    "\"REGIDOR\" : 0,\n",
    "\"Regidor\"  : 0,\n",
    "\"REGIDORA\" : 0,\n",
    "\"regidora\" : 0,\n",
    "\"SECRETARIA\" : 0,\n",
    "\"Secretaria\" : 0,\n",
    "\"SECRETARIO\" : 0,\n",
    "\"Secretario\" : 0,\n",
    "\"SINDICA\" : 0,\n",
    "\"Sindica\" : 0,\n",
    "\"SINDICO\" : 0,\n",
    "\"Sindico\" : 0,\n",
    "\"VICEPRESIDENTA\" : 0,\n",
    "\"Vicepresidenta\" : 0,\n",
    "\"VICEPRESIDENTE\" : 0,\n",
    "\"Vicepresidente\" : 0\n",
    "}\n",
    "\n",
    "end_of_file_tags = {\n",
    "    \"Anótese\" : 0,\n",
    "    \"Anotese\" : 0,\n",
    "    \"Publíquese\" : 0,\n",
    "    \"Publiquese\" : 0\n",
    "}\n",
    "# This dictionary contains some correspondences among different text headings. This is under development and needs further\n",
    "# improvement.The idea is to merge in a single name all the headings that point to the same conceptual concept. For example,\n",
    "# \"Definiciones\" is a heading that can come alone or together with other terms so it can appear as \"Definiciones básicas\" or\n",
    "# \"Definiciones generales\". With the dictionary we can fetch all headings that contain the word \"Definiciones\" and change the\n",
    "# heading to \"Definiciones\".\n",
    "merges = {\n",
    "    \"CONCEPTOS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"Considerando:\" : \"CONSIDERANDO\",\n",
    "    \"DEFINICIONES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES FINALES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES GENERALES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES PRELIMINARES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES REGULADORAS\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES RELATIVAS\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES ESPECIALES\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES TRANSITORIAS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES VARIAS Y TRANSITORIAS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES VARIAS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"INCENTIVOS\" : \"INCENTIVOS\",\n",
    "    \"INFRACCIONES\" : \"INFRACCIONES\",\n",
    "    \"INFRACCION ES\" : \"INFRACCIONES\",\n",
    "    \"OBJETIVO\" : \"OBJETO\",\n",
    "    \"OBJETO\" : \"OBJETO\",\n",
    "    \"DERECHOS\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"DEBERES\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACIONES\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACIONE\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACION\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGATORIEDAD\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"PROHIBICIONES\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"PROHIBICION\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"DE LAS FORMAS DE AUTORIZACION\" : \"PERMISOS\",\n",
    "    \"DE LOS PERMISOS Y LAS PATENTES\" : \"PERMISOS\",\n",
    "    \"DE LOS PERMISOS\" : \"PERMISOS\",\n",
    "    \"DE LAS SOLICITUDES DE PERMISOS\" : \"PERMISOS\",\n",
    "    \"DEL OTORGAMIENTO DEL PERMISO\" : \"PERMISOS\",\n",
    "    \"POR TANTO\" : \"POR TANTO\",\n",
    "    \"POR LO TANTO\" : \"POR TANTO\",\n",
    "    \"Decreto:\" : \"RESUELVO\",\n",
    "    \"Resuelvo:\" : \"RESUELVO\",\n",
    "    \"Se resuelve\" : \"RESUELVO\",\n",
    "    \"S e  r e s u e l v e:\" : \"RESUELVO\",\n",
    "    \"R e s u e l v o:\" : \"RESUELVO\",\n",
    "    \"FISCALIZACION Y SANCIONES\" : \"SANCIONES\",\n",
    "    \"DE LAS SANCIONES\" : \"SANCIONES\",\n",
    "    \"Visto:\" : \"VISTO\",\n",
    "    \"Vistos:\" : \"VISTO\",\n",
    "    \"Vistos estos antecedentes:\" : \"VISTO\",\n",
    "    \"--------------\" : \"HEADING\"\n",
    "}\n",
    "section_tags = {\n",
    "    \"Considerando:\" : \"CONSIDERANDO\",\n",
    "    \"Considerando\\n\" : \"CONSIDERANDO\",\n",
    "    \"Decreto:\" : \"RESUELVO\",\n",
    "    \"Resuelvo:\" : \"RESUELVO\",\n",
    "    \"R e s u e l v o:\" : \"RESUELVO\",\n",
    "    \"Se resuelve\" : \"RESUELVO\",\n",
    "    \"S e  r e s u e l v e:\" : \"RESUELVO\",\n",
    "    \"Visto:\" : \"VISTO\",\n",
    "    \"Vistos:\" : \"VISTO\",\n",
    "    \"Vistos estos antecedentes:\" : \"VISTO\",\n",
    "    \"--------------\" : \"HEADING\"\n",
    "}\n",
    "merges_lower = {}\n",
    "for key, value in merges.items():\n",
    "    merges_lower[key.lower()] = value\n",
    "# Eventhough the general gramar rule in Spanish is not to accent uppercase, there are many cases where a word in a heding might\n",
    "# appear accented. This is a dictionary to armonize all headings without accents. The list is rather comprehensive, but there is\n",
    "# still room for improvement.\n",
    "# If we find some bug beyond simple misspelling which will be solved by spell checker, we can include it here. The example is in\n",
    "# the first row with \"ACTIVIDADESUSOS\" which was found several times in headings.\n",
    "bugs = {\"ACTIVIDADESUSOS\" : \"ACTIVIDADES DE USOS\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to the AWS S3 bucket\n",
    "To effectively run this cell you need Omdena's credentials. Please keep them local and do not sync them in GitHub repos nor cloud drives. Before doing anything with this json file, please think of security!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "# json_folder = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "filename = \"Omdena_key_S3.json\"\n",
    "file = json_folder / filename\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    cred = json.load(f) \n",
    "\n",
    "for key in cred:\n",
    "    KEY = key\n",
    "    SECRET = cred[key]\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To clear html tags (here is basically to remove the page tags)\n",
    "cleanr = re.compile(r'<.*?>')\n",
    "\n",
    "# To catch accents and dictionary to change them\n",
    "accents_out = re.compile(r'[áéíóúÁÉÍÓÚ]')\n",
    "accents_dict = {\"á\":\"a\",\"é\":\"e\",\"í\":\"i\",\"ó\":\"o\",\"ú\":\"u\",\"Á\":\"A\",\"É\":\"E\",\"Í\":\"I\",\"Ó\":\"O\",\"Ú\":\"U\"}\n",
    "\n",
    "# To remove special characters\n",
    "clean_special_char = re.compile(r'(\\*\\.)|(\\”\\.)')\n",
    "\n",
    "# To search for acronyms\n",
    "clean_acron = re.compile(r'(A\\s*\\.M\\s*\\.)|(\\b[Aa][Rr][Tt][Ss]*\\s*\\.)|(\\b[Aa][Vv]\\s*\\.)|(\\bDr\\s*\\.)|(\\bIng\\s*\\.)|(\\bLic\\s*\\.)|(\\bLicda\\s*\\.)|(\\bLIC\\s*\\.)|(mm\\s*\\.)|(mts\\s*\\.)|([Oo]rd\\s*\\.)|(\\bNo\\s*\\.)|(P\\s*\\.M\\s*\\.)|(prof\\s*\\.)|(profa\\s*\\.)|(sp\\s*\\.)|(ssp\\s*\\.)|(sr\\s*\\.)|(sra\\s*\\.)|(to\\s*\\.)|(ta\\s*\\.)|(var\\s*\\.)')  \n",
    "\n",
    "# Remove extra white spaces\n",
    "whitespaces = re.compile(r'[ ]{2,}')\n",
    "\n",
    "# Regular expression to clear punctuation from a string\n",
    "clean_punct = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# Regular expression to clear words that introduce unnecessary variability to headings. Some regex still not work 100% we need\n",
    "# to improve them.\n",
    "clean_capitulo = re.compile(r'(APARTADO \\S*)|(APARTADO\\s)|(^ART\\.\\s*\\S*)|(^ART\\.\\s*)|(^Art\\.\\s*\\S*)|(^Art\\.\\s*)|(^Arts\\.\\s*\\S*)|(^[Aa][Rr][Tt][Ii][Cc][Uu][Ll][Oo]\\.*\\s*\\S*\\.-)|(Capítulo \\S*)|(CAPITULO \\S*)|(CAPITULO\\S*)|(CAPÍTULO \\S*)|(CAPITULÓ \\S*)|(CAPITULOS \\S*)|(CAPITUO \\S*)|(CATEGORIA\\b)|(CATEGORÍA\\b)|(SUBCATEGORIA\\b)|(SUBCATEGORÍA\\b)|(TITULO\\s\\S*)|(TÍTULO\\s\\S*)')\n",
    "clean_bullet_char = re.compile(r'\\b[A-Za-z]\\s*\\.|\\b[A-Za-z]\\s*\\.\\s*|\\b[A-Za-z]\\s*\\-\\s*|\\b[A-Za-z]\\s*\\)\\s*|\\.\\s*\\b[B-Za-z]\\b|\\b[A-Z]{1,4}\\s*\\.')\n",
    "clean_bullet_number = re.compile(r'^\\d+\\s*\\.\\s*-|^\\d+\\s*\\.\\s*\\d*|^\\d+\\s*\\.\\s*[A-Za-z]\\s|^\\d+\\)')\n",
    "clean_bullet_point = re.compile(r'^-\\s*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the uppercase ratio in a string. It is used to detect section headings\n",
    "def uppercase_ratio(string):\n",
    "    if len(re.findall(r'[a-z]',string)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return(len(re.findall(r'[A-Z]',string))/len(re.findall(r'[a-z]',string)))\n",
    "\n",
    "def end_of_heading(line, flag, content):\n",
    "    if \"URL\" in line and \"https:\" in line:\n",
    "        flag = False\n",
    "        content = False\n",
    "        return flag, content\n",
    "    else:\n",
    "        return flag, content\n",
    "\n",
    "def is_section(line):\n",
    "    section = False\n",
    "    for key in section_tags:\n",
    "        if key in line:\n",
    "            section = True\n",
    "            break\n",
    "    return section\n",
    "            \n",
    "def end_of_document(line):\n",
    "    end_of_file = False\n",
    "    for key in end_of_file_tags:\n",
    "        if key in line:\n",
    "            end_of_file = True\n",
    "            break\n",
    "    return end_of_file\n",
    "\n",
    "# Te function to clear html tags\n",
    "def clean_html_tags(string):\n",
    "  return cleanr.sub('', string)\n",
    "    \n",
    "def is_por_tanto(line):\n",
    "    if \"POR TANTO\" in line:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to remove the last lines of a document, the ones that contain the signatures of the officials. It depends on the\n",
    "# dictionary \"official_positions\"\n",
    "def remove_signatures(line):\n",
    "    signature = False\n",
    "    for key in official_positions:\n",
    "        if key in line:\n",
    "            signature = True\n",
    "            break\n",
    "    return signature\n",
    "\n",
    "# Function to change accented words by non-accented counterparts. It depends on the dictionary \"accent_marks_bugs\" \n",
    "def remove_accents(string):\n",
    "    for accent in accents_out.findall(string):\n",
    "        string = string.replace(accent, accents_dict[accent])\n",
    "    return string\n",
    "\n",
    "# Function to merge headlines expressing the same concept in different words. It depends on the dictionary \"merges\"\n",
    "def merge_concepts(line):\n",
    "    for key in merges:\n",
    "        if key in line:\n",
    "            line = merges[key]\n",
    "            break\n",
    "    return line\n",
    "\n",
    "def clean_bugs(line):\n",
    "    for key in bugs:\n",
    "        if key in line:\n",
    "            line = line.replace(key, bugs[key])\n",
    "    return line\n",
    "\n",
    "def clean_special_characters(line):\n",
    "    char = clean_special_char.findall(line)\n",
    "    for item in char:\n",
    "        for character in item:\n",
    "            if character != '':\n",
    "                line = line.replace(character, \"\")\n",
    "    return line\n",
    "\n",
    "def clean_acronyms(line):\n",
    "    acro = clean_acron.findall(line)\n",
    "    for item in acro:\n",
    "        for acronym in item:\n",
    "            if acronym != '':\n",
    "                line = line.replace(acronym, clean_punct.sub('', acronym))\n",
    "    return line\n",
    "\n",
    "def clean_whitespace(line):\n",
    "    if whitespaces.sub(' ', line).rstrip().lstrip() != None:\n",
    "        return whitespaces.sub(' ', line).rstrip().lstrip()\n",
    "    else:\n",
    "        return line\n",
    "\n",
    "decimal_points = re.compile(r'(\\b\\d+\\s*\\.\\s*\\d+)')\n",
    "def change_decimal_points(line):\n",
    "    dec = decimal_points.findall(line)\n",
    "    for decimal in dec:\n",
    "        if decimal != '':\n",
    "#             print(decimal)\n",
    "            line = line.replace(decimal, clean_punct.sub(',', decimal))\n",
    "    return line\n",
    "                \n",
    "# Function sentence\n",
    "def clean_sentence(string):\n",
    "    string = clean_capitulo.sub('', string)\n",
    "    string = clean_bullet_char.sub('', string).rstrip().lstrip()\n",
    "    string = clean_bullet_number.sub('', string).rstrip().lstrip()\n",
    "    string = clean_bullet_point.sub('', string).rstrip().lstrip()\n",
    "    if string != \"\":\n",
    "        return string\n",
    "    else:\n",
    "        return \"\"    \n",
    "    \n",
    "# points = re.compile(r'(\\b\\w+\\s*\\.\\s*\\b[^\\d\\W]+)')\n",
    "# def check_points(line):\n",
    "#     return points.findall(line)\n",
    "#     print(points.findall(line))\n",
    "\n",
    "points = re.compile(r'(\\b\\w+\\b\\s*){3,}')\n",
    "def check_sentence(line):\n",
    "    if points.findall(line):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def split_into_sentences(line, sep):\n",
    "    sentence_list = []\n",
    "    for sentence in line.split(sep):\n",
    "        if check_sentence(sentence):\n",
    "            sentence = sentence.rstrip().lstrip()\n",
    "            sentence_list.append(sentence)\n",
    "    return sentence_list\n",
    "\n",
    "# Function to add items to the dictionary with duplicate removal\n",
    "def add_to_dict(string, dictionary, dupl_dict):\n",
    "    if string in dupl_dict or string == None:\n",
    "        pass\n",
    "    else:\n",
    "        dupl_dict[string] = 0\n",
    "        if string in dictionary:\n",
    "            dictionary[string] = dictionary[string] + 1\n",
    "        else:\n",
    "            dictionary[string] = 1\n",
    "    return dictionary\n",
    "def full_cleaning(line):\n",
    "    line = clean_html_tags(line)\n",
    "    line = remove_accents(line)\n",
    "    line = clean_special_characters(line)\n",
    "    line = clean_bugs(line)\n",
    "    line = clean_acronyms(line)\n",
    "    line = clean_whitespace(line)\n",
    "#     print(line)\n",
    "    line = clean_sentence(line)\n",
    "#     print(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"Que el Art. 204 Ordinal 3*. y 5”. de la Constitución, regula. A. Hola, em dic Jordi. B. No sé massa perquè l'Art *. 22 conté 22.34€. Tanmateix sembla que la Licda. una cosa. voldria  55.22. no fotis\"\n",
    "# test_string = \"Prova senzilleta per veure què passa si no hi ha punt\"\n",
    "test_string = clean_sentence(test_string)\n",
    "test_string = clean_special_characters(test_string)\n",
    "test_string = clean_acronyms(test_string)\n",
    "test_string = clean_whitespace(test_string)\n",
    "test_string = change_decimal_points(test_string)\n",
    "print(test_string)\n",
    "sentences = []\n",
    "[sentences.append(sentence) for sentence in split_into_sentences(test_string)]\n",
    "print(sentences)\n",
    "# print(sentences)\n",
    "\n",
    "# if check_sentence(test_string):\n",
    "#     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to process files from S3 bucket\n",
    "By executing this cell you will go through all policies in El Salvador and process section headings that will be saved in a dictionary. This should be merged with the notebook that builds up the final json files out of plain txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = \"text-extraction/\"\n",
    "out_folder = \"JSON/\"\n",
    "counter = 0\n",
    "name4 = {}\n",
    "name5 = {}\n",
    "name6 = {}\n",
    "name7 = {}\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix='text-extraction'):\n",
    "    if in_folder in obj.key and obj.key.replace(in_folder, \"\") != \"\":# and filename in obj.key   # Un comment the previous string to run the code just in one sample document.\n",
    "        file = obj.get()['Body'].read().decode('utf-8')  #get the file from S3 and read the body content\n",
    "        lines = file.split(\"\\n\") # Split by end of line and pipe lines into a list\n",
    "        file_name = obj.key.replace(in_folder, \"\").replace('.pdf.txt', '')        \n",
    "        name4[file_name[0:4]] = 0\n",
    "        name5[file_name[0:5]] = 0\n",
    "        name6[file_name[0:6]] = 0\n",
    "        name7[file_name[0:7]] = 0\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter)\n",
    "print(len(name4))\n",
    "print(len(name5))\n",
    "print(len(name6))\n",
    "print(len(name7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = \"Chile/full/\"\n",
    "out_folder = \"JSON/\"\n",
    "# filename = \"00a55afe4f55256567397a68df5d7f97e642480b\" # This is only if you want to test on a single file\n",
    "# bag_of_words = {}\n",
    "# sentences = []\n",
    "# sentences_dict = {}\n",
    "json_file = {}\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix='Chile/full/'):\n",
    "    if in_folder in obj.key and obj.key.replace(in_folder, \"\") != \"\":# and filename in obj.key   # Un comment the previous string to run the code just in one sample document.\n",
    "        file = obj.get()['Body'].read().decode('utf-8')  #get the file from S3 and read the body content\n",
    "        lines = file.split(\"\\n\") # Split by end of line and pipe lines into a list\n",
    "        key = obj.key.replace(in_folder, out_folder).replace('pdf.txt', 'json')\n",
    "        filename = key.replace('.json', '').replace(out_folder, '')\n",
    "        print(filename)\n",
    "        json_file[filename] = {}\n",
    "        line_counter = 0\n",
    "        heading_flag = True\n",
    "        heading_content = False\n",
    "        has_section = False\n",
    "        json_file[filename][\"HEADING\"] = {\"tags\" : [], \"sentences\" : {}}\n",
    "        for line in lines:\n",
    "            line = clean_whitespace(line)\n",
    "            # Processing document heading\n",
    "            if heading_flag:\n",
    "                if \"Tipo Norma\" in line:\n",
    "                    heading_content = True\n",
    "                if heading_content:\n",
    "                    line = full_cleaning(line)\n",
    "                    if line != None:\n",
    "                        if \":\" in line:\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id] = {\"text\" : line, \"labels\" : []}\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = line\n",
    "                        else:\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] + \" \" + line\n",
    "#                 print(\"**\", line)\n",
    "                heading_flag, heading_content, line_counter = end_of_heading(line, heading_flag, heading_content, line_counter)\n",
    "\n",
    "            # Breaking when document signatures are found    \n",
    "            elif end_of_document(line):\n",
    "#                 print(line)\n",
    "                break\n",
    "            # Getting section headings\n",
    "            elif (uppercase_ratio(line) == 1 and len(line) > 10 and line_counter > 0) or is_section(line):\n",
    "#                 print(\"line--\", line)\n",
    "                line = remove_accents(line)\n",
    "                line = clean_bugs(line)\n",
    "                line = clean_sentence(line)\n",
    "                if line == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    has_section = True\n",
    "                    section = merge_concepts(line)\n",
    "#                     print(\"**\", section)\n",
    "                    json_file[filename][section] = {\"tags\" : [], \"sentences\" : {}}\n",
    "                    bag_of_words = add_to_dict(section, bag_of_words, duplicates_dict)\n",
    "                    if section == \"VISTO\" and len(split_into_sentences(line, \":\")) > 1:\n",
    "                        visto = split_into_sentences(line, \":\")\n",
    "                        for sentence in split_into_sentences(visto[1], \";\"):\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "\n",
    "            elif has_section:\n",
    "                line = full_cleaning(line)\n",
    "                if line == None:\n",
    "                    continue                    \n",
    "                else:\n",
    "                    line = change_decimal_points(line)\n",
    "                    for sentence in split_into_sentences(line, \".\"):\n",
    "                        line_counter += 1\n",
    "                        sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                        json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "#         s3.Object('wri-latin-talent', key).put(Body = str(json.dumps(json_file)))#This will save all the contents in the string variable \"content\" into a txt file in the Pre-processed folder\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Processed/\")\n",
    "filename = \"Chile.json\"\n",
    "file = out_folder / filename\n",
    "with open(file, 'w') as fp:\n",
    "    json.dump(json_file, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(json_file))\n",
    "# for k in sorted(sentences_dict):\n",
    "#     print(k, \":\", sentences_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After preprocessing there are {} different headings in El Salvador policies\".format(len(bag_of_words)))\n",
    "print(\"{} documents have been processed\".format(i))\n",
    "print(\"There are {} lines of text as sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict( sorted(bag_of_words.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by heading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bag_of_words):\n",
    "    print(k, \":\", bag_of_words[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving sentences as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/\")\n",
    "path = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/\")\n",
    "filename = \"sentences.npy\"\n",
    "file = path / filename\n",
    "np_sentences = np.array(sentences)\n",
    "with open(file, 'wb') as f:\n",
    "    np.save(f, np_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to process one file from HD folder\n",
    "This is a pipeline to process a test file in a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "files = os.listdir(path)\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "data_folder = Path(path)\n",
    "filename = \"00d91bcac2667290e3fb42452f5241be28b031c2\"\n",
    "\n",
    "\n",
    "files = os.listdir(path)\n",
    "files = [\"002c53058e85d383b057fa4cc25a6eb8e7d401e3\"]\n",
    "\n",
    "bag_of_words = {}\n",
    "json_file = {}\n",
    "\n",
    "i = 0\n",
    "for filename in files:\n",
    "#     if i == 0:\n",
    "    file_ = data_folder / filename\n",
    "    with open(file_, 'r', encoding = 'utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "#         print(\"\\n\", filename, \"\\n\")\n",
    "        json_file[filename] = {}\n",
    "        line_counter = 0\n",
    "        heading_flag = True\n",
    "        heading_content = False\n",
    "        has_section = False\n",
    "        json_file[filename][\"HEADING\"] = {\"tags\" : [], \"sentences\" : {}}\n",
    "        for line in lines:\n",
    "            line = clean_whitespace(line)\n",
    "            # Processing document heading\n",
    "            if heading_flag:\n",
    "                if \"Tipo Norma\" in line:\n",
    "                    heading_content = True\n",
    "                if heading_content:\n",
    "                    line = full_cleaning(line)\n",
    "                    if line != None:\n",
    "                        if \":\" in line:\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id] = {\"text\" : line, \"labels\" : []}\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = line\n",
    "                        else:\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] + \" \" + line\n",
    "#                 print(\"**\", line)\n",
    "                heading_flag, heading_content = end_of_heading(line, heading_flag, heading_content)\n",
    "\n",
    "            # Breaking when document signatures are found    \n",
    "            elif end_of_document(line):\n",
    "#                 print(line)\n",
    "                break\n",
    "            # Getting section headings\n",
    "            elif (uppercase_ratio(line) == 1 and len(line) > 10 and line_counter > 0) or is_section(line):\n",
    "#                 print(\"line--\", line)\n",
    "                line = remove_accents(line)\n",
    "                line = clean_bugs(line)\n",
    "                line = clean_sentence(line)\n",
    "                if line == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    has_section = True\n",
    "                    section = merge_concepts(line)\n",
    "#                     print(\"**\", section)\n",
    "                    json_file[filename][section] = {\"tags\" : [], \"sentences\" : {}}\n",
    "#                     bag_of_words = add_to_dict(section, bag_of_words, duplicates_dict)\n",
    "                    if section == \"VISTO\" and len(split_into_sentences(line, \":\")) > 1:\n",
    "                        visto = split_into_sentences(line, \":\")\n",
    "                        for sentence in split_into_sentences(visto[1], \";\"):\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "\n",
    "            elif has_section:\n",
    "                line = full_cleaning(line)\n",
    "                if line == None:\n",
    "                    continue                    \n",
    "                else:\n",
    "                    line = change_decimal_points(line)\n",
    "                    for sentence in split_into_sentences(line, \".\"):\n",
    "                        line_counter += 1\n",
    "                        sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                        json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "        i += 1\n",
    "    #     data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'002c53058e85d383b057fa4cc25a6eb8e7d401e3': {'HEADING': {'tags': [],\n",
       "   'sentences': {'002c530_1': {'text': 'Tipo Norma :Decreto 3157 EXENTO',\n",
       "     'labels': []},\n",
       "    '002c530_2': {'text': 'Fecha Publicacion :16-09-2016', 'labels': []},\n",
       "    '002c530_3': {'text': 'Fecha Promulgacion :18-08-2016', 'labels': []},\n",
       "    '002c530_4': {'text': 'Organismo :MUNICIPALIDAD DE PANQUEHUE',\n",
       "     'labels': []},\n",
       "    '002c530_5': {'text': 'Titulo :APRUEBA \"ORDENANZA PARA LA EXTRACCION DE ARIDOS EN CAUCES Y ALVEOS DE CURSOS NATURALES DE AGUA QUE CONSTITUYEN BIENES NACIONALES DE USO PUBLICO Y EN POZOS LASTREROS DE PROPIEDAD PARTICULAR EN LA COMUNA DE PANQUEHUE\" Y SUS RESPECTIVOS ANEXOS',\n",
       "     'labels': []},\n",
       "    '002c530_6': {'text': 'Tipo Version :Unica De : 16-SEP-2016',\n",
       "     'labels': []},\n",
       "    '002c530_7': {'text': 'Inicio Vigencia :16-09-2016', 'labels': []},\n",
       "    '002c530_8': {'text': 'Id Norma :1094879', 'labels': []},\n",
       "    '002c530_9': {'text': 'URL :https://www.leychile.cl/N?i=1094879&f=2016-09-16',\n",
       "     'labels': []}}},\n",
       "  'APRUEBA \"ORDENANZA PARA LA EXTRACCION DE ARIDOS EN CAUCES Y ALVEOS DE CURSOS NATURALES DE AGUA QUE CONSTITUYEN BIENES NACIONALES DE USO PUBLICO Y EN POZOS LASTREROS DE PROPIEDAD PARTICULAR EN LA COMUNA DE PANQUEHUE\" Y SUS RESPECTIVOS ANEXOS': {'tags': [],\n",
       "   'sentences': {'002c530_10': {'text': '3,157 exento- Panquehue, 18 de agosto de 2016',\n",
       "     'labels': []}}},\n",
       "  'VISTO': {'tags': [], 'sentences': {}},\n",
       "  'CONSIDERANDO': {'tags': [],\n",
       "   'sentences': {'002c530_17': {'text': 'Decreto N° 3,157, de fecha 18 de agosto de 2016, que aprueba la Ordenanza para la Extraccion de Aridos en Cauces y Alveos de cursos naturales de agua que constituyen Bienes Nacionales de Uso Publico y en Pozos Lastreros de propiedad particular, de la comuna de Panquehue, como a continuacion se indica',\n",
       "     'labels': []}}},\n",
       "  'RESUELVO': {'tags': [],\n",
       "   'sentences': {'002c530_15': {'text': 'Apruebese la \"Ordenanza para la extraccion de Aridos en Cauces y Alveos de Cursos Naturales de Agua que constituyen Bienes Nacionales de Uso Publico y en Pozos Lastreros de Propiedad Particular en la comuna de Panquehue\" y sus respectivos anexos',\n",
       "     'labels': []}}},\n",
       "  'ORDENANZA PARA LA EXTRACCION DE ARIDOS EN CAUCES Y ALVEOS DE CURSOS NATURALES DE AGUA QUE CONSTITUYEN BIENES NACIONALES DE USO PUBLICO Y EN POZOS LASTREROS DE PROPIEDAD PARTICULAR, DE LA COMUNA DE PANQUEHUE': {'tags': [],\n",
       "   'sentences': {'002c530_16': {'text': 'Panquehue, 18 de agosto de 2016',\n",
       "     'labels': []}}},\n",
       "  'OBJETO': {'tags': [],\n",
       "   'sentences': {'002c530_18': {'text': 'La presente Ordenanza tiene como objetivo fijar los procedimientos generales para la extraccion de aridos ya sea de Bienes Nacionales de Uso Publico y/o Pozos Lastreros en propiedad particular, reseñados en esta Ordenanza para la comuna de Panquehue',\n",
       "     'labels': []},\n",
       "    '002c530_19': {'text': 'De conformidad al articulo 36° de la Ley 18,695 de la Ley Organica Constitucional de Municipalidades, los Bienes Municipales o Nacionales de Uso Publico, incluido su subsuelo, que administre la Municipalidad, podran ser objeto de permisos',\n",
       "     'labels': []},\n",
       "    '002c530_20': {'text': 'Por consiguiente, para extraer aridos de los cauces de los rios, esteros y/o pozos lastreros, se requiere previamente el otorgamiento de un permiso municipal, el cual sera visado tecnicamente por la Direccion de Obras Hidraulicas del Ministerio de Obras Publicas o la institucion que lo supla en sus funciones',\n",
       "     'labels': []},\n",
       "    '002c530_21': {'text': 'Quedaran comprendidos en esta Ordenanza los siguientes Bienes Nacionales de Uso Publico, de administracion municipal, que en toda su extension corresponden a los cauces, dentro del territorio de la comuna de Panquehue:',\n",
       "     'labels': []},\n",
       "    '002c530_22': {'text': 'Otros esteros y cursos de aguas de la comuna',\n",
       "     'labels': []},\n",
       "    '002c530_23': {'text': 'Para definir el area y extension del cauce, se estara a lo dispuesto en el articulo 30° del Codigo de Aguas',\n",
       "     'labels': []}}},\n",
       "  'DERECHOS, OBLIGACIONES Y PROHIBICIONES': {'tags': [],\n",
       "   'sentences': {'002c530_109': {'text': 'Son obligaciones del beneficiario del permiso, las siguientes:',\n",
       "     'labels': []},\n",
       "    '002c530_110': {'text': 'Materializar en terreno los vertices del proyecto mediante monolitos de hormigon, ademas dejar estacas en cada uno de los perfiles transversales del proyecto',\n",
       "     'labels': []},\n",
       "    '002c530_111': {'text': 'Las excavaciones deben situarse dentro del cauce',\n",
       "     'labels': []},\n",
       "    '002c530_112': {'text': 'Por ningun motivo deben situarse en las riberas, pues contribuyen a debilitar su compactacion y estabilidad',\n",
       "     'labels': []},\n",
       "    '002c530_113': {'text': 'Las excavaciones deben ser orientadas en direccion paralela al eje del cauce',\n",
       "     'labels': []},\n",
       "    '002c530_114': {'text': 'Por ningun motivo deben orientarse en direccion transversal al eje del cauce (ribera a ribera) seccionandolo',\n",
       "     'labels': []},\n",
       "    '002c530_115': {'text': 'Las excavaciones no deben vulnerar, ni amagar obras de encauzamiento y defensas existentes (enrocado - gaviones), canales, bocatomas de regadio y otras',\n",
       "     'labels': []},\n",
       "    '002c530_116': {'text': 'El material excedente debera situarse en la ribera, conformando un pretil',\n",
       "     'labels': []},\n",
       "    '002c530_117': {'text': 'La explotacion de aridos no debe perjudicar a terceros, ni dificultar el libre escurrimiento de las aguas, por ningun motivo se debe procesar el material dentro del cauce, como tampoco hacer acopio dentro de el',\n",
       "     'labels': []},\n",
       "    '002c530_118': {'text': 'La extraccion debe obedecer a un perfil de escurrimiento de tipo \"trapezoidal\"',\n",
       "     'labels': []},\n",
       "    '002c530_119': {'text': 'Por ningun motivo se podra excavar bajo la cota de fondo que se señala en el perfil longitudinal del proyecto El ancho basal del perfil de explotacion no debe exceder la dimension indicada en el proyecto',\n",
       "     'labels': []},\n",
       "    '002c530_120': {'text': 'Todo daño de cualquier naturaleza que con motivo de la ejecucion de los trabajos autorizados, le cause a terceros o a cualquier obra de infraestructura, por negligencia, incumplimiento, o errores en la manipulacion del cauce, sera de exclusiva responsabilidad del beneficiario del permiso',\n",
       "     'labels': []},\n",
       "    '002c530_121': {'text': 'La zona especificada de extraccion, longitud y ancho estara medido por el eje del cauce',\n",
       "     'labels': []},\n",
       "    '002c530_122': {'text': 'Al termino de las faenas de extraccion de aridos, se exigira efectuar faenas de encauzamiento del rio, a fin de eliminar todo obstaculo que obstruya el libre escurrimiento de las aguas',\n",
       "     'labels': []},\n",
       "    '002c530_123': {'text': 'Le queda estrictamente prohibido realizar extracciones de aridos a menos de 300 metros de un puente, ya sea aguas arriba como aguas abajo',\n",
       "     'labels': []},\n",
       "    '002c530_124': {'text': 'Queda estrictamente prohibido a beneficiarios de permisos, obstruir el libre transito de personas, vehiculos y animales por el Bien Nacional de Uso Publico respectivo, las cercas e instalaciones que se permitiran lo seran unicamente cuanto su existencia se justifique y no perturbe el uso y goce del Bien Nacional de Uso Publico, autorizacion que se solicitara al Director de Obras Municipales',\n",
       "     'labels': []}}},\n",
       "  'PERMISOS': {'tags': [],\n",
       "   'sentences': {'002c530_64': {'text': 'Se otorgara permiso para extraer aridos en forma artesanal a determinadas personas naturales, solo frente a situaciones sociales y tecnicamente calificadas, y que tengan capacidad de extraer no mas de quince metros cubicos (15m3) de aridos al mes',\n",
       "     'labels': []},\n",
       "    '002c530_65': {'text': 'Estas personas tambien seran inscritas en el registro respectivo, para su posterior control; la referencia de esta extraccion sera hecha por el Municipio',\n",
       "     'labels': []},\n",
       "    '002c530_66': {'text': 'Sera a traves de un informe de la Dideco que determinara la situacion social del interesado',\n",
       "     'labels': []}}},\n",
       "  'SANCIONES': {'tags': [],\n",
       "   'sentences': {'002c530_80': {'text': 'El permiso, materia de la presente Ordenanza se extinguira, en conformidad con la Ley 18,695, Organica Constitucional de Municipalidades, por las siguientes causales:',\n",
       "     'labels': []},\n",
       "    '002c530_81': {'text': 'Cumplimiento del plazo por el que se otorgo;',\n",
       "     'labels': []},\n",
       "    '002c530_82': {'text': 'Incumplimiento grave de las obligaciones impuestas al beneficiario de un permiso, o;',\n",
       "     'labels': []},\n",
       "    '002c530_83': {'text': 'Mutuo acuerdo entre la Municipalidad y beneficiario de permiso, cuando ocurran razones de interes publico, calificados por la Municipalidad',\n",
       "     'labels': []},\n",
       "    '002c530_84': {'text': 'Todas aquellas faltas graves a la presente Ordenanza, relacionadas principalmente con detrimento del patrimonio municipal y/o publico, en terminos de percepcion de ingresos o de materias tecnicas que signifiquen intervencion no autorizada del cauce y de su entorno, tales como:',\n",
       "     'labels': []},\n",
       "    '002c530_85': {'text': 'El no pago oportuno de los derechos municipales en los casos que corresponda',\n",
       "     'labels': []},\n",
       "    '002c530_86': {'text': 'Transportar aridos sin guia de despacho o factura correspondiente',\n",
       "     'labels': []},\n",
       "    '002c530_87': {'text': 'Utilizar una misma guia de despacho o factura para diferentes volumenes de aridos transportados',\n",
       "     'labels': []},\n",
       "    '002c530_88': {'text': 'Realizar excavaciones o acopios de material en lugares expresamente prohibidos',\n",
       "     'labels': []},\n",
       "    '002c530_89': {'text': 'Por incumplimiento de los planes tecnicos señalados en el Informe Tecnico aprobatorio de la Direccion de Obras Hidraulicas presentados para la obtencion del permiso',\n",
       "     'labels': []},\n",
       "    '002c530_90': {'text': 'Extraccion de una cantidad mayor de material que el autorizado',\n",
       "     'labels': []},\n",
       "    '002c530_91': {'text': 'Por maltratar de cualquier forma al personal municipal que fiscalice',\n",
       "     'labels': []},\n",
       "    '002c530_92': {'text': 'Por extraer material sin autorizacion alguna para aquello',\n",
       "     'labels': []},\n",
       "    '002c530_93': {'text': 'y cualquier otra transgresion de la ley 11,402, articulo 11° y consideraciones tecnicas de la Direccion de Obras Hidraulicas',\n",
       "     'labels': []},\n",
       "    '002c530_94': {'text': 'Seran sancionadas con la suspension temporal de los permisos de extraccion por treinta dias y en caso de reiteracion de estas faltas se podra poner termino al permiso de extraccion',\n",
       "     'labels': []},\n",
       "    '002c530_95': {'text': 'La falta indicada en la letra sera sancionada con la caducidad del permiso de extraccion',\n",
       "     'labels': []},\n",
       "    '002c530_96': {'text': 'La falta indicada en la letra sera denunciada al Juzgado de Policia Local aplicandosele la multa maxima; la reiteracion del beneficiario, dara lugar a la peticion de fuerza publica al Gobernador Provincial para dar cumplimiento forzado de la clausura',\n",
       "     'labels': []},\n",
       "    '002c530_97': {'text': 'Las infracciones señaladas en este articulo seran sancionadas con multa unica de cincuenta Unidades Tributarias Mensuales por cada falta que cometa el beneficiario de un permiso',\n",
       "     'labels': []},\n",
       "    '002c530_98': {'text': 'No obstante lo anterior, ante faltas que se mencionan a continuacion, dara lugar a multa que no podra exceder de la cinco Unidades Tributarias Mensuales, en conformidad con la Ley 18,695 Organica Constitucional de Municipalidades, faltas que incluso podran originar la suspension de la calidad del beneficiario de un permiso, en caso de reiteracion comprobada',\n",
       "     'labels': []},\n",
       "    '002c530_99': {'text': 'Dichas faltas son las siguientes:', 'labels': []},\n",
       "    '002c530_100': {'text': 'Por no otorgamiento de las guias de control de aridos entregadas por la Municipalidad',\n",
       "     'labels': []},\n",
       "    '002c530_101': {'text': 'Por estar ejerciendo el permiso una persona distinta al titular del mismo',\n",
       "     'labels': []},\n",
       "    '002c530_102': {'text': 'Por no aportar informacion veraz respecto de la cantidad de arido extraido',\n",
       "     'labels': []},\n",
       "    '002c530_103': {'text': 'Personal de la Direccion de Obras Municipales e Inspectoria Municipal, revisara permanentemente la faena en terreno, verificara si el beneficiario del permiso otorgado ha dado cumplimiento con los compromisos de encauzamiento y mantencion del rio, y de las obligaciones que le son exigidas en el proyecto autorizado, en caso contrario, se negara cualquier nuevo permiso, sin perjuicio de las demas sanciones establecidas en este reglamento o leyes especiales que regulen la materia, de todo lo cual se informara a la Direccion de Obras Hidraulicas para los fines pertinentes',\n",
       "     'labels': []},\n",
       "    '002c530_104': {'text': 'Para los efectos administrativos, la Direccion de Obras Municipales llevara un Registro Publico de los permisos que se otorguen para la extraccion de aridos',\n",
       "     'labels': []},\n",
       "    '002c530_105': {'text': 'Este Registro sera ordenado numericamente y correspondera al numero del decreto alcaldicio, en el orden que sean otorgados los permisos',\n",
       "     'labels': []},\n",
       "    '002c530_106': {'text': 'Con el objeto de proteger el medio ambiente y de cautelar los intereses municipales, la Ilustre Municipalidad implementara un sistema de fiscalizacion de los procedimientos tecnicos de extraccion y del volumen de aridos extraidos revisando las guias de despacho y las facturas emitidas por el beneficiario de un permiso',\n",
       "     'labels': []},\n",
       "    '002c530_107': {'text': 'La presente Ordenanza queda sujeta a la fiscalizacion de la Direccion General de Aguas, Direccion de Obras Hidraulicas, Direccion de Obras Municipales, los inspectores municipales y carabineros de Chile, quienes podran recibir las denuncias, o en su caso denunciar el incumplimiento de las obligaciones y prohibiciones establecidas en la presente Ordenanza',\n",
       "     'labels': []},\n",
       "    '002c530_108': {'text': 'El Tribunal competente para conocer de las infracciones a esta Ordenanza sera el Juzgado de Policia Local de la comuna de Panquehue',\n",
       "     'labels': []}}},\n",
       "  'DISPOSICIONES GENERALES': {'tags': [],\n",
       "   'sentences': {'002c530_125': {'text': 'La presente Ordenanza debera ser aprobada por el Concejo Municipal, igual procedimiento regira para sus modificaciones',\n",
       "     'labels': []},\n",
       "    '002c530_126': {'text': 'Esta Ordenanza regira a contar de su promulgacion, previa publicacion en el Diario Oficial',\n",
       "     'labels': []},\n",
       "    '002c530_127': {'text': 'Esta Ordenanza tendra aplicacion en todo lo que no contravenga lo establecido en el articulo 10° letra de la ley 19,300 y sus modificaciones posteriores',\n",
       "     'labels': []},\n",
       "    '002c530_128': {'text': 'Los permisos constituidos con anterioridad a esta Ordenanza, se sujetaran a sus preceptos, sin embargo en lo relativo al  \"DEL TERMINO DEL PERMISO, DE LAS SANCIONES Y DE LOS PROCEDIMIENTOS DE FISCALIZACION\" se aplicara In Actum',\n",
       "     'labels': []}}}}}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict( sorted(bag_of_words.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by heading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bag_of_words):\n",
    "    print(k, \":\", bag_of_words[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "334.922px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
