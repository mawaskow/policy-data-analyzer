{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Headings Curation and sentences spliter of Chilean Policies\n",
    "\n",
    "In this notebook there are a series of dictionaries and methods to curate section headings of El Salvador policies. Policies from El Salvador have a rather definite structure, so that the law text is organized under section headings. There are two kinds of sections, the ones that are general and that can be often found in many policies, and the ones which are specific. The sections headings which are more general often come with a whole range of name variants which makes the task of machine recognition difficult.\n",
    "\n",
    "The goal of this notebook is to group all pretreatment methods that would harmonize sections heading to make the further processing machine friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import boto3, json, operator, os, re, string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries of particular vocabularies to help in the curation of section headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most policies come with the final signatures. This is a piece of text that we want to be able to recognize. To make the\n",
    "# detection of signatures easier, this dictionary contain the most common terms that can be found in these lines of text.\n",
    "official_positions = {\"ALCALDE\" : 0,\n",
    "\"Alcalde\" : 0,\n",
    "\"MINISTRA\" : 0,\n",
    "\"Ministra\" : 0,\n",
    "\"MINISTRO\" : 0,\n",
    "\"Ministro\" : 0,\n",
    "\"PRESIDENTA\" : 0,\n",
    "\"Presidenta\" : 0,\n",
    "\"PRESIDENTE\" : 0,\n",
    "\"Presidente\" : 0,\n",
    "\"REGIDOR\" : 0,\n",
    "\"Regidor\"  : 0,\n",
    "\"REGIDORA\" : 0,\n",
    "\"regidora\" : 0,\n",
    "\"SECRETARIA\" : 0,\n",
    "\"Secretaria\" : 0,\n",
    "\"SECRETARIO\" : 0,\n",
    "\"Secretario\" : 0,\n",
    "\"SINDICA\" : 0,\n",
    "\"Sindica\" : 0,\n",
    "\"SINDICO\" : 0,\n",
    "\"Sindico\" : 0,\n",
    "\"VICEPRESIDENTA\" : 0,\n",
    "\"Vicepresidenta\" : 0,\n",
    "\"VICEPRESIDENTE\" : 0,\n",
    "\"Vicepresidente\" : 0\n",
    "}\n",
    "\n",
    "end_of_file_tags = {\n",
    "    \"Anótese\" : 0,\n",
    "    \"Anotese\" : 0,\n",
    "    \"Publíquese\" : 0,\n",
    "    \"Publiquese\" : 0\n",
    "}\n",
    "# This dictionary contains some correspondences among different text headings. This is under development and needs further\n",
    "# improvement.The idea is to merge in a single name all the headings that point to the same conceptual concept. For example,\n",
    "# \"Definiciones\" is a heading that can come alone or together with other terms so it can appear as \"Definiciones básicas\" or\n",
    "# \"Definiciones generales\". With the dictionary we can fetch all headings that contain the word \"Definiciones\" and change the\n",
    "# heading to \"Definiciones\".\n",
    "merges = {\n",
    "    \"CONCEPTOS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"Considerando:\" : \"CONSIDERANDO\",\n",
    "    \"DEFINICIONES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES FINALES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES GENERALES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES PRELIMINARES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES REGULADORAS\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES RELATIVAS\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES ESPECIALES\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES TRANSITORIAS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES VARIAS Y TRANSITORIAS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES VARIAS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"INCENTIVOS\" : \"INCENTIVOS\",\n",
    "    \"INFRACCIONES\" : \"INFRACCIONES\",\n",
    "    \"INFRACCION ES\" : \"INFRACCIONES\",\n",
    "    \"OBJETIVO\" : \"OBJETO\",\n",
    "    \"OBJETO\" : \"OBJETO\",\n",
    "    \"DERECHOS\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"DEBERES\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACIONES\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACIONE\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACION\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGATORIEDAD\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"PROHIBICIONES\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"PROHIBICION\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"DE LAS FORMAS DE AUTORIZACION\" : \"PERMISOS\",\n",
    "    \"DE LOS PERMISOS Y LAS PATENTES\" : \"PERMISOS\",\n",
    "    \"DE LOS PERMISOS\" : \"PERMISOS\",\n",
    "    \"DE LAS SOLICITUDES DE PERMISOS\" : \"PERMISOS\",\n",
    "    \"DEL OTORGAMIENTO DEL PERMISO\" : \"PERMISOS\",\n",
    "    \"POR TANTO\" : \"POR TANTO\",\n",
    "    \"POR LO TANTO\" : \"POR TANTO\",\n",
    "    \"Decreto:\" : \"RESUELVO\",\n",
    "    \"Resuelvo:\" : \"RESUELVO\",\n",
    "    \"Se resuelve\" : \"RESUELVO\",\n",
    "    \"S e  r e s u e l v e:\" : \"RESUELVO\",\n",
    "    \"R e s u e l v o:\" : \"RESUELVO\",\n",
    "    \"FISCALIZACION Y SANCIONES\" : \"SANCIONES\",\n",
    "    \"DE LAS SANCIONES\" : \"SANCIONES\",\n",
    "    \"Visto:\" : \"VISTO\",\n",
    "    \"Vistos:\" : \"VISTO\",\n",
    "    \"Vistos estos antecedentes:\" : \"VISTO\",\n",
    "    \"--------------\" : \"HEADING\"\n",
    "}\n",
    "section_tags = {\n",
    "    \"Considerando:\" : \"CONSIDERANDO\",\n",
    "    \"Considerando\\n\" : \"CONSIDERANDO\",\n",
    "    \"Decreto:\" : \"RESUELVO\",\n",
    "    \"Resuelvo:\" : \"RESUELVO\",\n",
    "    \"R e s u e l v o:\" : \"RESUELVO\",\n",
    "    \"Se resuelve\" : \"RESUELVO\",\n",
    "    \"S e  r e s u e l v e:\" : \"RESUELVO\",\n",
    "    \"Visto:\" : \"VISTO\",\n",
    "    \"Vistos:\" : \"VISTO\",\n",
    "    \"Vistos estos antecedentes:\" : \"VISTO\",\n",
    "    \"--------------\" : \"HEADING\"\n",
    "}\n",
    "merges_lower = {}\n",
    "for key, value in merges.items():\n",
    "    merges_lower[key.lower()] = value\n",
    "# Eventhough the general gramar rule in Spanish is not to accent uppercase, there are many cases where a word in a heding might\n",
    "# appear accented. This is a dictionary to armonize all headings without accents. The list is rather comprehensive, but there is\n",
    "# still room for improvement.\n",
    "# If we find some bug beyond simple misspelling which will be solved by spell checker, we can include it here. The example is in\n",
    "# the first row with \"ACTIVIDADESUSOS\" which was found several times in headings.\n",
    "bugs = {\"ACTIVIDADESUSOS\" : \"ACTIVIDADES DE USOS\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to the AWS S3 bucket\n",
    "To effectively run this cell you need Omdena's credentials. Please keep them local and do not sync them in GitHub repos nor cloud drives. Before doing anything with this json file, please think of security!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "# json_folder = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "filename = \"Omdena_key_S3.json\"\n",
    "file = json_folder / filename\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    cred = json.load(f) \n",
    "\n",
    "for key in cred:\n",
    "    KEY = key\n",
    "    SECRET = cred[key]\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To clear html tags (here is basically to remove the page tags)\n",
    "cleanr = re.compile(r'<.*?>')\n",
    "\n",
    "# To catch accents and dictionary to change them\n",
    "accents_out = re.compile(r'[áéíóúÁÉÍÓÚ]')\n",
    "accents_dict = {\"á\":\"a\",\"é\":\"e\",\"í\":\"i\",\"ó\":\"o\",\"ú\":\"u\",\"Á\":\"A\",\"É\":\"E\",\"Í\":\"I\",\"Ó\":\"O\",\"Ú\":\"U\"}\n",
    "\n",
    "# To remove special characters\n",
    "clean_special_char = re.compile(r'(\\*\\.)|(\\”\\.)')\n",
    "\n",
    "# To search for acronyms\n",
    "clean_acron = re.compile(r'(A\\s*\\.M\\s*\\.)|(\\bart\\s*\\.)|(\\bArt\\s*\\.)|(\\bART\\s*\\.)|(\\bArts\\s*\\.)|(\\bAV\\s*\\.)|(\\bDr\\s*\\.)|(\\bIng\\s*\\.)|(\\bLic\\s*\\.)|(\\bLicda\\s*\\.)|(\\bLIC\\s*\\.)|(mm\\s*\\.)|(mts\\s*\\.)|(\\bNo\\s*\\.)|(P\\s*\\.M\\s*\\.)|(prof\\s*\\.)|(profa\\s*\\.)|(sp\\s*\\.)|(ssp\\s*\\.)|(sr\\s*\\.)|(sra\\s*\\.)|(to\\s*\\.)|(ta\\s*\\.)|(var\\s*\\.)')  \n",
    "\n",
    "# Remove extra white spaces\n",
    "whitespaces = re.compile(r'[ ]{2,}')\n",
    "\n",
    "# Regular expression to clear punctuation from a string\n",
    "clean_punct = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# Regular expression to clear words that introduce unnecessary variability to headings. Some regex still not work 100% we need\n",
    "# to improve them.\n",
    "clean_capitulo = re.compile(r'(APARTADO \\S*)|(APARTADO\\s)|(^ART\\.\\s*\\S*)|(^ART\\.\\s*)|(^Art\\.\\s*\\S*)|(^Art\\.\\s*)|(^Arts\\.\\s*\\S*)|(Capítulo \\S*)|(CAPITULO \\S*)|(CAPITULO\\S*)|(CAPÍTULO \\S*)|(CAPITULÓ \\S*)|(CAPITULOS \\S*)|(CAPITUO \\S*)|(CATEGORIA\\b)|(CATEGORÍA\\b)|(SUBCATEGORIA\\b)|(SUBCATEGORÍA\\b)|(TITULO\\s\\S*)|(TÍTULO\\s\\S*)')\n",
    "clean_bullet_char = re.compile(r'\\b[A-Za-z]\\s*\\.|\\b[A-Za-z]\\s*\\.\\s*|\\b[A-Za-z]\\s*\\-\\s*|\\b[A-Za-z]\\s*\\)\\s*|\\.\\s*\\b[B-Za-z]\\b|\\b[A-Z]{1,4}\\s*\\.')\n",
    "clean_bullet_number = re.compile(r'^\\d+\\s*\\.\\s*-|^\\d+\\s*\\.\\s*\\D+|^\\d+\\)')\n",
    "clean_bullet_point = re.compile(r'^-\\s*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the uppercase ratio in a string. It is used to detect section headings\n",
    "def uppercase_ratio(string):\n",
    "    if len(re.findall(r'[a-z]',string)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return(len(re.findall(r'[A-Z]',string))/len(re.findall(r'[a-z]',string)))\n",
    "\n",
    "def end_of_heading(line, flag, content, counter):\n",
    "    if \"URL\" in line and \"https:\" in line:\n",
    "        flag = False\n",
    "        content = False\n",
    "        counter = 0\n",
    "        return flag, content, counter\n",
    "    else:\n",
    "        return flag, content, counter\n",
    "\n",
    "def is_section(line):\n",
    "    section = False\n",
    "    for key in section_tags:\n",
    "        if key in line:\n",
    "            section = True\n",
    "            break\n",
    "    return section\n",
    "            \n",
    "def end_of_document(line):\n",
    "    end_of_file = False\n",
    "    for key in end_of_file_tags:\n",
    "        if key in line:\n",
    "            end_of_file = True\n",
    "            break\n",
    "    return end_of_file\n",
    "\n",
    "# Te function to clear html tags\n",
    "def clean_html_tags(string):\n",
    "  return cleanr.sub('', string)\n",
    "    \n",
    "def is_por_tanto(line):\n",
    "    if \"POR TANTO\" in line:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to remove the last lines of a document, the ones that contain the signatures of the officials. It depends on the\n",
    "# dictionary \"official_positions\"\n",
    "def remove_signatures(line):\n",
    "    signature = False\n",
    "    for key in official_positions:\n",
    "        if key in line:\n",
    "            signature = True\n",
    "            break\n",
    "    return signature\n",
    "\n",
    "# Function to change accented words by non-accented counterparts. It depends on the dictionary \"accent_marks_bugs\" \n",
    "def remove_accents(string):\n",
    "    for accent in accents_out.findall(string):\n",
    "        string = string.replace(accent, accents_dict[accent])\n",
    "    return string\n",
    "\n",
    "# Function to merge headlines expressing the same concept in different words. It depends on the dictionary \"merges\"\n",
    "def merge_concepts(line):\n",
    "    for key in merges:\n",
    "        if key in line:\n",
    "            line = merges[key]\n",
    "            break\n",
    "    return line\n",
    "\n",
    "def clean_bugs(line):\n",
    "    for key in bugs:\n",
    "        if key in line:\n",
    "            line = line.replace(key, bugs[key])\n",
    "    return line\n",
    "\n",
    "def clean_special_characters(line):\n",
    "    char = clean_special_char.findall(line)\n",
    "    for item in char:\n",
    "        for character in item:\n",
    "            if character != '':\n",
    "                line = line.replace(character, \"\")\n",
    "    return line\n",
    "\n",
    "def clean_acronyms(line):\n",
    "    acro = clean_acron.findall(line)\n",
    "    for item in acro:\n",
    "        for acronym in item:\n",
    "            if acronym != '':\n",
    "                line = line.replace(acronym, clean_punct.sub('', acronym))\n",
    "    return line\n",
    "\n",
    "def clean_whitespace(line):\n",
    "    if whitespaces.sub(' ', line).rstrip().lstrip() != None:\n",
    "        return whitespaces.sub(' ', line).rstrip().lstrip()\n",
    "    else:\n",
    "        return line\n",
    "\n",
    "decimal_points = re.compile(r'(\\b\\d+\\s*\\.\\s*\\d+)')\n",
    "def change_decimal_points(line):\n",
    "    dec = decimal_points.findall(line)\n",
    "    for decimal in dec:\n",
    "        if decimal != '':\n",
    "#             print(decimal)\n",
    "            line = line.replace(decimal, clean_punct.sub(',', decimal))\n",
    "    return line\n",
    "                \n",
    "# Function sentence\n",
    "def clean_sentence(string):\n",
    "    string = clean_capitulo.sub('', string)\n",
    "    string = clean_bullet_char.sub('', string).rstrip().lstrip()\n",
    "    string = clean_bullet_number.sub('', string).rstrip().lstrip()\n",
    "    string = clean_bullet_point.sub('', string).rstrip().lstrip()\n",
    "    if string != \"\":\n",
    "        return string\n",
    "    else:\n",
    "        return \"\"    \n",
    "    \n",
    "# points = re.compile(r'(\\b\\w+\\s*\\.\\s*\\b[^\\d\\W]+)')\n",
    "# def check_points(line):\n",
    "#     return points.findall(line)\n",
    "#     print(points.findall(line))\n",
    "\n",
    "points = re.compile(r'(\\b\\w+\\b\\s*){3,}')\n",
    "def check_sentence(line):\n",
    "    if points.findall(line):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def split_into_sentences(line, sep):\n",
    "    sentence_list = []\n",
    "    for sentence in line.split(sep):\n",
    "        if check_sentence(sentence):\n",
    "            sentence = sentence.rstrip().lstrip()\n",
    "            sentence_list.append(sentence)\n",
    "    return sentence_list\n",
    "\n",
    "# Function to add items to the dictionary with duplicate removal\n",
    "def add_to_dict(string, dictionary, dupl_dict):\n",
    "    if string in dupl_dict or string == None:\n",
    "        pass\n",
    "    else:\n",
    "        dupl_dict[string] = 0\n",
    "        if string in dictionary:\n",
    "            dictionary[string] = dictionary[string] + 1\n",
    "        else:\n",
    "            dictionary[string] = 1\n",
    "    return dictionary\n",
    "def full_cleaning(line):\n",
    "    line = clean_html_tags(line)\n",
    "    line = remove_accents(line)\n",
    "    line = clean_special_characters(line)\n",
    "    line = clean_bugs(line)\n",
    "    line = clean_acronyms(line)\n",
    "    line = clean_whitespace(line)\n",
    "    print(line)\n",
    "    line = clean_sentence(line)\n",
    "    print(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"Que el Art. 204 Ordinal 3*. y 5”. de la Constitución, regula. A. Hola, em dic Jordi. B. No sé massa perquè l'Art *. 22 conté 22.34€. Tanmateix sembla que la Licda. una cosa. voldria  55.22. no fotis\"\n",
    "# test_string = \"Prova senzilleta per veure què passa si no hi ha punt\"\n",
    "test_string = clean_sentence(test_string)\n",
    "test_string = clean_special_characters(test_string)\n",
    "test_string = clean_acronyms(test_string)\n",
    "test_string = clean_whitespace(test_string)\n",
    "test_string = change_decimal_points(test_string)\n",
    "print(test_string)\n",
    "sentences = []\n",
    "[sentences.append(sentence) for sentence in split_into_sentences(test_string)]\n",
    "print(sentences)\n",
    "# print(sentences)\n",
    "\n",
    "# if check_sentence(test_string):\n",
    "#     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to process files from S3 bucket\n",
    "By executing this cell you will go through all policies in El Salvador and process section headings that will be saved in a dictionary. This should be merged with the notebook that builds up the final json files out of plain txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = \"text-extraction/\"\n",
    "out_folder = \"JSON/\"\n",
    "counter = 0\n",
    "name4 = {}\n",
    "name5 = {}\n",
    "name6 = {}\n",
    "name7 = {}\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix='text-extraction'):\n",
    "    if in_folder in obj.key and obj.key.replace(in_folder, \"\") != \"\":# and filename in obj.key   # Un comment the previous string to run the code just in one sample document.\n",
    "        file = obj.get()['Body'].read().decode('utf-8')  #get the file from S3 and read the body content\n",
    "        lines = file.split(\"\\n\") # Split by end of line and pipe lines into a list\n",
    "        file_name = obj.key.replace(in_folder, \"\").replace('.pdf.txt', '')        \n",
    "        name4[file_name[0:4]] = 0\n",
    "        name5[file_name[0:5]] = 0\n",
    "        name6[file_name[0:6]] = 0\n",
    "        name7[file_name[0:7]] = 0\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter)\n",
    "print(len(name4))\n",
    "print(len(name5))\n",
    "print(len(name6))\n",
    "print(len(name7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = \"Chile/full/\"\n",
    "out_folder = \"JSON/\"\n",
    "# filename = \"00a55afe4f55256567397a68df5d7f97e642480b\" # This is only if you want to test on a single file\n",
    "# bag_of_words = {}\n",
    "# sentences = []\n",
    "# sentences_dict = {}\n",
    "json_file = {}\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix='Chile/full/'):\n",
    "    if in_folder in obj.key and obj.key.replace(in_folder, \"\") != \"\":# and filename in obj.key   # Un comment the previous string to run the code just in one sample document.\n",
    "        file = obj.get()['Body'].read().decode('utf-8')  #get the file from S3 and read the body content\n",
    "        lines = file.split(\"\\n\") # Split by end of line and pipe lines into a list\n",
    "        key = obj.key.replace(in_folder, out_folder).replace('pdf.txt', 'json')\n",
    "        filename = key.replace('.json', '').replace(out_folder, '')\n",
    "        print(filename)\n",
    "        json_file[filename] = {}\n",
    "        line_counter = 0\n",
    "        heading_flag = True\n",
    "        heading_content = False\n",
    "        has_section = False\n",
    "        json_file[filename][\"HEADING\"] = {\"tags\" : [], \"sentences\" : {}}\n",
    "        for line in lines:\n",
    "            line = clean_whitespace(line)\n",
    "            # Processing document heading\n",
    "            if heading_flag:\n",
    "                if \"Tipo Norma\" in line:\n",
    "                    heading_content = True\n",
    "                if heading_content:\n",
    "                    line = full_cleaning(line)\n",
    "                    if line != None:\n",
    "                        if \":\" in line:\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id] = {\"text\" : line, \"labels\" : []}\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = line\n",
    "                        else:\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] + \" \" + line\n",
    "#                 print(\"**\", line)\n",
    "                heading_flag, heading_content, line_counter = end_of_heading(line, heading_flag, heading_content, line_counter)\n",
    "\n",
    "            # Breaking when document signatures are found    \n",
    "            elif end_of_document(line):\n",
    "#                 print(line)\n",
    "                break\n",
    "            # Getting section headings\n",
    "            elif (uppercase_ratio(line) == 1 and len(line) > 10 and line_counter > 0) or is_section(line):\n",
    "#                 print(\"line--\", line)\n",
    "                line = remove_accents(line)\n",
    "                line = clean_bugs(line)\n",
    "                line = clean_sentence(line)\n",
    "                if line == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    has_section = True\n",
    "                    section = merge_concepts(line)\n",
    "#                     print(\"**\", section)\n",
    "                    json_file[filename][section] = {\"tags\" : [], \"sentences\" : {}}\n",
    "                    bag_of_words = add_to_dict(section, bag_of_words, duplicates_dict)\n",
    "                    if section == \"VISTO\" and len(split_into_sentences(line, \":\")) > 1:\n",
    "                        visto = split_into_sentences(line, \":\")\n",
    "                        for sentence in split_into_sentences(visto[1], \";\"):\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "\n",
    "            elif has_section:\n",
    "                line = full_cleaning(line)\n",
    "                if line == None:\n",
    "                    continue                    \n",
    "                else:\n",
    "                    line = change_decimal_points(line)\n",
    "                    for sentence in split_into_sentences(line, \".\"):\n",
    "                        line_counter += 1\n",
    "                        sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                        json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "#         s3.Object('wri-latin-talent', key).put(Body = str(json.dumps(json_file)))#This will save all the contents in the string variable \"content\" into a txt file in the Pre-processed folder\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Processed/\")\n",
    "filename = \"Chile.json\"\n",
    "file = out_folder / filename\n",
    "with open(file, 'w') as fp:\n",
    "    json.dump(json_file, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(json_file))\n",
    "# for k in sorted(sentences_dict):\n",
    "#     print(k, \":\", sentences_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After preprocessing there are {} different headings in El Salvador policies\".format(len(bag_of_words)))\n",
    "print(\"{} documents have been processed\".format(i))\n",
    "print(\"There are {} lines of text as sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict( sorted(bag_of_words.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by heading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bag_of_words):\n",
    "    print(k, \":\", bag_of_words[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving sentences as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/\")\n",
    "path = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/\")\n",
    "filename = \"sentences.npy\"\n",
    "file = path / filename\n",
    "np_sentences = np.array(sentences)\n",
    "with open(file, 'wb') as f:\n",
    "    np.save(f, np_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to process one file from HD folder\n",
    "This is a pipeline to process a test file in a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "files = os.listdir(path)\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo Norma :Decreto 1496 EXENTO\n",
      "Tipo Norma :Decreto 1496 EXENTO\n",
      "Fecha Publicacion :04-05-2013\n",
      "Fecha Publicacion :04-05-2013\n",
      "Fecha Promulgacion :23-04-2013\n",
      "Fecha Promulgacion :23-04-2013\n",
      "Organismo :MINISTERIO DE JUSTICIA\n",
      "Organismo :MINISTERIO DE JUSTICIA\n",
      "Titulo :CONCEDE PERSONALIDAD JURIDICA Y APRUEBA ESTATUTOS A\n",
      "Titulo :CONCEDE PERSONALIDAD JURIDICA Y APRUEBA ESTATUTOS A\n",
      "\"ASOCIACION DE AUTODESARROLLO SOSTENIBLE EN COMUNIDADES\",\n",
      "\"ASOCIACION DE AUTODESARROLLO SOSTENIBLE EN COMUNIDADES\",\n",
      "DE QUILLOTA\n",
      "DE QUILLOTA\n",
      "Tipo Version :Unica De : 04-MAY-2013\n",
      "Tipo Version :Unica De : 04-MAY-2013\n",
      "Inicio Vigencia :04-05-2013\n",
      "Inicio Vigencia :04-05-2013\n",
      "Id Norma :1050772\n",
      "Id Norma :1050772\n",
      "URL :https://www.leychile.cl/N?i=1050772&f=2013-05-04\n",
      "URL :https://www.leychile.cl/N?i=1050772&f=2013-05-04\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1.- Concedese personalidad juridica a la entidad denominada \"Asociacion de Autodesarrollo Sostenible en Comunidades\", la que podra usar indistintamente y para todos los efectos legales el nombre \"Asociacion Sembra\", con domicilio en la provincia de Quillota, Region de Valparaiso.\n",
      "Concedese personalidad juridica a la entidad denominada \"Asociacion de Autodesarrollo Sostenible en Comunidades\", la que podra usar indistintamente y para todos los efectos legales el nombre \"Asociacion Sembra\", con domicilio en la provincia de Quillota, Region de Valparaiso.\n",
      "2.- Apruebanse los estatutos por los cuales se ha de regir la citada entidad, en los terminos que dan testimonio las escrituras publicas de fechas 31 de enero y 31 de agosto, ambas de 2012, otorgadas ante el Notario Publico de Santiago, don Osvaldo Pereira Gonzalez.\n",
      "Apruebanse los estatutos por los cuales se ha de regir la citada entidad, en los terminos que dan testimonio las escrituras publicas de fechas 31 de enero y 31 de agosto, ambas de 2012, otorgadas ante el Notario Publico de Santiago, don Osvaldo Pereira Gonzalez.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "data_folder = Path(path)\n",
    "filename = \"00d91bcac2667290e3fb42452f5241be28b031c2\"\n",
    "\n",
    "\n",
    "files = os.listdir(path)\n",
    "files = [\"00d91bcac2667290e3fb42452f5241be28b031c2\"]\n",
    "\n",
    "bag_of_words = {}\n",
    "json_file = {}\n",
    "\n",
    "i = 0\n",
    "for filename in files:\n",
    "#     if i == 0:\n",
    "    file_ = data_folder / filename\n",
    "    with open(file_, 'r', encoding = 'utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "#         print(\"\\n\", filename, \"\\n\")\n",
    "        json_file[filename] = {}\n",
    "        line_counter = 0\n",
    "        heading_flag = True\n",
    "        heading_content = False\n",
    "        has_section = False\n",
    "        json_file[filename][\"HEADING\"] = {\"tags\" : [], \"sentences\" : {}}\n",
    "        for line in lines:\n",
    "            line = clean_whitespace(line)\n",
    "            # Processing document heading\n",
    "            if heading_flag:\n",
    "                if \"Tipo Norma\" in line:\n",
    "                    heading_content = True\n",
    "                if heading_content:\n",
    "                    line = full_cleaning(line)\n",
    "                    if line != None:\n",
    "                        if \":\" in line:\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id] = {\"text\" : line, \"labels\" : []}\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = line\n",
    "                        else:\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] + \" \" + line\n",
    "#                 print(\"**\", line)\n",
    "                heading_flag, heading_content, line_counter = end_of_heading(line, heading_flag, heading_content, line_counter)\n",
    "\n",
    "            # Breaking when document signatures are found    \n",
    "            elif end_of_document(line):\n",
    "#                 print(line)\n",
    "                break\n",
    "            # Getting section headings\n",
    "            elif (uppercase_ratio(line) == 1 and len(line) > 10 and line_counter > 0) or is_section(line):\n",
    "#                 print(\"line--\", line)\n",
    "                line = remove_accents(line)\n",
    "                line = clean_bugs(line)\n",
    "                line = clean_sentence(line)\n",
    "                if line == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    has_section = True\n",
    "                    section = merge_concepts(line)\n",
    "#                     print(\"**\", section)\n",
    "                    json_file[filename][section] = {\"tags\" : [], \"sentences\" : {}}\n",
    "#                     bag_of_words = add_to_dict(section, bag_of_words, duplicates_dict)\n",
    "                    if section == \"VISTO\" and len(split_into_sentences(line, \":\")) > 1:\n",
    "                        visto = split_into_sentences(line, \":\")\n",
    "                        for sentence in split_into_sentences(visto[1], \";\"):\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "\n",
    "            elif has_section:\n",
    "                line = full_cleaning(line)\n",
    "                if line == None:\n",
    "                    continue                    \n",
    "                else:\n",
    "                    line = change_decimal_points(line)\n",
    "                    for sentence in split_into_sentences(line, \".\"):\n",
    "                        line_counter += 1\n",
    "                        sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                        json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "        i += 1\n",
    "    #     data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00d91bcac2667290e3fb42452f5241be28b031c2': {'HEADING': {'tags': [],\n",
       "   'sentences': {'00d91bc_1': {'text': 'Tipo Norma :Decreto 1496 EXENTO',\n",
       "     'labels': []},\n",
       "    '00d91bc_2': {'text': 'Fecha Publicacion :04-05-2013', 'labels': []},\n",
       "    '00d91bc_3': {'text': 'Fecha Promulgacion :23-04-2013', 'labels': []},\n",
       "    '00d91bc_4': {'text': 'Organismo :MINISTERIO DE JUSTICIA', 'labels': []},\n",
       "    '00d91bc_5': {'text': 'Titulo :CONCEDE PERSONALIDAD JURIDICA Y APRUEBA ESTATUTOS A \"ASOCIACION DE AUTODESARROLLO SOSTENIBLE EN COMUNIDADES\", DE QUILLOTA',\n",
       "     'labels': []},\n",
       "    '00d91bc_6': {'text': 'Tipo Version :Unica De : 04-MAY-2013',\n",
       "     'labels': []},\n",
       "    '00d91bc_7': {'text': 'Inicio Vigencia :04-05-2013', 'labels': []},\n",
       "    '00d91bc_8': {'text': 'Id Norma :1050772', 'labels': []},\n",
       "    '00d91bc_9': {'text': 'URL :https://www.leychile.cl/N?i=1050772&f=2013-05-04',\n",
       "     'labels': []}}},\n",
       "  'VISTO': {'tags': [], 'sentences': {}},\n",
       "  'RESUELVO': {'tags': [],\n",
       "   'sentences': {'00d91bc_1': {'text': 'Concedese personalidad juridica a la entidad denominada \"Asociacion de Autodesarrollo Sostenible en Comunidades\", la que podra usar indistintamente y para todos los efectos legales el nombre \"Asociacion Sembra\", con domicilio en la provincia de Quillota, Region de Valparaiso',\n",
       "     'labels': []},\n",
       "    '00d91bc_2': {'text': 'Apruebanse los estatutos por los cuales se ha de regir la citada entidad, en los terminos que dan testimonio las escrituras publicas de fechas 31 de enero y 31 de agosto, ambas de 2012, otorgadas ante el Notario Publico de Santiago, don Osvaldo Pereira Gonzalez',\n",
       "     'labels': []}}}}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict( sorted(bag_of_words.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by heading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bag_of_words):\n",
    "    print(k, \":\", bag_of_words[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "334.922px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
