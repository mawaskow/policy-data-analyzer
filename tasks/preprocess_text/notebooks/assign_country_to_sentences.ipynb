{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Headings Curation and sentences spliter of Chilean Policies\n",
    "\n",
    "In this notebook we will go through the list of documents i each country to create a dictionary of the first 7 characters of the document code associated to a country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import boto3, json, operator, os, re, string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to the AWS S3 bucket\n",
    "To effectively run this cell you need Omdena's credentials. Please keep them local and do not sync them in GitHub repos nor cloud drives. Before doing anything with this json file, please think of security!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "# json_folder = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "filename = \"Omdena_key_S3.json\"\n",
    "file = json_folder / filename\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    cred = json.load(f) \n",
    "\n",
    "for key in cred:\n",
    "    KEY = key\n",
    "    SECRET = cred[key]\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_empty = re.compile(r'^\\s*\\r+\\n*|^\\s*\\.\\s*\\r+\\n*')\n",
    "def is_empty_line(line):\n",
    "    return bool(is_empty.findall(line))\n",
    "\n",
    "is_line_break = re.compile(r'\\.\\s*$|\\.\\t*$|;\\s*$|:\\s*$|\\s+y$|,\\s*$|\\d+\\s*$')\n",
    "def is_end_of_paragraph(line):\n",
    "    return bool(is_line_break.findall(line))\n",
    "\n",
    "# Function to calculate the uppercase ratio in a string. It is used to detect section headings\n",
    "def uppercase_ratio(string):\n",
    "    if len(re.findall(r'[a-z]',string)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return(len(re.findall(r'[A-Z]',string))/len(re.findall(r'[a-z]',string)))\n",
    "\n",
    "def end_of_heading(line, flag, content):\n",
    "    if \"URL\" in line and \"https:\" in line:\n",
    "        flag = False\n",
    "        content = False\n",
    "        return flag, content\n",
    "    else:\n",
    "        return flag, content\n",
    "\n",
    "def is_section(line):\n",
    "    section = False\n",
    "    for key in section_tags:\n",
    "        if key in line:\n",
    "            section = True\n",
    "            break\n",
    "    return section\n",
    "            \n",
    "def end_of_document(line):\n",
    "    end_of_file = False\n",
    "    for key in end_of_file_tags:\n",
    "        if key in line:\n",
    "            end_of_file = True\n",
    "            break\n",
    "    return end_of_file\n",
    "\n",
    "# Te function to clear html tags\n",
    "def clean_html_tags(string):\n",
    "  return cleanr.sub('', string)\n",
    "    \n",
    "def is_por_tanto(line):\n",
    "    if \"POR TANTO\" in line:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to remove the last lines of a document, the ones that contain the signatures of the officials. It depends on the\n",
    "# dictionary \"official_positions\"\n",
    "def remove_signatures(line):\n",
    "    signature = False\n",
    "    for key in official_positions:\n",
    "        if key in line:\n",
    "            signature = True\n",
    "            break\n",
    "    return signature\n",
    "\n",
    "# Function to change accented words by non-accented counterparts. It depends on the dictionary \"accent_marks_bugs\" \n",
    "def remove_accents(string):\n",
    "    for accent in accents_out.findall(string):\n",
    "        string = string.replace(accent, accents_dict[accent])\n",
    "    return string\n",
    "\n",
    "# Function to merge headlines expressing the same concept in different words. It depends on the dictionary \"merges\"\n",
    "def merge_concepts(line):\n",
    "    for key in merges:\n",
    "        if key in line:\n",
    "            line = merges[key]\n",
    "            break\n",
    "    return line\n",
    "\n",
    "def clean_bugs(line):\n",
    "    for key in bugs:\n",
    "        if key in line:\n",
    "            line = line.replace(key, bugs[key])\n",
    "    return line\n",
    "\n",
    "def clean_special_characters(line):\n",
    "    char = clean_special_char.findall(line)\n",
    "    for item in char:\n",
    "        for character in item:\n",
    "            if character != '':\n",
    "                line = line.replace(character, \"\")\n",
    "    return line\n",
    "\n",
    "def clean_acronyms(line):\n",
    "    acro = clean_acron.findall(line)\n",
    "    for item in acro:\n",
    "        for acronym in item:\n",
    "            if acronym != '':\n",
    "                line = line.replace(acronym, clean_punct.sub('', acronym))\n",
    "    return line\n",
    "\n",
    "def clean_whitespace(line):\n",
    "    if whitespaces.sub(' ', line).rstrip().lstrip() != None:\n",
    "        return whitespaces.sub(' ', line).rstrip().lstrip()\n",
    "    else:\n",
    "        return line\n",
    "\n",
    "decimal_points = re.compile(r'(\\b\\d+\\s*\\.\\s*\\d+|\\(\\.\\)|\\b\\d+\\.\\s*[A-Za-z]|\\b[A-Za-z]\\.\\s*\\d+)')\n",
    "def change_decimal_points(line):\n",
    "    dec = decimal_points.findall(line)\n",
    "    for decimal in dec:\n",
    "        if decimal != '':\n",
    "#             print(decimal)\n",
    "            line = line.replace(decimal, clean_period.sub('^', decimal))\n",
    "    return line\n",
    "\n",
    "def get_decimal_points_back(line):\n",
    "    line = line.replace(\"^\", \".\")\n",
    "    return line\n",
    "                \n",
    "# Function sentence\n",
    "def clean_sentence(string):\n",
    "    string = clean_capitulo.sub('', string)\n",
    "    string = clean_bullet_char.sub('', string).rstrip().lstrip()\n",
    "    string = clean_bullet_number.sub('', string).rstrip().lstrip()\n",
    "    string = clean_bullet_point.sub('', string).rstrip().lstrip()\n",
    "    if string != \"\":\n",
    "        return string\n",
    "    else:\n",
    "        return \"\"    \n",
    "    \n",
    "# points = re.compile(r'(\\b\\w+\\s*\\.\\s*\\b[^\\d\\W]+)')\n",
    "# def check_points(line):\n",
    "#     return points.findall(line)\n",
    "#     print(points.findall(line))\n",
    "# This is to check sentences that have just three words\n",
    "points = re.compile(r'(\\b\\w+\\b\\s*){3,}')\n",
    "def check_sentence(line):\n",
    "    if points.findall(line):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def split_into_sentences(line, sep):\n",
    "    sentence_list = []\n",
    "    for sentence in line.split(sep):\n",
    "        if check_sentence(sentence):\n",
    "            sentence = sentence.rstrip().lstrip()\n",
    "            sentence_list.append(sentence)\n",
    "    return sentence_list\n",
    "\n",
    "# Function to add items to the dictionary with duplicate removal\n",
    "def add_to_dict(string, dictionary, dupl_dict):\n",
    "    if string in dupl_dict or string == None:\n",
    "        pass\n",
    "    else:\n",
    "        dupl_dict[string] = 0\n",
    "        if string in dictionary:\n",
    "            dictionary[string] = dictionary[string] + 1\n",
    "        else:\n",
    "            dictionary[string] = 1\n",
    "    return dictionary\n",
    "def full_cleaning(line):\n",
    "    line = clean_html_tags(line)\n",
    "    line = remove_accents(line)\n",
    "    line = clean_special_characters(line)\n",
    "    line = clean_bugs(line)\n",
    "    line = clean_acronyms(line)\n",
    "    line = clean_whitespace(line)\n",
    "#     print(line)\n",
    "    line = clean_sentence(line)\n",
    "#     print(line)\n",
    "    return line\n",
    "def is_visto(string, section_name, counter, dictionary):\n",
    "    if section_name == \"VISTO\" and len(split_into_sentences(string, \":\")) > 1:\n",
    "        visto = split_into_sentences(string, \":\")\n",
    "        for sentence in split_into_sentences(visto[1], \";\"):\n",
    "            counter += 1\n",
    "            sentence_id = filename[0:7] + '_' + str(counter)\n",
    "            dictionary[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "    elif section == \"VISTO\" and len(split_into_sentences(string, \":\")) < 2:\n",
    "        for sentence in split_into_sentences(string, \";\"):\n",
    "            counter += 1\n",
    "            sentence_id = filename[0:7] + '_' + str(counter)\n",
    "            dictionary[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "    return counter, dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to process files from S3 bucket\n",
    "By executing this cell you will go through all policies in El Salvador and process section headings that will be saved in a dictionary. This should be merged with the notebook that builds up the final json files out of plain txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = {\"Chile/full/\" : \"Chile\", \"El Salvador/\" : \"El Salvador\"}\n",
    "out_folder = \"JSON/\"\n",
    "filename = \"0185327971f7a647a20ea48ad0d253fa2549ac63\" # This is only if you want to test on a single file\n",
    "# bag_of_words = {}\n",
    "# sentences = []\n",
    "# sentences_dict = {}\n",
    "json_file = {}\n",
    "for path in in_folder:\n",
    "    for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix= path):\n",
    "        if path in obj.key and obj.key.replace(path, \"\") != \"\":# and filename in obj.key   # Un comment the previous string to run the code just in one sample document.\n",
    "            key = obj.key.replace(path,\"\")\n",
    "            json_file[key[0:7]] = in_folder[path]\n",
    "           \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01dcc9c\n",
      "029d411\n",
      "02e3658\n",
      "03030d8\n",
      "04731ac\n",
      "060720b\n",
      "069d9e5\n",
      "074b95f\n",
      "07d1de2\n",
      "07eec23\n",
      "0817764\n",
      "081affe\n",
      "08f5a1a\n",
      "092e4c7\n",
      "0c5f5ad\n",
      "0cc5362\n",
      "0cffb35\n",
      "0d090c5\n",
      "0d471c6\n",
      "0e2a5ca\n",
      "0e2ad48\n",
      "0e9333e\n",
      "0febf9d\n",
      "11771b4\n",
      "12244f1\n",
      "135664a\n",
      "161d1f4\n",
      "1629dfa\n",
      "17252d8\n",
      "18d1b46\n",
      "196c699\n",
      "19a0f82\n",
      "1a77612\n",
      "1b7669f\n",
      "1c909b8\n",
      "1cd36a0\n",
      "1ce9e23\n",
      "1d19c44\n",
      "1d5dfdc\n",
      "1f05fcc\n",
      "1f15406\n",
      "1ffdee7\n",
      "20a2b41\n",
      "2144417\n",
      "21dcf30\n",
      "2235b37\n",
      "2336314\n",
      "2369a4b\n",
      "23f3997\n",
      "2418c2b\n",
      "27d89cb\n",
      "28e15d8\n",
      "29c84d6\n",
      "29cb81a\n",
      "2ab0f3c\n",
      "2b97317\n",
      "2c0677d\n",
      "2ce32ee\n",
      "2dff019\n",
      "30220c4\n",
      "303730b\n",
      "30ba71c\n",
      "323ef51\n",
      "326cdc5\n",
      "3294386\n",
      "32ff28d\n",
      "334f31a\n",
      "348ca50\n",
      "34f1f1d\n",
      "354dd06\n",
      "35c4d4a\n",
      "3610a3c\n",
      "36443d4\n",
      "369c7dd\n",
      "36ad0ca\n",
      "37354e2\n",
      "37d59e7\n",
      "38507e7\n",
      "3c8c7a4\n",
      "3ce4a28\n",
      "3eb7dca\n",
      "3f18af2\n",
      "3f95071\n",
      "406a87b\n",
      "40a1fbd\n",
      "40c8358\n",
      "4245aca\n",
      "42dd77a\n",
      "43f0373\n",
      "443a5b6\n",
      "47963c5\n",
      "47d5420\n",
      "485f957\n",
      "48bdadf\n",
      "498128e\n",
      "498aa7d\n",
      "499d53d\n",
      "4ae166c\n",
      "4b50e70\n",
      "4c4c6e4\n",
      "4c71364\n",
      "4ce45b1\n",
      "4e220b5\n",
      "4e77902\n",
      "50f28b4\n",
      "51a0d9e\n",
      "5295ca3\n",
      "545e813\n",
      "55b3aab\n",
      "55fdbb4\n",
      "5705f27\n",
      "5729835\n",
      "58a2ada\n",
      "5a47e49\n",
      "5bd487f\n",
      "5c54e9a\n",
      "5ca494a\n",
      "5ea6d0c\n",
      "5eb3387\n",
      "5f9ee58\n",
      "6381fac\n",
      "643a949\n",
      "644a9bb\n",
      "644dcc9\n",
      "6458c88\n",
      "646824a\n",
      "66f81cd\n",
      "6961123\n",
      "6a85bfd\n",
      "6bff306\n",
      "6c5f083\n",
      "6d5968b\n",
      "6dd2466\n",
      "6df75ce\n",
      "6ed3f17\n",
      "6f2b5a5\n",
      "6fbfd56\n",
      "70be962\n",
      "71e0ae7\n",
      "7289291\n",
      "731dbf0\n",
      "735ad07\n",
      "73cfebc\n",
      "7401043\n",
      "753300f\n",
      "7620569\n",
      "765d4ea\n",
      "767c669\n",
      "768f335\n",
      "76b6e7f\n",
      "77c4ed3\n",
      "7951d1c\n",
      "7bd959f\n",
      "7c3fe0d\n",
      "7c4a8f5\n",
      "7c850af\n",
      "7c8d2c5\n",
      "7d1f7bb\n",
      "7d5b453\n",
      "7eab6c1\n",
      "7ec223d\n",
      "7ef9125\n",
      "7f99b94\n",
      "7fc5fe1\n",
      "803a964\n",
      "81673d5\n",
      "82e70cc\n",
      "838e245\n",
      "8458e2b\n",
      "846f57e\n",
      "85d7464\n",
      "881adeb\n",
      "889f980\n",
      "88ad2ff\n",
      "88d44c3\n",
      "89c78b3\n",
      "8a29c72\n",
      "8b0a679\n",
      "8b8fe01\n",
      "8bfe671\n",
      "8c031b9\n",
      "8e81fe2\n",
      "8ed012f\n",
      "8f01c90\n",
      "8fb61f8\n",
      "901a837\n",
      "902bedc\n",
      "908e581\n",
      "909d5b5\n",
      "90e11f5\n",
      "9244573\n",
      "9296544\n",
      "930897c\n",
      "93435af\n",
      "93b0d84\n",
      "9410dd5\n",
      "941814b\n",
      "9462c2d\n",
      "963ad51\n",
      "972f711\n",
      "97c6f7d\n",
      "9ac6cbd\n",
      "9ac7630\n",
      "9bc4888\n",
      "9c819f6\n",
      "9e19d27\n",
      "9e306e3\n",
      "9e4e898\n",
      "9eee974\n",
      "9ffd782\n",
      "a0a19ee\n",
      "a0dcb6f\n",
      "a17ad75\n",
      "a187a36\n",
      "a215dae\n",
      "a2cc704\n",
      "a63e3ea\n",
      "a722ac1\n",
      "a765ce8\n",
      "a8fbc1f\n",
      "abaff8f\n",
      "aee3681\n",
      "af82608\n",
      "af8a2f2\n",
      "b016406\n",
      "b04399e\n",
      "b107cc9\n",
      "b155c42\n",
      "b189d15\n",
      "b2ef15f\n",
      "b3d2f9b\n",
      "b430144\n",
      "b49dfc7\n",
      "b821b11\n",
      "b8922be\n",
      "b8bb2bb\n",
      "b8cd173\n",
      "b8dd82f\n",
      "b8ff0ff\n",
      "b94f45d\n",
      "b98c5a5\n",
      "b9d5d3c\n",
      "b9ecd6f\n",
      "bad1075\n",
      "bb1e13e\n",
      "bb579f7\n",
      "bba5bdb\n",
      "bbde3f8\n",
      "bcf7f64\n",
      "bd6c84b\n",
      "bdc0ae9\n",
      "be91b48\n",
      "becb289\n",
      "beedd83\n",
      "bf78089\n",
      "bf842d0\n",
      "c00e1d3\n",
      "c06de9d\n",
      "c13fd29\n",
      "c4000b7\n",
      "c47808f\n",
      "c56175e\n",
      "c61568e\n",
      "c70ee7c\n",
      "c85c73b\n",
      "c89d85e\n",
      "c9463b0\n",
      "ca0130b\n",
      "cb0526d\n",
      "cb102f8\n",
      "cb2206a\n",
      "cd5eeaa\n",
      "ce0e2c3\n",
      "ce1b45c\n",
      "ce2bd55\n",
      "cefe040\n",
      "cfe0ee3\n",
      "d059638\n",
      "d1b8337\n",
      "d1d9220\n",
      "d44820b\n",
      "d491a15\n",
      "d56d911\n",
      "d6c44cc\n",
      "daafe74\n",
      "dab533a\n",
      "db5877b\n",
      "dd6b5ad\n",
      "dd7b815\n",
      "de02487\n",
      "deb5beb\n",
      "df1255e\n",
      "df5ecd4\n",
      "e2a3198\n",
      "e2bb383\n",
      "e33e6ef\n",
      "e3b7f39\n",
      "e406275\n",
      "e5e0011\n",
      "e7ec4a0\n",
      "e97f743\n",
      "ea0ec0f\n",
      "ea91c69\n",
      "eaf4e0d\n",
      "eb2af8d\n",
      "eb2d73d\n",
      "eb99685\n",
      "eec48fc\n",
      "eefa471\n",
      "f054d17\n",
      "f082837\n",
      "f13fd1f\n",
      "f2b4ee4\n",
      "f321dac\n",
      "f35c8d1\n",
      "f371310\n",
      "f3fd738\n",
      "f4e4e13\n",
      "f55adda\n",
      "f5d3dd3\n",
      "f66a75a\n",
      "f6b2045\n",
      "f79613e\n",
      "f949be6\n",
      "fa20955\n",
      "fae05b2\n",
      "fc7393e\n",
      "fc882ef\n",
      "fe2403d\n",
      "fe4ed35\n",
      "ff41a50\n"
     ]
    }
   ],
   "source": [
    "for item in json_file:\n",
    "    if \"El Salvador\" in json_file[item]:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Processed/\")\n",
    "filename = \"Chile_V2_210109.json\"\n",
    "file = out_folder / filename\n",
    "with open(file, 'w') as fp:\n",
    "    json.dump(json_file, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading back the sentences dictionary\n",
    "out_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Processed/\")\n",
    "filename = \"Chile_V2_210109.json\"\n",
    "file = out_folder / filename\n",
    "with open(file, 'r') as fp:\n",
    "    json_file = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(json_file))\n",
    "# for k in sorted(sentences_dict):\n",
    "#     print(k, \":\", sentences_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After preprocessing there are {} different headings in El Salvador policies\".format(len(bag_of_words)))\n",
    "print(\"{} documents have been processed\".format(i))\n",
    "print(\"There are {} lines of text as sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict( sorted(bag_of_words.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by heading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bag_of_words):\n",
    "    print(k, \":\", bag_of_words[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = {}\n",
    "for filename in json_file:\n",
    "    for section in json_file[filename]:\n",
    "        for sentence_id in json_file[filename][section][\"sentences\"]:\n",
    "            length = len(json_file[filename][section][\"sentences\"][sentence_id][\"text\"])\n",
    "            if length in sentence_length:\n",
    "                sentence_length[length] += 1\n",
    "            else:\n",
    "                sentence_length[length] = 1\n",
    "            if length == 13:\n",
    "                print(sentence_id, \"--\", json_file[filename][section][\"sentences\"][sentence_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length_list = []\n",
    "for key, value in sentence_length.items():\n",
    "    sentence_length_list.append([round(key), round(value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_length = sorted(sentence_length_list,key=lambda x: x[0])\n",
    "df = pd.DataFrame(sorted_length, columns = ['Sentence_length','Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[len(df)-20:len(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_max = df[\"Sentence_length\"].max()\n",
    "x_max = 65\n",
    "x_sep = 1\n",
    "df[0:x_max].plot.bar(x='Sentence_length', y='Frequency', rot=45, figsize = (18, 10), xticks=range(0, x_max, x_sep), xlim = (0, x_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_arr = np.asarray(sorted_length)\n",
    "labels, ys = zip(*sorted_arr)\n",
    "xs = np.arange(len(labels)) \n",
    "width = 1\n",
    "\n",
    "plt.bar(xs, ys, width, align='center')\n",
    "\n",
    "plt.xticks(xs, labels) #Replace default x-ticks with xs, then replace xs with labels\n",
    "plt.yticks(ys)\n",
    "\n",
    "# plt.savefig('netscore.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving sentences as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/\")\n",
    "path = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/\")\n",
    "filename = \"sentences.npy\"\n",
    "file = path / filename\n",
    "np_sentences = np.array(sentences)\n",
    "with open(file, 'wb') as f:\n",
    "    np.save(f, np_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to process one file from HD folder\n",
    "This is a pipeline to process a test file in a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "files = os.listdir(path)\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "data_folder = Path(path)\n",
    "filename = \"00d91bcac2667290e3fb42452f5241be28b031c2\"\n",
    "\n",
    "\n",
    "files = os.listdir(path)\n",
    "files = [\"002c53058e85d383b057fa4cc25a6eb8e7d401e3\"]\n",
    "\n",
    "bag_of_words = {}\n",
    "json_file = {}\n",
    "\n",
    "i = 0\n",
    "for filename in files:\n",
    "#     if i == 0:\n",
    "    file_ = data_folder / filename\n",
    "    with open(file_, 'r', encoding = 'utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "#         print(\"\\n\", filename, \"\\n\")\n",
    "        json_file[filename] = {}\n",
    "        line_counter = 0\n",
    "        heading_flag = True\n",
    "        heading_content = False\n",
    "        has_section = False\n",
    "        json_file[filename][\"HEADING\"] = {\"tags\" : [], \"sentences\" : {}}\n",
    "        for line in lines:\n",
    "            line = clean_whitespace(line)\n",
    "            # Processing document heading\n",
    "            if heading_flag:\n",
    "                if \"Tipo Norma\" in line:\n",
    "                    heading_content = True\n",
    "                if heading_content:\n",
    "                    line = full_cleaning(line)\n",
    "                    if line != None:\n",
    "                        if \":\" in line:\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id] = {\"text\" : line, \"labels\" : []}\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = line\n",
    "                        else:\n",
    "                            json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] + \" \" + line\n",
    "#                 print(\"**\", line)\n",
    "                heading_flag, heading_content = end_of_heading(line, heading_flag, heading_content)\n",
    "\n",
    "            # Breaking when document signatures are found    \n",
    "            elif end_of_document(line):\n",
    "#                 print(line)\n",
    "                break\n",
    "            # Getting section headings\n",
    "            elif (uppercase_ratio(line) == 1 and len(line) > 10 and line_counter > 0) or is_section(line):\n",
    "#                 print(\"line--\", line)\n",
    "                line = remove_accents(line)\n",
    "                line = clean_bugs(line)\n",
    "                line = clean_sentence(line)\n",
    "                if line == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    has_section = True\n",
    "                    section = merge_concepts(line)\n",
    "#                     print(\"**\", section)\n",
    "                    json_file[filename][section] = {\"tags\" : [], \"sentences\" : {}}\n",
    "#                     bag_of_words = add_to_dict(section, bag_of_words, duplicates_dict)\n",
    "                    if section == \"VISTO\" and len(split_into_sentences(line, \":\")) > 1:\n",
    "                        visto = split_into_sentences(line, \":\")\n",
    "                        for sentence in split_into_sentences(visto[1], \";\"):\n",
    "                            line_counter += 1\n",
    "                            sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                            json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "\n",
    "            elif has_section:\n",
    "                line = full_cleaning(line)\n",
    "                if line == None:\n",
    "                    continue                    \n",
    "                else:\n",
    "                    line = change_decimal_points(line)\n",
    "                    for sentence in split_into_sentences(line, \".\"):\n",
    "                        line_counter += 1\n",
    "                        sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                        json_file[filename][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "        i += 1\n",
    "    #     data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict( sorted(bag_of_words.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by heading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bag_of_words):\n",
    "    print(k, \":\", bag_of_words[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "334.917px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
