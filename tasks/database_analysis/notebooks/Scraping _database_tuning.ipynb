{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7948a1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import BytesIO;\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4386a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/propietari/Documents/fitxers importants/WRI/Scraping_results/\"\n",
    "\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c8225f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ElSalvador_20210730.csv',\n",
       " 'USFR_2_20210311.csv',\n",
       " 'Mexico_20210423.csv',\n",
       " 'USFR_20210311.csv',\n",
       " 'USFR_1_20210311.csv',\n",
       " 'USFR_20210702.csv',\n",
       " 'LeyChile_20210702.csv',\n",
       " 'India_20210310.csv',\n",
       " 'Mexico_20210703.csv',\n",
       " 'USFR_20210310.csv',\n",
       " 'India_20210703.csv',\n",
       " 'Mexico_20210809.csv',\n",
       " 'El_Salvador_no_duplicates.csv']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e2d2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Users/jordi/Documents/claus/'\n",
    "key_path = '/home/propietari/Documents/claus/'\n",
    "filename = 'AWS_S3_keys_wri.json'\n",
    "file = key_path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "# s3BucketName = \"wri-testing\"\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "# region = 'eu-central-1'\n",
    "region = \"us-east-1\"\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e25223d",
   "metadata": {},
   "source": [
    "### El Salvador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46d8e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the csv files from scraping to be compared\n",
    "file_position = 0\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    ElSalvador_20210730 = [row[3] for row in reader]\n",
    "\n",
    "file_position = 12\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    El_Salvador_no_duplicates = [row[7] for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c24dea54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 460 items in the old file and 271 in the new one\n",
      "There are 0 duplicates in the new version\n",
      "The difference between the first scraping and the last version is of 189 entries\n",
      "There are 236 files in the old version that are not in the new version\n",
      "There are 47 files in the new version that are not in the old version\n",
      "There are 507 unique ids in the union of the files\n"
     ]
    }
   ],
   "source": [
    "ElSalvador_20210730_SET = set(ElSalvador_20210730)\n",
    "print(f\"There are {len(El_Salvador_no_duplicates)} items in the old file and {len(ElSalvador_20210730)} in the new one\")\n",
    "print(f\"There are {len(ElSalvador_20210730) - len(ElSalvador_20210730_SET)} duplicates in the new version\")\n",
    "print(f\"The difference between the first scraping and the last version is of {len(El_Salvador_no_duplicates) - len(ElSalvador_20210730)} entries\")\n",
    "not_in_newest_version = set(El_Salvador_no_duplicates) - ElSalvador_20210730_SET\n",
    "print(f\"There are {len(not_in_newest_version)} files in the old version that are not in the new version\")\n",
    "not_in_oldest_version =  ElSalvador_20210730_SET - set(El_Salvador_no_duplicates)\n",
    "print(f\"There are {len(not_in_oldest_version)} files in the new version that are not in the old version\")\n",
    "print(f\"There are {len(set(El_Salvador_no_duplicates) | ElSalvador_20210730_SET)} unique ids in the union of the files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5755b44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going to merge the two files in one. The fields were not saved in the sae order so it is necessary to rearrange them\n",
    "\n",
    "file_position = 0\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    ElSalvador_20210730 = list(reader)\n",
    "\n",
    "file_position = 12\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    El_Salvador_no_duplicates = list(reader)\n",
    "    \n",
    "final_list = []\n",
    "sv = []\n",
    "ids = {}\n",
    "for item in El_Salvador_no_duplicates:\n",
    "    if item[7] not in ids:\n",
    "        final_item = []\n",
    "        final_item.append(item[7])\n",
    "        final_item.append('El Salvador')\n",
    "        final_item.append('Diario Oficial')\n",
    "        final_item.append(item[1])\n",
    "        final_item.append(item[4])\n",
    "        final_item.append(item[2])\n",
    "        final_item.append(item[6])\n",
    "        final_item.append(item[5])\n",
    "        final_item.append(item[3])\n",
    "        final_list.append(final_item)\n",
    "        ids[item[7]] = 0\n",
    "        sv.append(item[7])\n",
    "    \n",
    "for item in ElSalvador_20210730:\n",
    "    if item[3] not in ids:\n",
    "        final_item = []\n",
    "        final_item.append(item[3])\n",
    "        final_item.append(item[1])\n",
    "        final_item.append(item[2])\n",
    "        final_item.append(item[0])\n",
    "        final_item.append(item[7])\n",
    "        final_item.append(item[6])\n",
    "        final_item.append(item[10])\n",
    "        final_item.append(item[9])\n",
    "        final_item.append(item[4])\n",
    "        final_list.append(final_item)\n",
    "        ids[item[3]] = 0\n",
    "        sv.append(item[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "opponent-subscription",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'765d4ea2383aaf45bb29bf75feadea6e06a80fd2'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14153d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the differences with bucket files\n",
    "\n",
    "language = \"spanish\"\n",
    "in_prefix = f\"{language}_documents/HSSC/sentences/\"\n",
    "ids_S3 = {}\n",
    "for i, obj in enumerate(s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix)):\n",
    "    if \".json\" in obj.key and \"ing\" not in obj.key:\n",
    "        name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        ids_S3[name] = 0\n",
    "        \n",
    "processed_list = []\n",
    "for key in ids_S3:\n",
    "    processed_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7eabc89a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of scraped items that are not in sentences is 236\n",
      "507\n",
      "507\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of scraped items that are not in sentences is {len(set(sv) - set(processed_list))}\")\n",
    "# print(f\"The number of scraped items that are not in sentences is {len(set(sv) - set(new_list))}\")\n",
    "print(len(set(sv)))\n",
    "print(len(sv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8d81b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    }
   ],
   "source": [
    "# Here we go and fetch the missing files and put them in the raw_pdf_updated folder\n",
    "prefix = f\"{language}_documents/raw_pdf_updated/\"\n",
    "\n",
    "\n",
    "counter = 0\n",
    "bucket = s3.Bucket(s3BucketName)\n",
    "for item in final_list:\n",
    "    if item[0] not in ids_S3:\n",
    "        counter += 1\n",
    "        key = prefix + item[0] + \".pdf\"\n",
    "        with requests.get(item[8], stream=True) as r:\n",
    "            bucket.upload_fileobj(r.raw, key)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24facd84",
   "metadata": {},
   "source": [
    "### Chile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1c80e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_position = 6\n",
    "with open(path + onlyfiles[file_position], 'r', encoding=\"Latin1\") as f:#, encoding = \"Latin1\"\n",
    "    reader = csv.reader(f)\n",
    "    chile = list(reader)\n",
    "    \n",
    "# final_list = []\n",
    "# ids = {}\n",
    "# dates = {}\n",
    "ch = []\n",
    "for item in chile:\n",
    "    if item[3].split(\".\")[0] not in ids:\n",
    "        final_item = []\n",
    "        final_item.append(item[3].split(\".\")[0])\n",
    "        final_item.append(item[1])\n",
    "        final_item.append(item[2])\n",
    "        final_item.append(item[0])\n",
    "        final_item.append(item[5])\n",
    "        final_item.append(item[6])\n",
    "#         dates[item[6]] = 0\n",
    "        final_item.append(item[10])\n",
    "        final_item.append(item[9])\n",
    "        final_item.append(item[4])\n",
    "        final_list.append(final_item)\n",
    "        ids[item[3].split(\".\")[0]] = 0\n",
    "        ch.append(item[3].split(\".\")[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5088afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of scraped items that are not in sentences is 4\n",
      "860\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of scraped items that are not in sentences is {len(set(ch) - set(processed_list))}\")\n",
    "# print(f\"The number of scraped items that are not in sentences is {len(set(ch) - set(new_list))}\")\n",
    "print(len(set(ch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fb4c2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the differences with bucket files\n",
    "\n",
    "language = \"spanish\"\n",
    "in_prefix = f\"{language}_documents/HSSC/sentences/\"\n",
    "ids_S3 = {}\n",
    "for i, obj in enumerate(s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix)):\n",
    "    if \".json\" in obj.key and \"ing\" not in obj.key:\n",
    "        name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        ids_S3[name] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f05095d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of scraped items that are not in sentences is 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of scraped items that are not in sentences is {len(ids.keys() - ids_S3.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f051aed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Here we go and fetch the missing files and put them in the raw_pdf_updated folder\n",
    "prefix = f\"{language}_documents/text_files/HSSC/new/\"\n",
    "    \n",
    "counter = 0\n",
    "for item in final_list:\n",
    "    if item[0] not in ids_S3:\n",
    "        counter += 1\n",
    "        print(counter, end = \"\\r\")\n",
    "        key = prefix + item[0] + \".txt\"\n",
    "        with requests.get(item[8], stream=True) as r:\n",
    "            s3.Object(s3BucketName, key).put(Body=r.content)\n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f5802",
   "metadata": {},
   "source": [
    "### Mexico\n",
    "\n",
    "There is a bug in the mexican database and documents which affects the document ID. The document ID is the HSA1 of the http of the document. The http contains the date, and the variable used to set the date in the http address was not always the one it should be. For this reason the document name, as it is the HSA1 of the http, was not the right one.\n",
    "\n",
    "Thus, first we will change the codes of the saved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "juvenile-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_position = 8\n",
    "id_conv_july = {}\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:#, encoding=\"ISO-8859-1\"\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        id_conv_july[row[7]] = row[3].split(\".\")[0]\n",
    "\n",
    "file_position = 11\n",
    "id_conv_august = {}\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:#, encoding=\"ISO-8859-1\"\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        id_conv_august[row[7]] = row[3].split(\".\")[0]\n",
    "conversor = {}\n",
    "count = 0\n",
    "lst = []\n",
    "for key, value in id_conv_july.items():\n",
    "    try:\n",
    "        if value != id_conv_august[key]:\n",
    "            conversor[value] = id_conv_august[key]\n",
    "    except:\n",
    "        count += 1\n",
    "        lst.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "double-policy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(conversor))\n",
    "count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-cornwall",
   "metadata": {},
   "source": [
    "The *conversor* dict contains the old codes as keys and the new codes as values.\\\n",
    "Now, we need to change all the codes in the saved documents:\\\n",
    "1. The names of the saved text documents\n",
    "2. The names of the saved json files with sentences in all the places that we have which is currently 3\n",
    "3. The names of the documents inside the json files\n",
    "4. The names of the sentences inside the jeson files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "parliamentary-astronomy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spanish_documents/text_files/HSSC/processed/\n",
      "0\n",
      "spanish_documents/HSSC/sentences/\n",
      "3953\n",
      "spanish_documents/HSSC/updated_sentences/\n",
      "3953\n",
      "spanish_documents/HSSC/simple_transformers/sentences/\n",
      "3953\n",
      "spanish_documents/HSSC/simple_transformers/updated_sentences/\n",
      "3953\n"
     ]
    }
   ],
   "source": [
    "prefixes = [\"spanish_documents/text_files/HSSC/processed/\", \n",
    "            \"spanish_documents/HSSC/sentences/\", \n",
    "            \"spanish_documents/HSSC/updated_sentences/\", \n",
    "            \"spanish_documents/HSSC/simple_transformers/sentences/\", \n",
    "            \"spanish_documents/HSSC/simple_transformers/updated_sentences/\"]\n",
    "# prefixes = [\"Testing/\"]\n",
    "\n",
    "# conversor = {'3bd31dba0056c4aba34ad22ac6485b02f68f9a11' : 'af9916f651877d2d4238a66cbb2bca6b4724a625'}\n",
    "\n",
    "for prefix in prefixes:\n",
    "    print(prefix)\n",
    "    count = 0\n",
    "    for obj in s3.Bucket(s3BucketName).objects.all().filter(Prefix = prefix):\n",
    "        if \".txt\" in obj.key or \".json\" in obj.key: #and count < 3\n",
    "            name = obj.key.split(\"/\")[-1].split(\"_\")[0].split(\".\")[0]\n",
    "            if name in conversor and \"sentences\" in prefix:\n",
    "                sentences = json.loads(obj.get()['Body'].read().decode('utf-8'))\n",
    "#                 print(f\"{sentences}\\n\")\n",
    "#                 print(conversor[name])\n",
    "                # changing the main key of the dict\n",
    "                sentences[conversor[name]] = sentences.pop(name)\n",
    "                # changing the keys of the sentences\n",
    "                keys = [f'{conversor[name]}_sent_{i}' for i in range(0, len(sentences[conversor[name]][\"sentences\"]))]\n",
    "                values = list(sentences[conversor[name]][\"sentences\"].values())\n",
    "                sentences[conversor[name]][\"sentences\"] = {key:value for key, value in zip(keys, values)}\n",
    "                # deleting the old file\n",
    "                s3.Object(s3BucketName,obj.key).delete()\n",
    "                #uploading the new file\n",
    "                s3.Object(s3BucketName, f\"{prefix}{conversor[name]}_sents.json\").put(Body=(json.dumps(sentences, indent=4)))\n",
    "                count +=1\n",
    "            elif name in conversor and \"processed\" in prefix:\n",
    "                s3.meta.client.copy_object(Bucket=s3BucketName,\n",
    "                                                 CopySource=f\"{s3BucketName}/{obj.key}\",\n",
    "                                                 Key=f\"{prefix}{conversor[name]}.txt\")\n",
    "                s3.Object(s3BucketName,obj.key).delete()\n",
    "                count +=1\n",
    "                \n",
    "    print(count)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "171c5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_position = 11\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:#, encoding=\"ISO-8859-1\"\n",
    "    reader = csv.reader(f)\n",
    "    mexico_august = list(reader)\n",
    "    \n",
    "# final_list = []\n",
    "# ids = {}\n",
    "# dates = {}\n",
    "mx_au = []\n",
    "for item in mexico_august:\n",
    "    if item[3].split(\".\")[0] not in ids:\n",
    "        final_item = []\n",
    "        final_item.append(item[3].split(\".\")[0])\n",
    "        final_item.append(item[1])\n",
    "        final_item.append(item[2])\n",
    "        final_item.append(item[0])\n",
    "        final_item.append(item[5])\n",
    "        final_item.append(item[6])\n",
    "#         dates[item[6]] = 0\n",
    "        final_item.append(item[10])\n",
    "        final_item.append(item[9])\n",
    "        final_item.append(item[4])\n",
    "        final_list.append(final_item)\n",
    "        ids[item[3].split(\".\")[0]] = 0\n",
    "        mx_au.append(item[3].split(\".\")[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c49b7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of newly scraped items that are not processed is 31\n",
      "5099\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of newly scraped items that are not processed is {len(set(mx_au) - set(processed_list))}\")\n",
    "# print(f\"The number of scraped items that are not in sentences is {len(set(mx_au) - set(new_list))}\")\n",
    "print(len(set(mx_au)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a75f48e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the differences with bucket files\n",
    "\n",
    "language = \"spanish\"\n",
    "in_prefix = f\"{language}_documents/HSSC/sentences/\"\n",
    "ids_S3 = {}\n",
    "for i, obj in enumerate(s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix)):\n",
    "    if \".json\" in obj.key and \"ing\" not in obj.key:\n",
    "        name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        ids_S3[name] = 0\n",
    "\n",
    "processed_list = []\n",
    "for key in ids_S3:\n",
    "    processed_list.append(key)\n",
    "        \n",
    "in_prefix = f\"{language}_documents/text_files/HSSC/new/\"        \n",
    "ids_S3_new = {}\n",
    "for i, obj in enumerate(s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix)):\n",
    "    if \".txt\" in obj.key:\n",
    "        name = obj.key.split(\"/\")[-1].split(\"_\")[0].split(\".\")[0]\n",
    "        ids_S3_new[name] = 0\n",
    "        \n",
    "new_list = []\n",
    "for key in ids_S3_new:\n",
    "    new_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "268e701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of scraped new that are not in processed is 4351\n",
      "The number of scraped processed that are not in new is 8562\n",
      "00009282cf7af3ed1a9e25e1b43791837e7c144a\n",
      "000fe0bdf5e9a0024ca7e9bb8b15dadf5648ae8c\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of scraped new that are not in processed is {len(set(new_list) - set(processed_list))}\")\n",
    "print(f\"The number of scraped processed that are not in new is {len(set(processed_list) - set(new_list))}\")\n",
    "print(processed_list[0])\n",
    "print(new_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33dfcbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\r"
     ]
    }
   ],
   "source": [
    "# Here we go and fetch the missing files and put them in the raw_pdf_updated folder\n",
    "language = \"spanish\"\n",
    "prefix = f\"{language}_documents/text_files/HSSC/new/\"\n",
    "\n",
    "\n",
    "def save_to_s3(s3, bucket, file_key, text):\n",
    "    s3.Object(bucket, file_key).put(Body = text)\n",
    "\n",
    "def parse_other(response, key):\n",
    "    soup = BeautifulSoup(response.css('div#DivDetalleNota').get(), features = \"lxml\")\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    text = \"\"\n",
    "    if len(paragraphs) == 0:\n",
    "        text = text + soup.text\n",
    "    else:\n",
    "        tables = soup.find_all(\"td\")\n",
    "        for line in paragraphs[1:]:\n",
    "            text = text + line.text + \"\\n\"\n",
    "        text = text + \"<table>\" + \"\\n\"\n",
    "        for cell in tables:\n",
    "            if \"En el documento que usted está visualizando\" not in cell.text:\n",
    "                text = text + cell.text + \"\\n\"\n",
    "        text = text + \"<\\\\table>\" + \"\\n\"\n",
    "    save_to_s3(s3, s3BucketName, key, text.replace(\"\\t\", \"\"))  \n",
    "\n",
    "\n",
    "counter = 0\n",
    "for item in final_list:\n",
    "    if item[0] not in ids_S3:\n",
    "        counter += 1\n",
    "        key = prefix + item[0] + \".txt\"\n",
    "        with requests.get(item[8], stream=True) as r:\n",
    "            resp = TextResponse(body=r.content, url=item[8])\n",
    "            parse_other(resp, key)\n",
    "        print(counter, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-winning",
   "metadata": {},
   "source": [
    "Now we have all the Spanish records in a single list *final_list* we will save them as a single csv document.\n",
    "* El Salvador 507 records\n",
    "* Chile 860 records\n",
    "* Mexico 5099 records\n",
    "* Total Spanish 6466 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "mechanical-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/propietari/Documents/fitxers importants/WRI/Scraping_results/Final_DB_july_2021/\"\n",
    "file_name = \"Spanish_metadata.csv\"\n",
    "file = path + file_name\n",
    "with open(file, \"w\", encoding=\"Latin1\") as f:\n",
    "    db_writer = csv.writer(f)\n",
    "    for item in final_list:\n",
    "        db_writer.writerow(item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6366eded",
   "metadata": {},
   "source": [
    "### India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "encouraging-chest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ElSalvador_20210730.csv',\n",
       " 'USFR_2_20210311.csv',\n",
       " 'Mexico_20210423.csv',\n",
       " 'USFR_20210311.csv',\n",
       " 'USFR_1_20210311.csv',\n",
       " 'USFR_20210702.csv',\n",
       " 'LeyChile_20210702.csv',\n",
       " 'India_20210310.csv',\n",
       " 'Mexico_20210703.csv',\n",
       " 'USFR_20210310.csv',\n",
       " 'India_20210703.csv',\n",
       " 'Mexico_20210809.csv',\n",
       " 'El_Salvador_no_duplicates.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91f11a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the csv files from scraping to be compared\n",
    "file_position = 7\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    india_march = [row[3] for row in reader]\n",
    "\n",
    "file_position = 10\n",
    "with open(path + onlyfiles[file_position], 'r', encoding = \"Latin1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    india_july = [row[3] for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cosmetic-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the March scraping there were 1954 results out of which ** 1040 were unique\n",
      "in the July scraping there were 179 results out of which ** 179 were unique\n",
      "in the March scraping there were 1001 datapoints that were not in the July scraping\n",
      "in the March scraping there were 140 datapoints that were not in the March scraping\n",
      "1180\n"
     ]
    }
   ],
   "source": [
    "print(f\"in the March scraping there were {len(india_march)} results out of which ** {len(set(india_march))} were unique\")\n",
    "print(f\"in the July scraping there were {len(india_july)} results out of which ** {len(set(india_july))} were unique\")\n",
    "print(f\"in the March scraping there were {len(set(india_march)-set(india_july))} datapoints that were not in the July scraping\")\n",
    "print(f\"in the July scraping there were {len(set(india_july)-set(india_march))} datapoints that were not in the March scraping\")\n",
    "india_all_codes = set(india_july)\n",
    "india_all_codes |= set(india_march)\n",
    "india_all_codes = list(india_all_codes)\n",
    "print(len(india_all_codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a893226",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_position = 7\n",
    "with open(path + onlyfiles[file_position], 'r', encoding=\"ISO-8859-1\") as f:#, encoding = \"Latin1\"\n",
    "    reader = csv.reader(f)\n",
    "    india_old = list(reader)\n",
    "\n",
    "file_position = 10\n",
    "with open(path + onlyfiles[file_position], 'r', encoding=\"ISO-8859-1\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    india_new = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bda33ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/ Department of AGRICULTURE AND MARKETING',\n",
       " 'India',\n",
       " 'India Code',\n",
       " '859644d69fdd0b1e66f1dcf5a42f2d9a3f31fe90.pdf',\n",
       " 'https://www.indiacode.nic.in/bitstream/123456789/6515/1/kishan_ayog_2016_hindi_.pdf#search=Miner OR Ore OR Pit OR Bog OR Buffer OR Corridor OR (Country planning) OR Cropland OR (Degraded land) OR Desert OR Floodplain OR Forestland OR Freshwater OR Grassland OR (Land use) OR Landowner OR Mangrove OR Marsh OR Meadow [1950 TO 2021]',\n",
       " 'Act',\n",
       " ' 2016-11-30',\n",
       " ' 201629',\n",
       " ' Uttarakhand',\n",
       " '',\n",
       " 'The Uttarakhand state farmer comission Act',\n",
       " 'https://www.indiacode.nic.in/handle/123456789/2109?view_type=search&sam_handle=123456789/1362']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "india_old[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.indiacode.nic.in/bitstream/123456789/2109/1/201039.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dda4de3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'india_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cf58889838cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindia_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'india_new' is not defined"
     ]
    }
   ],
   "source": [
    "india_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-trace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
