{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231cf771",
   "metadata": {},
   "source": [
    "# Quality Analysis over the DB search with the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3118664",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc10438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d4250",
   "metadata": {},
   "source": [
    "### Connection to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3564962",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/propietari/Documents/claus/AWS_S3_keys_wri.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-03ec1431f8c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'AWS_S3_keys_wri.json'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcredentials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/propietari/Documents/claus/AWS_S3_keys_wri.json'"
     ]
    }
   ],
   "source": [
    "# path = 'C:/Users/jordi/Documents/claus/'\n",
    "path = '/home/propietari/Documents/claus/'\n",
    "filename = 'AWS_S3_keys_wri.json'\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "# s3BucketName = \"wri-testing\"\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "# region = 'eu-central-1'\n",
    "region = \"us-east-1\"\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a51918e",
   "metadata": {},
   "source": [
    "## Praparation of the sentences for QA\n",
    "\n",
    "The QA is performed in the following way:\n",
    "1. A random sample of 50 documents, containing at least one incentive, is choosen for each language.\n",
    "2. From each document the sentences and the labels are retrieved and saved as an excel file.\n",
    "3. From each of the above documents, a random sample of non-incentive sentennces is selected. The size of the sample is the same than the number of incentive sentences in the same document. In this way there are the same number of incentives and non-incentives from each document. The results are saved in a an excel file.\n",
    "\n",
    "The excel files containing the taged sentences are transfered to the human analist who will confirm the labeling done by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744648e3",
   "metadata": {},
   "source": [
    "### Random picking of documents with incentives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17677c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "language = \"spanish\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be18721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below is to pick random document codes from the list of documents with incentives\n",
    "model = \"simple_transformers\"\n",
    "path = \"../output/\"\n",
    "# file_name = f\"incentives_{language}.csv\"\n",
    "file_name = f\"incentives_{model}_{language}.csv\"\n",
    "file = path + file_name\n",
    "\n",
    "with open(file, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    file_code = [row[0] for row in reader]\n",
    "\n",
    "codes = {}\n",
    "for item in random.sample(file_code, 50):\n",
    "    codes[item] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d95286",
   "metadata": {},
   "source": [
    "### Retrieve the sentences labeled as incentives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0188f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pick the sentences refering to incentives from the selected documents\n",
    "\n",
    "# in_prefix = f\"{language}_documents/HSSC/updated_sentences/\"  \n",
    "in_prefix = f\"{language}_documents/HSSC/{model}/updated_sentences/\"  \n",
    "\n",
    "i = 0\n",
    "documents_with_incentives = 0\n",
    "results_list = [[\"Sentence_name\", \"Sentence text\", \"Label\"]]\n",
    "\n",
    "incentives = 0\n",
    "for obj in s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix):\n",
    "    name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "#     print(name)\n",
    "    if name in codes :\n",
    "        sentences = json.loads(obj.get()['Body'].read().decode('utf-8'))\n",
    "        for sentence_id in sentences[name][\"sentences\"]:\n",
    "            incentive = sentences[name][\"sentences\"][sentence_id][\"label\"]\n",
    "            if len(incentive) > 1 and incentive[0] == \"Incentive\":\n",
    "                incentives += 1\n",
    "                results_list.append([])\n",
    "                results_list[incentives].append(name)\n",
    "                results_list[incentives].append(sentences[name][\"sentences\"][sentence_id][\"text\"])\n",
    "                results_list[incentives].append(incentive[1])\n",
    "                \n",
    "with open(f\"../output/incentive_sentences_{model}_{language}.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_list)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ecdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below is to retrieve de document codes of the sentences which refer to incentives \n",
    "# that has been selected in the previous cell. \n",
    "# This is only in  case you execute the next cell after restarting the kernel. Otherwise, the \"codes\" dictionary is already set.\n",
    "\n",
    "path = \"../output/\"\n",
    "file_name = f\"incentive_sentences_{model}_{language}.csv\"\n",
    "file = path + file_name\n",
    "\n",
    "with open(file, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader)\n",
    "    file_code = [row[0].split(\"_\")[0] for row in reader]\n",
    "\n",
    "codes = {}\n",
    "for item in file_code:\n",
    "    codes[item] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ab0ab",
   "metadata": {},
   "source": [
    "### Retrieve sentences labeled as non-incentives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To pick up the sentences refering to non-incentives from the selected documents\n",
    "# (1) We first retrieve the documents that where selected to pickup incentives\n",
    "# (2) Next we collect all non-incentive sentences\n",
    "# (3) Finally we ramdomly select the same number of non-incentive sentences as there where incentive centences.\n",
    "\n",
    "in_prefix = f\"{language}_documents/HSSC/{model}/updated_sentences/\"  \n",
    "\n",
    "i = 0\n",
    "documents_with_incentives = 0\n",
    "results_list = [[\"Sentence_name\", \"Sentence text\", \"Label\"]]\n",
    "\n",
    "\n",
    "for obj in s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix):\n",
    "    name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "#     print(name)\n",
    "    if name in codes :\n",
    "        sentences = json.loads(obj.get()['Body'].read().decode('utf-8'))\n",
    "        n_incentives = incentive = sentences[name][\"metadata\"][\"number_of_incentives\"]\n",
    "#        print(n_incentives)\n",
    "        non_incentives = [[]]\n",
    "        incentives = 0\n",
    "        for sentence_id in sentences[name][\"sentences\"]:\n",
    "            incentive = sentences[name][\"sentences\"][sentence_id][\"label\"]\n",
    "            if len(incentive) == 1 and incentive[0] == \"not_Incentive\":\n",
    "                non_incentives.append([])\n",
    "                incentives += 1\n",
    "                non_incentives[incentives].append(name)\n",
    "                non_incentives[incentives].append(sentences[name][\"sentences\"][sentence_id][\"text\"])\n",
    "                non_incentives[incentives].append(incentive[0])\n",
    "        \n",
    "        for item in random.sample(non_incentives, n_incentives):\n",
    "            results_list.append(item)\n",
    "                \n",
    "                \n",
    "with open(f\"../output/non_incentive_sentences_{model}_{language}.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(results_list)          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3cd1ae",
   "metadata": {},
   "source": [
    "#### Processing of the reviewed sentences\n",
    "\n",
    "The post-annotation system that we have used, consists of confirming the labels of the model's classification. We have two types of data:\n",
    "* Sentences that the model has classified first as incentives and then classified as one of the six instruments. We are going to call them *Predicted positives* PP.\n",
    "* Sentences that the model has classified as not incentives. We are going to call them *predicted negatives* PN\n",
    "The analist has classified each label into four different categories:\n",
    "1. Category 1 is for sentences which have been correctly classified as incentives with the right policy instrument.\n",
    "2. Category 2 is for sentences that refer to a policy instrument, but that are not incentives, they may refer to social benefits or other types of assistance.\n",
    "3. Category 3 is for sentences that even they refer to some incentive, it is not related to any of the policy instruments under consideration.\n",
    "4. Category 4 is for sentences that are plainly missclassified, they are not incentives nor they are similar to policy instruments.\n",
    "\n",
    "In practical terms, the files where the analist has been annotating the sentences contains two labeling columns, the first one is called \"Incentive?\" and the second one \"Correct instrument?\" the values are 0 or 1 as False and True values. Thus, (0,0) are category 4 sentences, (1,0) are category 3 sentences, (0,1) are category 2 sentences and (1,1) sentences are category 1 sentences.\n",
    "\n",
    "With the available data the following evaluation metrics can be computed:\n",
    "\n",
    "1. PPV for incentives\n",
    "2. FOR for incentives\n",
    "3. Average PPV for instruments\n",
    "4. Specific PPV for each instrument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00846f5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "incentive_sentences_english.xlsx\n",
      "PPV for incentives is 0.42\n",
      "PPV for intruments is 0.69\n",
      "There are 18 datapoints of instrument Fine\n",
      "PPV for Fine is 0.33\n",
      "There are 16 datapoints of instrument Supplies\n",
      "PPV for Supplies is 0.0\n",
      "There are 34 datapoints of instrument Tax benefit\n",
      "PPV for Tax benefit is 0.53\n",
      "There are 156 datapoints of instrument Loan\n",
      "PPV for Loan is 0.94\n",
      "There are 184 datapoints of instrument Direct payment\n",
      "PPV for Direct payment is 0.65\n",
      "There are 18 datapoints of instrument Technical assistance\n",
      "PPV for Technical assistance is 0.28\n",
      "\n",
      "incentive_sentences_spanish.xlsx\n",
      "PPV for incentives is 0.42\n",
      "PPV for intruments is 0.5\n",
      "There are 21 datapoints of instrument Tax benefit\n",
      "PPV for Tax benefit is 0.57\n",
      "There are 147 datapoints of instrument Direct payment\n",
      "PPV for Direct payment is 0.55\n",
      "There are 29 datapoints of instrument Loan\n",
      "PPV for Loan is 0.52\n",
      "There are 175 datapoints of instrument Technical assistance\n",
      "PPV for Technical assistance is 0.43\n",
      "There are 21 datapoints of instrument Fine\n",
      "PPV for Fine is 0.71\n",
      "There are 14 datapoints of instrument Supplies\n",
      "PPV for Supplies is 0.36\n",
      "\n",
      "non_incentive_sentences_english.xlsx\n",
      "FOR for non-incentives is 0.0\n",
      "\n",
      "non_incentive_sentences_spanish.xlsx\n",
      "FOR for non-incentives is 0.0\n",
      "\n",
      "incentive_sentences_simple_transformers_english.xlsx\n",
      "PPV for incentives is 0.63\n",
      "PPV for intruments is 0.71\n",
      "There are 159 datapoints of instrument Direct payment\n",
      "PPV for Direct payment is 0.67\n",
      "There are 17 datapoints of instrument Technical assistance\n",
      "PPV for Technical assistance is 0.41\n",
      "There are 39 datapoints of instrument Fine\n",
      "PPV for Fine is 0.79\n",
      "There are 10 datapoints of instrument Supplies\n",
      "PPV for Supplies is 0.0\n",
      "There are 59 datapoints of instrument Loan\n",
      "PPV for Loan is 0.92\n",
      "There are 54 datapoints of instrument Tax benefit\n",
      "PPV for Tax benefit is 0.78\n",
      "\n",
      "incentive_sentences_simple_transformers_spanish.xlsx\n",
      "PPV for incentives is 0.2\n",
      "PPV for intruments is 0.2\n",
      "There are 882 datapoints of instrument Direct payment\n",
      "PPV for Direct payment is 0.19\n",
      "There are 40 datapoints of instrument Fine\n",
      "PPV for Fine is 0.18\n",
      "There are 190 datapoints of instrument Technical assistance\n",
      "PPV for Technical assistance is 0.31\n",
      "There are 41 datapoints of instrument Tax benefit\n",
      "PPV for Tax benefit is 0.1\n",
      "There are 78 datapoints of instrument Supplies\n",
      "PPV for Supplies is 0.15\n",
      "There are 28 datapoints of instrument Loan\n",
      "PPV for Loan is 0.18\n",
      "\n",
      "non_incentive_sentences_simple_transformers_english.xlsx\n",
      "FOR for non-incentives is 0.0\n",
      "\n",
      "non_incentive_sentences_simple_transformers_spanish.xlsx\n",
      "FOR for non-incentives is 0.0\n"
     ]
    }
   ],
   "source": [
    "path = \"../input/\"\n",
    "incentives = [\"incentive\", \"non_incentive\"]\n",
    "models = [\"sentences\", \"sentences_simple_transformers\"]\n",
    "languages = [\"english\", \"spanish\"]\n",
    "\n",
    "lst_ppv_occurrences = []\n",
    "\n",
    "for model in models:\n",
    "    for incentive in incentives:\n",
    "        for language in languages:\n",
    "            filename = f\"{incentive}_{model}_{language}.xlsx\"\n",
    "            file = path + filename\n",
    "            df = pd.read_excel(file, engine='openpyxl', usecols = \"A:E\")#, sheet_name = f\"{incentive}_{model}_{language}\"\n",
    "            cols = df.columns.values\n",
    "            print(f\"\\n{filename}\")\n",
    "            if incentive == \"incentive\":\n",
    "                instruments = df[cols[2]].unique()\n",
    "                PPV = round(df[df[cols[3]] == 1].count()[cols[3]]/len(df), 2)\n",
    "                print(f\"PPV for incentives is {PPV}\")\n",
    "                if len(instruments) == 6:\n",
    "                    PPV = round(df[df[cols[4]] == 1].count()[cols[4]]/len(df), 2)\n",
    "                    print(f\"PPV for intruments is {PPV}\")\n",
    "                    for instrument in instruments:\n",
    "                        PPV = round(df[(df[cols[2]] == instrument) & (df[cols[4]] == 1)].count()[cols[2]]/df[df[cols[2]] == instrument].count()[cols[2]], 2)\n",
    "                        n_items = len(df[df[cols[2]] == instrument])\n",
    "                        lst_ppv_occurrences.append([instrument, PPV, n_items])\n",
    "                        print(f\"There are {n_items} datapoints of instrument {instrument}\")\n",
    "                        print(f\"PPV for {instrument} is {PPV}\")\n",
    "                else:\n",
    "                    print(\"WARNING! the number of policy instruments is not six\")\n",
    "\n",
    "            elif incentive == \"non_incentive\":\n",
    "                FOR = round(df[df[cols[3]] == 0].count()[cols[3]]/len(df), 2)\n",
    "                print(f\"FOR for non-incentives is {FOR}\")\n",
    "# print(f\"Instruments are {instr}\")\n",
    "# print(f\"The length of the file is: {len(df)}\")\n",
    "# # print(f\"The values of the column \\\"{cols[3]}\\\" are {df[cols[3]].unique()} and there are \")\n",
    "# print(f\"\\t\\tCounts for incentive labels\\n{df.groupby(cols[3]).count()}\")\n",
    "# print(f\"\\n\\t\\tCounts for instrument labels\\n{df.groupby(cols[4]).count()}\")\n",
    "# policy_instrument = {}\n",
    "# for index, row in df.iterrows():\n",
    "#     if row['Policy instrument'] in policy_instrument:\n",
    "#         policy_instrument[row['Policy instrument']] = policy_instrument[row['Policy instrument']] + 1\n",
    "#     else:\n",
    "#         policy_instrument[row['Policy instrument']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85a70fc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Fine', 0.33, 18],\n",
       " ['Supplies', 0.0, 16],\n",
       " ['Tax benefit', 0.53, 34],\n",
       " ['Loan', 0.94, 156],\n",
       " ['Direct payment', 0.65, 184],\n",
       " ['Technical assistance', 0.28, 18],\n",
       " ['Tax benefit', 0.57, 21],\n",
       " ['Direct payment', 0.55, 147],\n",
       " ['Loan', 0.52, 29],\n",
       " ['Technical assistance', 0.43, 175],\n",
       " ['Fine', 0.71, 21],\n",
       " ['Supplies', 0.36, 14],\n",
       " ['Direct payment', 0.67, 159],\n",
       " ['Technical assistance', 0.41, 17],\n",
       " ['Fine', 0.79, 39],\n",
       " ['Supplies', 0.0, 10],\n",
       " ['Loan', 0.92, 59],\n",
       " ['Tax benefit', 0.78, 54],\n",
       " ['Direct payment', 0.19, 882],\n",
       " ['Fine', 0.18, 40],\n",
       " ['Technical assistance', 0.31, 190],\n",
       " ['Tax benefit', 0.1, 41],\n",
       " ['Supplies', 0.15, 78],\n",
       " ['Loan', 0.18, 28]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_ppv_occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61966c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_name</th>\n",
       "      <th>Sentence text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Incentive?</th>\n",
       "      <th>Correct instrument?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1d4756b6080454bc595852ede312f2b213401fe2</td>\n",
       "      <td>Anyone taking, attempting to take, or otherwis...</td>\n",
       "      <td>Fine</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>61a4dde264607bbddfa3f8f4a477ead5c6a71568</td>\n",
       "      <td>This action would strengthen our enforcement o...</td>\n",
       "      <td>Fine</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>61a4dde264607bbddfa3f8f4a477ead5c6a71568</td>\n",
       "      <td>Finally, we are proposing to require that any ...</td>\n",
       "      <td>Fine</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>61a4dde264607bbddfa3f8f4a477ead5c6a71568</td>\n",
       "      <td>The proposed uniform penalty protocol may bene...</td>\n",
       "      <td>Fine</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>7f500a8f818d080984d819aee645e76949cabb1e</td>\n",
       "      <td>Power to levy and collect fee: A Water User's ...</td>\n",
       "      <td>Fine</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Sentence_name  \\\n",
       "86   1d4756b6080454bc595852ede312f2b213401fe2   \n",
       "286  61a4dde264607bbddfa3f8f4a477ead5c6a71568   \n",
       "287  61a4dde264607bbddfa3f8f4a477ead5c6a71568   \n",
       "288  61a4dde264607bbddfa3f8f4a477ead5c6a71568   \n",
       "302  7f500a8f818d080984d819aee645e76949cabb1e   \n",
       "\n",
       "                                         Sentence text Label  Incentive?  \\\n",
       "86   Anyone taking, attempting to take, or otherwis...  Fine           1   \n",
       "286  This action would strengthen our enforcement o...  Fine           1   \n",
       "287  Finally, we are proposing to require that any ...  Fine           1   \n",
       "288  The proposed uniform penalty protocol may bene...  Fine           1   \n",
       "302  Power to levy and collect fee: A Water User's ...  Fine           1   \n",
       "\n",
       "     Correct instrument?  \n",
       "86                     1  \n",
       "286                    1  \n",
       "287                    1  \n",
       "288                    1  \n",
       "302                    1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[cols[2]] == instrument) & (df[cols[3]] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418be996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
