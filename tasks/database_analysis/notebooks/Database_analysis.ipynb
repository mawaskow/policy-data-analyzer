{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1558cb5",
   "metadata": {},
   "source": [
    "# Database analysis - EDA\n",
    "\n",
    "In this notebook we perform Exploratory Data Analysis on the sentence database.\n",
    "Som of the files we are going to use are in the *input* folder (also in the drive), and the json files with the sentence-level information are in the S3 bucket.\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a10766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6948d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access to S3 bucket\n",
    "\n",
    "# path = 'C:/Users/jordi/Documents/claus/'\n",
    "# path = '/home/propietari/Documents/claus/'\n",
    "path = 'C:/Users/hrpeg/Documents/claus/'\n",
    "filename = 'AWS_S3_keys_wri.json'\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "# s3BucketName = \"wri-testing\"\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "# region = 'eu-central-1'\n",
    "region = \"us-east-1\"\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254cf207",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"spanish\"\n",
    "model = \"simple_transformers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772a9339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To execute just in case the process was cbroken and we need to continueu filling the json file\n",
    "\n",
    "# path = 'C:/Users/hrpeg/Documents/GitHub/policy-data-analyzer/tasks/database_analysis/output/'\n",
    "# file_name = f'sentence_metadata_{model.split(\"/\")[0]}{language}.json'\n",
    "# file = path + file_name\n",
    "# with open(file, 'r') as fp:\n",
    "#     results_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74b32739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8291\r"
     ]
    }
   ],
   "source": [
    "in_prefix = f\"{language}_documents/HSSC/{model}updated_sentences/\"\n",
    "\n",
    "start_point = 0\n",
    "i = 0\n",
    "\n",
    "results_dict = {'n_sentences': [], 'min_sent_length': [], 'max_sent_length': [], 'avg_sent_length': [],\n",
    "                'number_of_incentives': [], 'Supplies': [], 'Loan': [], 'Fine': [], 'Direct payment': [], \n",
    "                'Technical assistance': [], 'Tax benefit': []}\n",
    "\n",
    "for obj in s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix):#.:\n",
    "    if \".json\" in obj.key and \"ing\" not in obj.key:\n",
    "        i += 1\n",
    "        if i > start_point:\n",
    "            print(i,  end='\\r')\n",
    "            name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "            flag = False\n",
    "            sentences = json.loads(obj.get()['Body'].read().decode('utf-8'))\n",
    "            for metadata, value in sentences[name][\"metadata\"].items():\n",
    "                try:\n",
    "                    results_dict[metadata].append(value)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "path = 'C:/Users/hrpeg/Documents/GitHub/policy-data-analyzer/tasks/database_analysis/output/'\n",
    "file_name = f'sentence_metadata_{model.split(\"/\")[0]}_{language}.json'\n",
    "file = path + file_name\n",
    "with open(file, 'w') as fp:\n",
    "    json.dump(results_dict, fp, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1150e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Users/hrpeg/Documents/GitHub/policy-data-analyzer/tasks/database_analysis/output/'\n",
    "file_name = f'sentence_metadata_{model.split(\"/\"[0])}_{language}.json'\n",
    "file = path + file_name\n",
    "with open(file, 'w') as fp:\n",
    "    json.dump(results_dict, fp, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d688ba79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3eda60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7731"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results_dict[\"n_sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08081cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#             incentive_counter = 0\n",
    "#             sentences_counter = 0\n",
    "#             min_sentence_length = 100000\n",
    "#             max_sentence_length = 0\n",
    "#             sum_sentence_length = 0\n",
    "#             last_sentence_position = 0\n",
    "#             incentives_dict = {'Supplies': 0,\n",
    "#                                'Loan': 0,\n",
    "#                                'Fine': 0,\n",
    "#                                'Direct payment': 0,\n",
    "#                                'Technical assistance': 0,\n",
    "#                                'Tax benefit': 0}\n",
    "#             for sentence_id in sentences[name][\"sentences\"]:\n",
    "#                 sentences_counter += 1\n",
    "#                 sentence = sentences[name][\"sentences\"][sentence_id][\"text\"]\n",
    "#                 min_sentence_length, max_sentence_length, sum_sentence_length = sentence_stats(sentence, min_sentence_length, max_sentence_length, sum_sentence_length)\n",
    "#                 incentive = sentences[name][\"sentences\"][sentence_id][\"label\"]  \n",
    "#                 if len(incentive) > 1 and incentive[0] == \"Incentive\":\n",
    "#                     incentive_counter += 1\n",
    "#                     sentence_number = sentence_id.split(\"_\")[2]\n",
    "#                     policy = sentences[name][\"sentences\"][sentence_id][\"label\"][1]\n",
    "#                     if last_sentence_position == 0:\n",
    "#                         results_list.append([])\n",
    "#                         last_sentence_position = sentence_number\n",
    "#                         sentences[name][\"sentences\"][sentence_id][\"closest_sentence\"] = 0\n",
    "#                         flag = True\n",
    "#                     else:\n",
    "#                         sentences[name][\"sentences\"][sentence_id][\"closest_sentence\"] = int(sentence_number) - int(last_sentence_position)\n",
    "#                         last_sentence_position = sentence_number\n",
    "\n",
    "#                     relative_position = int(sentence_number) / sentences[name][\"metadata\"][\"n_sentences\"]\n",
    "#                     sentences[name][\"sentences\"][sentence_id][\"relative_position\"] = round(relative_position, 2)\n",
    "#                     incentives_dict[policy] += 1\n",
    "#     #                 print(incentive, \"\\n\", sentence_id, \" ** \", last_sentence_position)\n",
    "\n",
    "#             # Updating the dictionary\n",
    "#             sentences[name][\"metadata\"][\"min_sent_length\"] = min_sentence_length\n",
    "#             sentences[name][\"metadata\"][\"max_sent_length\"] = max_sentence_length\n",
    "#             sentences[name][\"metadata\"][\"avg_sent_length\"] = int(sum_sentence_length/sentences_counter)\n",
    "#             sentences[name][\"metadata\"][\"number_of_incentives\"] = incentive_counter\n",
    "#             for key, value in incentives_dict.items():\n",
    "#                 sentences[name][\"metadata\"][key] = value\n",
    "#             s3.Object(s3BucketName, f\"{out_prefix}{name}_sents.json\").put(Body = (json.dumps(sentences, indent = 4)))\n",
    "#             # Creating the results list\n",
    "#             if flag:\n",
    "#                 documents_with_incentives += 1\n",
    "#                 results_list[documents_with_incentives].append(name)\n",
    "#                 results_list[documents_with_incentives].append(incentive_counter)\n",
    "#                 for key, value in incentives_dict.items():\n",
    "#                     results_list[documents_with_incentives].append(value)\n",
    "\n",
    "# with open(f\"../output/incentives_simple_transformers_{language}.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57519894",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
