{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data compilation from database scan\n",
    "\n",
    "The trained models have been used for scaning the whole database looking for incentives.\\\n",
    "Each sentence has been firs classified into one of to categories, *Incentive* or *Not-Incentive*.\\\n",
    "Sentences which were classified as incentives were further classified into one of the six policy instruments.\n",
    "\n",
    "Thus \n",
    "* Number of sentences per document\n",
    "* Average length of sentences\n",
    "* Length of smallest sentence\n",
    "* Length of longest sentence\n",
    "* Relative distance of the first incentive from the head of the document\n",
    "* Distance to the closest previous incentive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Users/jordi/Documents/claus/'\n",
    "# path = '/home/propietari/Documents/claus/'\n",
    "path = 'C:/Users/hrpeg/Documents/claus/'\n",
    "filename = 'AWS_S3_keys_wri.json'\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "# s3BucketName = \"wri-testing\"\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "# region = 'eu-central-1'\n",
    "region = \"us-east-1\"\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_stats(sentence_text, min_sentence_length, max_sentence_length, sum_sentence_length):\n",
    "    sum_sentence_length = sum_sentence_length + len(sentence_text)\n",
    "    if len(sentence_text) < min_sentence_length:\n",
    "        min_sentence_length = len(sentence_text)\n",
    "    if len(sentence_text) > max_sentence_length:\n",
    "        max_sentence_length = len(sentence_text)\n",
    "    return min_sentence_length, max_sentence_length, sum_sentence_length\n",
    "\n",
    "\n",
    "# def sentences_processing()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"/home/propietari/Documents/GitHub/policy-data-analyzer/tasks/database_analysis/input/\"\n",
    "path = \"C:/Users/hrpeg/Documents/GitHub/policy-data-analyzer/tasks/database_analysis/input/\"\n",
    "file_name = \"ElSalvadorKeys.json\"\n",
    "file_name = \"english_new_document_sentences.json\"\n",
    "file = path + file_name\n",
    "\n",
    "with open(file, 'r') as fp:\n",
    "    filter_keys = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3490\r"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-06b0cde5dca8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"min_sent_length\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_sentence_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"max_sent_length\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_sentence_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"avg_sent_length\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum_sentence_length\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msentences_counter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"metadata\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"number_of_incentives\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mincentive_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mincentives_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "language = \"english\"\n",
    "model = \"\"#\"simple_transformers/\"\n",
    "in_prefix = f\"{language}_documents/HSSC/{model}sentences/\"\n",
    "out_prefix = f\"{language}_documents/HSSC/{model}updated_sentences/\"\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "documents_with_incentives = 0\n",
    "results_list = [[\"file_name\", \"n_incentives\", \"Supplies\", \"Loan\", \"Fine\", \"Direct payment\", \"Technical assistance\", \"Tax benefit\"]]\n",
    "incentives_dict = {}\n",
    "\n",
    "for obj in s3.Bucket(s3BucketName).objects.all().filter(Prefix = in_prefix):#.:\n",
    "    if \".json\" in obj.key and \"ing\" not in obj.key:\n",
    "        i += 1\n",
    "        name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "        if i % 1000 == 0:\n",
    "            print(i,  end='\\r')\n",
    "        if name in filter_keys:\n",
    "            j += 1\n",
    "            print(j, end = \"\\r\")\n",
    "#             print(name)\n",
    "            \n",
    "#           output.clear()\n",
    "#         if i < 100000:\n",
    "    #         print(f\"\\n\\nFile number: {i}\\n\\n\")\n",
    "#             print(i,  end='\\r')\n",
    "#             name = obj.key.split(\"/\")[-1].split(\"_\")[0]\n",
    "            flag = False\n",
    "            sentences = json.loads(obj.get()['Body'].read().decode('utf-8'))\n",
    "            incentive_counter = 0\n",
    "            sentences_counter = 0\n",
    "            min_sentence_length = 100000\n",
    "            max_sentence_length = 0\n",
    "            sum_sentence_length = 0\n",
    "            last_sentence_position = 0\n",
    "            incentives_dict = {'Supplies': 0,\n",
    "                               'Loan': 0,\n",
    "                               'Fine': 0,\n",
    "                               'Direct payment': 0,\n",
    "                               'Technical assistance': 0,\n",
    "                               'Tax benefit': 0}\n",
    "            for sentence_id in sentences[name][\"sentences\"]:\n",
    "                sentences_counter += 1\n",
    "                sentence = sentences[name][\"sentences\"][sentence_id][\"text\"]\n",
    "                min_sentence_length, max_sentence_length, sum_sentence_length = sentence_stats(sentence, min_sentence_length, max_sentence_length, sum_sentence_length)\n",
    "                incentive = sentences[name][\"sentences\"][sentence_id][\"label\"]  \n",
    "                if len(incentive) > 1 and incentive[0] == \"Incentive\":\n",
    "                    incentive_counter += 1\n",
    "                    sentence_number = sentence_id.split(\"_\")[2]\n",
    "                    policy = sentences[name][\"sentences\"][sentence_id][\"label\"][1]\n",
    "                    if last_sentence_position == 0:\n",
    "                        results_list.append([])\n",
    "                        last_sentence_position = sentence_number\n",
    "                        sentences[name][\"sentences\"][sentence_id][\"closest_sentence\"] = 0\n",
    "                        flag = True\n",
    "                    else:\n",
    "                        sentences[name][\"sentences\"][sentence_id][\"closest_sentence\"] = int(sentence_number) - int(last_sentence_position)\n",
    "                        last_sentence_position = sentence_number\n",
    "\n",
    "                    relative_position = int(sentence_number) / sentences[name][\"metadata\"][\"n_sentences\"]\n",
    "                    sentences[name][\"sentences\"][sentence_id][\"relative_position\"] = round(relative_position, 2)\n",
    "                    incentives_dict[policy] += 1\n",
    "    #                 print(incentive, \"\\n\", sentence_id, \" ** \", last_sentence_position)\n",
    "\n",
    "            # Updating the dictionary\n",
    "            sentences[name][\"metadata\"][\"min_sent_length\"] = min_sentence_length\n",
    "            sentences[name][\"metadata\"][\"max_sent_length\"] = max_sentence_length\n",
    "            sentences[name][\"metadata\"][\"avg_sent_length\"] = int(sum_sentence_length/sentences_counter)\n",
    "            sentences[name][\"metadata\"][\"number_of_incentives\"] = incentive_counter\n",
    "            for key, value in incentives_dict.items():\n",
    "                sentences[name][\"metadata\"][key] = value\n",
    "            s3.Object(s3BucketName, f\"{out_prefix}{name}_sents.json\").put(Body = (json.dumps(sentences, indent = 4)))\n",
    "#             # Creating the results list\n",
    "#             if flag:\n",
    "#                 documents_with_incentives += 1\n",
    "#                 results_list[documents_with_incentives].append(name)\n",
    "#                 results_list[documents_with_incentives].append(incentive_counter)\n",
    "#                 for key, value in incentives_dict.items():\n",
    "#                     results_list[documents_with_incentives].append(value)\n",
    "\n",
    "# with open(f\"../output/incentives_simple_transformers_{language}.csv\", \"w\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerows(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incentives_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
