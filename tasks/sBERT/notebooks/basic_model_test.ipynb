{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activate the environment where the wri project is located. If there is not one, install requirements in the wrilatinamerica.txt. https://github.com/OmdenaAI/wrilatinamerica/blob/master/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda activate wri_omdena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import segment_highlighter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "import string\n",
    "\n",
    "import nltk # imports the natural language toolkit\n",
    "import plotly\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from PyPDF2 import PdfFileReader\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we use the excel file with the selected phrases of each country, process them and get N-grams to define basic queries for the SBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'WRI_Policy_Tags (1).xlsx', sheet_name = None)\n",
    "df = None\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        if not isinstance(df,pd.DataFrame):\n",
    "            df = value\n",
    "        else:\n",
    "            df = df.append(value)\n",
    "else:\n",
    "    df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[\"relevant sentences\"].apply(lambda x: x.split(\";\") if isinstance(x,str) else x)\n",
    "sentence = []\n",
    "\n",
    "for elem in sentences:\n",
    "    if isinstance(elem,float) or len(elem) == 0:\n",
    "        continue\n",
    "    elif isinstance(elem,list):\n",
    "        for i in elem:\n",
    "            if len(i.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sentence.append(i.strip())\n",
    "    else:\n",
    "        if len(elem.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            sentence.append(elem.strip())\n",
    "\n",
    "sentence\n",
    "words_per_sentence = [len(x.split(\" \")) for x in sentence]\n",
    "plt.hist(words_per_sentence, bins = 50)\n",
    "plt.title(\"Histogram of number of words per sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "\n",
    "noise_words = []\n",
    "stopwords_corpus = nltk.corpus.stopwords\n",
    "sp_stop_words = stopwords_corpus.words('spanish')\n",
    "noise_words.extend(sp_stop_words)\n",
    "print(len(noise_words))\n",
    "\n",
    "if \"no\" in noise_words:\n",
    "    noise_words.remove(\"no\")\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(''.join(sentence))\n",
    "word_freq = Counter(tokenized_words)\n",
    "# word_freq.most_common(20)\n",
    "# list(ngrams(tokenized_words, 3))\n",
    "\n",
    "word_tokens_clean = [re.findall(r\"[a-zA-Z]+\",each) for each in tokenized_words if each.lower() not in noise_words and len(each.lower()) > 1]\n",
    "word_tokens_clean = [each[0].lower() for each in word_tokens_clean if len(each)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams size\n",
    "We define the size of the n-gram that we want to find. The larger it is, the less frequent it will be, unless we substantially increase the number of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 2\n",
    "\n",
    "top_k_ngrams(word_tokens_clean, n_grams, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "filename = \"Omdena_key.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_dict:\n",
    "    KEY = key\n",
    "    SECRET = key_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the list of objects in the bucket that are relevant\n",
    "policy_list = s3.Bucket('wri-latin-policies').objects.all().filter(Prefix='full')\n",
    "\n",
    "## This allows to loop through the files\n",
    "\n",
    "# i = 0\n",
    "# for obj in s3.Bucket('wri-latin-policies').objects.all().filter(Prefix='full'):\n",
    "#     if i < 1: #Limit for testing purposes. if present the loop will go only through the first element\n",
    "#         key = \"Pre-processed/\" + obj.key.replace(\"full/\", '') + \".txt\" \n",
    "#         file = obj.get()['Body'].read() #get the file from S3\n",
    "#         pdf = PdfFileReader(BytesIO(file)) #load the file in pdf format if necessary\n",
    "#         for page in range(0, pdf.getNumPages()):\n",
    "#             pdf_page = pdf.getPage(page) #Retrieve the content of each page\n",
    "#             pdf_content = pdf_page.extractText() #Extract only the text of each page\n",
    "# #           HERE YOU SHOULD RUN YOUR PRE-PROCESSING PIPELINE AND ADD UP EVERY PAGE IN A VARIABLE called \"content\" as string\n",
    "#         s3.Object('wri-latin-policies', key).put(Body = content)#This will save all the contents in the string variable \"content\" into a txt file in the Pre-processed folder\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the sBERT model. Several transformers are available and documentation is here: https://github.com/UKPLab/sentence-transformers <br>\n",
    "\n",
    "The following functions are:\n",
    "- Get cosine similarity between two texts\n",
    "- Highlight: a function that receives the model, document, query and precision and returns all the highlights that are above that precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_name='xlm-r-100langs-bert-base-nli-mean-tokens'\n",
    "transformer_name = \"distiluse-base-multilingual-cased\"\n",
    "model = SentenceEmbeddings(transformer_name)\n",
    "\n",
    "def get_similarity(model, text1, text2):\n",
    "    '''\n",
    "    Given two texts, calculate the cosine similarity between their sentence embeddings.\n",
    "    '''\n",
    "    text1_embedding = model.encode(text1)\n",
    "    text2_embedding = model.encode(text2)\n",
    "    return 1 - distance.cosine(text1_embedding, text2_embedding)\n",
    "\n",
    "\n",
    "def highlight(model, document, query, precision):\n",
    "\n",
    "    '''document must be the json or txt document to be able to extract page'''\n",
    "    highlights = []\n",
    "    scores = []\n",
    "    pages = []\n",
    "    \n",
    "## Modify this part to change the processing of the json / dict policy ---------------------------\n",
    "\n",
    "    if isinstance(document, dict): \n",
    "        for page_num, text in document.items():\n",
    "            \n",
    "            ## This section is preprocessing ---------------------------------------\n",
    "            page_num = page_num.split(\"_\")[1]\n",
    "            for sentence in text.split(\"\\n\\n\"):\n",
    "                sentence = re.sub(\"\\n\", \" \", sentence)\n",
    "                sentence = re.sub(\" +\", \" \", sentence)\n",
    "                sentence = sentence.strip()\n",
    "                if len(sentence) < 60:\n",
    "                    continue\n",
    "             ## ---------------------------------------------------------------------------\n",
    "             ## Next, get the scores and stores the highlights\n",
    "                \n",
    "                score = get_similarity(model, sentence, query)\n",
    "                if score > precision:\n",
    "                    highlights.append(sentence)\n",
    "                    scores.append(score)\n",
    "                    pages.append(page_num)\n",
    "        sorted_idxs = np.argsort(scores)[::-1]\n",
    "        highlights = [highlights[idx] for idx in sorted_idxs]\n",
    "        scores = [scores[idx] for idx in sorted_idxs]\n",
    "        pages = [pages[idx] for idx in sorted_idxs]\n",
    "\n",
    "        return highlights, scores, pages\n",
    "# -----------------------------------------------------------------------------\n",
    "    else:\n",
    "        preprocessor = TextPreprocessor()\n",
    "        clean_text = preprocessor.clean_sentence(document)\n",
    "        paragraphs = preprocessor.split_into_paragraphs(document)\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            paragraph = re.sub(\"\\n\", \" \", sentence)\n",
    "            paragraph = re.sub(\" +\", \" \", sentence)\n",
    "            paragraph = paragraph.strip()\n",
    "            if len(paragraph) < 60:\n",
    "                continue\n",
    "            score = get_similarity(model, paragraph, query)\n",
    "            if score > precision:\n",
    "                highlights.append(paragraph)\n",
    "                scores.append(score)\n",
    "        sorted_idxs = np.argsort(scores)[::-1]\n",
    "        highlights = [highlights[idx] for idx in sorted_idxs]\n",
    "        scores = [scores[idx] for idx in sorted_idxs]\n",
    "\n",
    "        return highlights, scores, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure module\n",
    "highlighter_class = \"sbert\"\n",
    "highlighter_id = \"X\"\n",
    "highlighter_query = \"beneficio económico\"\n",
    "highlighter_precision = 0.05\n",
    "\n",
    "# Instantiate models from the Omdena files\n",
    "# highlighter_class = highlighter_classes[highlighter_class]\n",
    "# highlighter = highlighter_class.load(highlighter_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_folder = r\"_____\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for policy in policy_list:  #Uncomment and comment the next 2 lines if the files are fetched from S3 bucket.\n",
    "\n",
    "highlight_list = []\n",
    "scores_list = []\n",
    "pages_list = []\n",
    "\n",
    "for policy in os.listdir(policy_folder):\n",
    "    policy = os.path.join(policy_folder, policy)\n",
    "    \n",
    "    if \"json\" in policy:\n",
    "        with open(policy_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            policy = json.load(f)\n",
    "        highlights, score, pages = highlight(\n",
    "            policy, highlighter_query, highlighter_precision\n",
    "        )\n",
    "            \n",
    "    elif \"txt\" in policy:\n",
    "        policy = open(policy_path, \"r\", encoding=\"utf-8\")\n",
    "        highlights, score, pages = highlight(\n",
    "            policy, highlighter_query, highlighter_precision\n",
    "        )\n",
    "    \n",
    "    highlight_list.append(highlights)\n",
    "    scores_list.append(highlights)\n",
    "    pages_list.append(highlights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
