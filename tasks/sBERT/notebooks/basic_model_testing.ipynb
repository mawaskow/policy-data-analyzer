{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a search engine based on sBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook there is a basic implementation of sBERT for searching a database of sentences with queries.\n",
    "\n",
    "The goal is to increase the amount of labeled data that we have in order to later fine tune a model to be used for sentence classification. First of all we have to find a pool of queries that represent the six labels of the six policy instruments. With these queries we can pull a set of sentences that can be automaticaly labeled with the same label of the query. In this way we can increase the diversity of labeled sentences in each label category. This approach will be complemented with a manual curation step to produce a high quality training data set.\n",
    "\n",
    "The policy instruments that we want to find and that correspond to the different labels are:\n",
    "* Direct payment (PES)\n",
    "* Tax deduction\n",
    "* Credit/guarantee\n",
    "* Technical assistance\n",
    "* Supplies\n",
    "* Fines\n",
    "\n",
    "This notebook is intended for the following purposes:\n",
    "* Try different query strategies to find the optimal retrieval of sentences in each policy instrument category\n",
    "* Try different transformers\n",
    "* Be the starting point for further enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is self contained, it does not depend on any other class of the sBERT folder.\n",
    "\n",
    "You just have to create an environment where you install the external dependencies. Usually the dependencies that you have to install are:\n",
    "\n",
    "**For the basic sentence similarity calculation**\n",
    "*  pandas\n",
    "*  boto3\n",
    "*  pytorch\n",
    "*  sentence_transformers\n",
    "\n",
    "**If you want to use ngrams to generate queries**\n",
    "*  nltk\n",
    "*  plotly\n",
    "*  wordcloud\n",
    "\n",
    "**If you want to do evaluation and ploting with pyplot**\n",
    "*  matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your environment is called nlp then you execute this cell otherwise you change the name of the environment\n",
    "!conda activate nlp"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 1,
=======
   "execution_count": 8,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# General purpose libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
<<<<<<< Updated upstream
=======
    "import time\n",
>>>>>>> Stashed changes
    "\n",
    "# Model libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Libraries for model evaluation\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Libraries to be used in the process of definig queries\n",
    "import nltk # imports the natural language toolkit\n",
    "import plotly\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "## Defining Queries"
=======
    "## Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents from El Salvador have been preprocessed and their contents saved in a JSON file. In the JSON file there are the sentences of interest.\n",
    "\n",
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep the credentials in a local folder out of GitHub, you can change the path to adapt it to your needs.\n",
    "# Please, comment out other users lines and set your own\n",
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in desktop\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in laptop\n",
    "# path = \"\"\n",
    "#If you put the credentials file in the same \"notebooks\" folder then you can use the following path\n",
    "# path = \"\"\n",
    "filename = \"Omdena_key_S3.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_dict:\n",
    "    KEY = key\n",
    "    SECRET = key_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentence database from El Salvador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'JSON/ElSalvador.json'\n",
    "\n",
    "obj = s3.Object('wri-latin-talent',filename)\n",
    "serializedObject = obj.get()['Body'].read()\n",
    "policy_list = json.loads(serializedObject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a list of potentially relevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "                    \"POR TANTO\" : 0,\n",
    "                    \"DISPOSICIONES GENERALES\" : 0,\n",
    "                    \"OBJETO\" : 0,\n",
    "                    \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0}\n",
    "sentences = {}\n",
    "def slim_dict(counter, slim_factor): # This is to shrink the sentences dict by a user set factor. It will pick only one sentence every \"slim_factor\"\n",
    "    if counter % slim_factor == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "slim_by = 1\n",
    "count = 0\n",
    "for key, value in policy_list.items():\n",
    "    for item in value: \n",
    "        if item in is_not_incentive:\n",
    "            continue\n",
    "        else:\n",
    "            for sentence in policy_list[key][item]['sentences']:\n",
    "                count += 1\n",
    "                if slim_dict(count, slim_by):\n",
    "                    sentences[sentence] = policy_list[key][item]['sentences'][sentence]\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10031"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)\n",
    "# for sentence in sentences:\n",
    "#     print(sentences[sentence]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams approach"
   ]
>>>>>>> Stashed changes
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following lines, we use the excel file with the selected phrases of each country, process them and get N-grams to define basic queries for the SBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'WRI_Policy_Tags (1).xlsx', sheet_name = None)\n",
    "df = None\n",
    "\n",
    "if isinstance(data, dict):\n",
    "    for key, value in data.items():\n",
    "        if not isinstance(df,pd.DataFrame):\n",
    "            df = value\n",
    "        else:\n",
    "            df = df.append(value)\n",
    "else:\n",
    "    df = data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df[\"relevant sentences\"].apply(lambda x: x.split(\";\") if isinstance(x,str) else x)\n",
    "sentence = []\n",
    "\n",
    "for elem in sentences:\n",
    "    if isinstance(elem,float) or len(elem) == 0:\n",
    "        continue\n",
    "    elif isinstance(elem,list):\n",
    "        for i in elem:\n",
    "            if len(i.strip()) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sentence.append(i.strip())\n",
    "    else:\n",
    "        if len(elem.strip()) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            sentence.append(elem.strip())\n",
    "\n",
    "sentence\n",
    "words_per_sentence = [len(x.split(\" \")) for x in sentence]\n",
    "plt.hist(words_per_sentence, bins = 50)\n",
    "plt.title(\"Histogram of number of words per sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "\n",
    "noise_words = []\n",
<<<<<<< Updated upstream
    "stopwords_corpus = nltk.corpus.stopwords\n",
=======
    "stopwords_corpus = nltk.corpus.stopword\n",
>>>>>>> Stashed changes
    "sp_stop_words = stopwords_corpus.words('spanish')\n",
    "noise_words.extend(sp_stop_words)\n",
    "print(len(noise_words))\n",
    "\n",
    "if \"no\" in noise_words:\n",
    "    noise_words.remove(\"no\")\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(''.join(sentence))\n",
    "word_freq = Counter(tokenized_words)\n",
    "# word_freq.most_common(20)\n",
    "# list(ngrams(tokenized_words, 3))\n",
    "\n",
    "word_tokens_clean = [re.findall(r\"[a-zA-Z]+\",each) for each in tokenized_words if each.lower() not in noise_words and len(each.lower()) > 1]\n",
    "word_tokens_clean = [each[0].lower() for each in word_tokens_clean if len(each)>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "### n-grams size\n",
=======
>>>>>>> Stashed changes
    "We define the size of the n-gram that we want to find. The larger it is, the less frequent it will be, unless we substantially increase the number of phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 2\n",
    "\n",
    "top_k_ngrams(word_tokens_clean, n_grams, 20)"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accesing documents in S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents from El Salvador have been preprocessed and their contents saved in a JSON file. In the JSON file there are the sentences of interest.\n",
    "\n",
    "Use the json file with the key and password to access the S3 bucket if necessary. \n",
    "If not, skip this section and use files in a local folder. "
=======
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword approach"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to keep the credentials in a local folder out of GitHub, you can change the path to adapt it to your needs.\n",
    "# Please, comment out other users lines and set your own\n",
    "path = \"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in desktop\n",
    "# path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\" # Jordi's local path in laptop\n",
    "# path = \"\"\n",
    "#If you put the credentials file in the same \"notebooks\" folder then you can use the following path\n",
    "# path = \"\"\n",
    "filename = \"Omdena_key_S3.json\"\n",
    "file = path + filename\n",
    "with open(file, 'r') as dict:\n",
    "    key_dict = json.load(dict)"
=======
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"asitencia técnica\", \n",
    "            \" ayuda \", \" ayudas \", \n",
    "            \" bono \", \" bonos \", \n",
    "            \" crédito \", \" créditos \", \n",
    "            \" deducción \", \" deducciones \",\n",
    "            \" devolución \", \" devoluciones \",\n",
    "            \" incentivo \", \" incentivos \", \n",
    "            \" insumo \", \" insumos \", \n",
    "            \" multa \", \" multas \"]\n",
    "\n",
    "families = {\n",
    "    \"asitencia técnica\" : \"asitencia técnica\",\n",
    "    \" ayuda \" : \"ayuda\",\n",
    "    \" ayudas \" : \"ayuda\",\n",
    "    \" bono \" : \"bono\",\n",
    "    \" bonos \" : \"bono\",\n",
    "    \" crédito \":  \"crédito\",\n",
    "    \" créditos \":  \"crédito\",\n",
    "    \" deducción \" : \"deducción\",\n",
    "    \" deducciones \" : \"deducción\",\n",
    "    \" devolución \" : \"devolución\",\n",
    "    \" devoluciones \" : \"devolución\",\n",
    "    \" incentivo \" : \"incentivo\",\n",
    "    \" incentivos \" : \"incentivo\",\n",
    "    \" insumo \" : \"insumo\",\n",
    "    \" insumos \" : \"insumo\",\n",
    "    \" multa \" : \"multa\",\n",
    "    \" multas \" : \"multa\"\n",
    "}"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in key_dict:\n",
    "    KEY = key\n",
    "    SECRET = key_dict[key]"
=======
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_in_sentences = []\n",
    "for sentence in sentences:\n",
    "    for keyword in keywords:\n",
    "        if keyword in sentences[sentence]['text'].lower():\n",
    "            keyword_in_sentences.append([families[keyword], sentences[sentence]['text']]) \n",
    "            "
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the sentence database from El Salvador"
=======
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['devolución',\n",
       "  'Asimismo, formaran parte de los bienes indicados en el articulo anterior, las contribuciones indicadas en la Ley de Creacion del Fondo de Emergencia para el Cafe que correspondan al pago de las obligaciones financieras que el Fondo adeuda al Ministerio de Hacienda, asi como las recuperaciones en dinero de las contribuciones adeudadas por los productores de cafe que el Fondo realice a partir de la entrada en vigencia de esta Ley, despues de haber realizado las deducciones o devoluciones que por la Ley de Creacion del Fondo de Emergencia para el Cafe correspondan']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(keyword_in_sentences))\n",
    "keyword_in_sentences = sorted(keyword_in_sentences, key = lambda x : x[0])\n",
    "# print(keyword_in_sentences[0:20])\n",
    "filtered = [row for row in keyword_in_sentences if row[0] == \"devolución\"]\n",
    "filtered"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'JSON/ElSalvador.json'\n",
    "\n",
    "obj = s3.Object('wri-latin-talent',filename)\n",
    "serializedObject = obj.get()['Body'].read()\n",
    "policy_list = json.loads(serializedObject)"
=======
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assitencia técnica -- 0\n",
      "ayuda -- 11\n",
      "bono -- 6\n",
      "crédito -- 0\n",
      "deducción -- 1\n",
      "devolución -- 1\n",
      "incentivo -- 66\n",
      "insumo -- 30\n",
      "multa -- 1253\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in families.items():\n",
    "    if i % 2 == 0:\n",
    "        print(value, \"--\", len([row for row in keyword_in_sentences if row[0] == value]))\n",
    "    i += 1\n",
    "    "
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "### Building a list of potentially relevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_not_incentive = {\"CONSIDERANDO:\" : 0,\n",
    "                    \"POR TANTO\" : 0,\n",
    "                    \"DISPOSICIONES GENERALES\" : 0,\n",
    "                    \"OBJETO\" : 0,\n",
    "                    \"COMPETENCIA, PROCEDIMIENTOS Y RECURSOS.\" : 0}\n",
    "sentences = {}\n",
    "for key, value in policy_list.items():\n",
    "    for item in value:\n",
    "        if item in is_not_incentive:\n",
    "            continue\n",
    "        else:\n",
    "            for sentence in policy_list[key][item]['sentences']:\n",
    "                sentences[sentence] = policy_list[key][item]['sentences'][sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)\n",
    "# for sentence in sentences:\n",
    "#     print(sentences[sentence]['text'])"
=======
    "## Using the model"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "## Initializing the model"
=======
    "### Initializing the model"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the sBERT model. Several transformers are available and documentation is here: https://github.com/UKPLab/sentence-transformers <br>\n",
    "\n",
    "Then we build a simple function that takes four inputs:\n",
    "1. The model as we have set it in the previous line of code\n",
    "2. A dictionary that contains the sentences {\"\\<sentence_ID\\>\" : {\"text\" : \"The actual sentence\", labels : []}\n",
    "3. A query in the form of a string\n",
    "4. A similarity treshold. It is a float that we can use to limit the results list to the most relevant.\n",
    "\n",
    "The output of the function is a list with three columns with the following content:\n",
    "1. Column 1 contains the id of the sentence\n",
    "2. Column 2 contains the similarity score\n",
    "3. Column 3 contains the text of the sentence that has been compared with the query"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 11,
=======
   "execution_count": 13,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer_name='xlm-r-100langs-bert-base-nli-mean-tokens'\n",
    "transformer_name = \"distiluse-base-multilingual-cased\"\n",
    "model = SentenceTransformer(transformer_name)\n",
    "\n",
    "def highlight(model, sentences_dict, query, similarity_treshold):\n",
    "    query_embedding = model.encode(query)\n",
    "    highlights = []\n",
    "    for sentence in sentences_dict:\n",
    "        sentence_embedding = model.encode(sentence)\n",
    "        score = 1 - distance.cosine(sentence_embedding, query_embedding)\n",
    "        if score > similarity_treshold:\n",
<<<<<<< Updated upstream
    "            highlights.append([sentence, score, document[sentence]['text']])\n",
=======
    "            highlights.append([sentence, score, sentences_dict[sentence]['text']])\n",
>>>>>>> Stashed changes
    "    highlights = sorted(highlights, key = lambda x : x[1], reverse = True)\n",
    "    return highlights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "## Running the search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ti = time.perf_counter()\n",
    "\n",
    "highlighter_query = \"Todos aquellos agricultores que fueron beneficiados con el otorgamiento de hasta dos créditos\"\n",
=======
    "### Running the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity search for El Salvador sentences done in 623.1546 seconds\n"
     ]
    }
   ],
   "source": [
    "Ti = time.perf_counter()\n",
    "\n",
    "highlighter_query = \"bono\"\n",
>>>>>>> Stashed changes
    "highlighter_precision = 0.2\n",
    "\n",
    "label_1 = highlight(model, sentences, highlighter_query, highlighter_precision)\n",
    "\n",
    "Tf = time.perf_counter()\n",
    "\n",
    "print(f\"similarity search for El Salvador sentences done in {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting the results"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_1))\n",
    "label_1[0:10]"
=======
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['00a55af_1',\n",
       "  0.7085415720939636,\n",
       "  'El Concejo Municipal de Candelaria de la Frontera, Departamento de Santa Ana'],\n",
       " ['00a55af_64',\n",
       "  0.6808415055274963,\n",
       "  '- Previa calificacion del ente competente, los Planes de Manejo para la conservacion de las areas naturales, debera contener los objetivos señalados en el art 79 de la Ley de Medio Ambiente'],\n",
       " ['00a55af_65',\n",
       "  0.6789363622665405,\n",
       "  'Tambien la Municipalidad en coordinacion con la Mancomunidad Trinacional Fronteriza Rio Lempa, promoveran el Manejo Sostenible para la conservacion de las Areas Naturales y Manejo Sostenible de los Bosques'],\n",
       " ['00a55af_44',\n",
       "  0.6731448769569397,\n",
       "  'De los planes sostenibles de los bosques'],\n",
       " ['00a55af_46',\n",
       "  0.6717902421951294,\n",
       "  '- Sin detrimento de lo establecido en otras disposiciones concernientes al aprovechamiento de madera, leña, recursos no renovables y del ecoturismo de los bosques, la municipalidad vigilara, que los permisos otorgados por las diferentes instituciones se cumplan al tenor de lo ahi consignado; so pena de aplicar la presente ordenanza y de realizar las acciones legales correspondientes'],\n",
       " ['00a55af_67',\n",
       "  0.6712285280227661,\n",
       "  'Los propietarios de terrenos privados y los pobladores ubicados en estas zonas, podran realizar actividades productivas sujetandose a las normas tecnicas y a los usos del suelo que el Plan de Manejo respectivo establezca, o en su defecto lo que determine la Unidad Ambiental Municipal en coordinacion con el Ministerio de Medio Ambiente y Recursos Naturales'],\n",
       " ['00a55af_71',\n",
       "  0.6708526015281677,\n",
       "  '- En caso de producirse un incendio en las Zonas de Amortiguamiento, se aplicara lo dispuesto en el art 26 de la Ley Forestal'],\n",
       " ['00a55af_30',\n",
       "  0.6703100204467773,\n",
       "  'Cobrar Multas Ambientales con base al Codigo Municipal y a la presente Ordenanza'],\n",
       " ['00a55af_43',\n",
       "  0.669257402420044,\n",
       "  'Ademas, implementara un programa de desarrollo economico y social, con las familias que viven dentro y en las cercanias de los bosques, para ir cambiando el habito del consumo de la leña por otras alternativas que beneficien al bosque'],\n",
       " ['00a55af_34',\n",
       "  0.669070303440094,\n",
       "  'Organizar e integrar Comites Asesores Locales, como el principal instrumento de participacion ciudadana y coordinacion entre el Area Natural y su espacio social aledaño, de acuerdo al articulo 8 Literal c) de la Ley de Areas Naturales Protegidas'],\n",
       " ['00a55af_105',\n",
       "  0.6689942479133606,\n",
       "  'Incumplir las medidas y disposiciones que se dicten sobre plagas, enfermedades forestales y otras: 3 a 5 salarios minimos'],\n",
       " ['00a55af_115',\n",
       "  0.6688268780708313,\n",
       "  '- La municipalidad solamente tendra competencia para sancionar las infracciones aludidas en esta'],\n",
       " ['00a55af_55',\n",
       "  0.6683053970336914,\n",
       "  '- En relacion al Art 4 de la Ley del Medio Ambiente, la municipalidad protegera, conservara y manejara sosteniblemente los recursos naturales que prestan bienes y servicios ambientales a los pobladores'],\n",
       " ['00a55af_56',\n",
       "  0.6681047081947327,\n",
       "  'Ademas, declarara y administrara las micro cuencas productoras de agua y reconocimiento de los corredores biologicos como estrategia de integracion y ordenamiento territorial'],\n",
       " ['00a55af_120',\n",
       "  0.6679098010063171,\n",
       "  'Art el Delegado Municipal, que tuviere conocimiento por cualquier medio, quien iniciara el procedimiento y'],\n",
       " ['00a55af_61',\n",
       "  0.6673386693000793,\n",
       "  'Para esta clase de ayuda sera prioritario planificar proyectos que sean gestionados ante los organismos respectivos'],\n",
       " ['00a55af_31',\n",
       "  0.6667841672897339,\n",
       "  'Fomentar la cultura de la denuncia individual y/o colectiva, principalmente de los delitos y faltas ambientales reguladas en la Ley de Medio Ambiente, Ley Forestal, Ley de Areas Naturales Protegidas, esta ordenanza y otros instrumentos legales existentes'],\n",
       " ['00a55af_101',\n",
       "  0.6661826968193054,\n",
       "  '- Se prohibe ubicar asentamientos humanos, bases militares, instalaciones industriales o de cualquier otro tipo en las zonas boscosas, areas naturales y zonas de amortiguamiento'],\n",
       " ['00a55af_73',\n",
       "  0.6652201414108276,\n",
       "  '- La Municipalidad dispondra como minimo de dos Guarda Recursos para que desempeñe funciones de vigilancia y promocion ambiental de los Bosques, Areas Naturales y Zonas de Amortiguamiento del municipio, quienes seran capacitados, para conocer todo lo que respecta al manejo sostenible de los bosques, proteccion y conservacion de areas naturales protegidas y zonas de amortiguamiento, en terminos tecnicos, cientificos legales, administrativos y las demas que sean necesarias, procurando la inclusion de mujeres de los cuales el 25% podran ser del sexo femenino'],\n",
       " ['00a55af_51', 0.6647389531135559, 'Uso Publico de los bosques']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(label_1))\n",
    "label_1[0:20]"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< Updated upstream
    "##### Further filtering of the results by using the similarity score"
=======
    "#### Further filtering of the results by using the similarity score"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_treshold = 0.5\n",
    "filtered = [row for row in label_1 if row[1] > similarity_treshold]\n",
    "filtered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
<<<<<<< Updated upstream
=======
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "757px",
    "left": "586px",
    "top": "630px",
    "width": "309px"
   },
   "toc_section_display": true,
   "toc_window_display": true
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
