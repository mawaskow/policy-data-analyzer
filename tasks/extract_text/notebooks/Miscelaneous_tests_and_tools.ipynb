{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fitted-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import codecs\n",
    "import csv\n",
    "import datetime\n",
    "import holidays\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-television",
   "metadata": {},
   "source": [
    "### To adjust the filter for dates in the USFR spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "solved-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_business_day(date, time_span, time_unit, country_code):\n",
    "    ONE_DAY = datetime.timedelta(days=1)\n",
    "    HOLIDAYS = holidays.CountryHoliday(country_code)\n",
    "    next_day = date + ONE_DAY\n",
    "    while next_day.weekday() in holidays.WEEKEND or next_day in HOLIDAYS:\n",
    "       next_day += ONE_DAY\n",
    "    return next_day\n",
    "\n",
    "def create_date_list(start_date, to_date, time_span, time_unit, country_code):\n",
    "    dates = []\n",
    "    start_date = datetime.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    to_date = datetime.datetime.strptime(to_date, '%Y-%m-%d').date()\n",
    "    while start_date < to_date:\n",
    "        start_date = next_business_day(start_date, time_span, time_unit, country_code)\n",
    "        dates.append(start_date)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "iraqi-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-04\n",
      "2010-01-05\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/propietari/Documents/GitHub/policy-data-analyzer/tasks/Scrapy/scrapy_official_newspapers/output/\"\n",
    "file_name = \"USFR_20210310_1.csv\"\n",
    "file = path + file_name\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "with open(file, 'r', errors=\"surrogateescape\") as f:   \n",
    "    reader = csv.reader(f)\n",
    "    for row in reader: #unicode(reader, errors='ignore'):\n",
    "        dates_dict[datetime.datetime.strptime(row[6], '%Y-%m-%d').date()] = 0\n",
    "        \n",
    "from_date = \"2010-01-01\"\n",
    "to_date = \"2010-01-07\"\n",
    "country_code = \"US\"\n",
    "for day in create_date_list(from_date, to_date, 1, \"days\", country_code):\n",
    "    if day in dates_dict:\n",
    "        print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wrapped-uniform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(from_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-professor",
   "metadata": {},
   "source": [
    "### To remove duplicates from csv by file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "reverse-drove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of entries: 18202 \n",
      "* Number of unique entries: 15594\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/propietari/Documents/GitHub/policy-data-analyzer/tasks/Scrapy/scrapy_official_newspapers/output/\"\n",
    "file_name = \"USFR_20210310.csv\"\n",
    "file = path + file_name\n",
    "\n",
    "no_dups_dict = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "with open(file, 'r', errors=\"surrogateescape\") as f:  # \n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        i += 1\n",
    "        no_dups_dict[row[3]] = row\n",
    "        \n",
    "print(\"* Number of entries:\", i, \"\\n* Number of unique entries:\", len(no_dups_dict))\n",
    "\n",
    "file_name = \"USFR_metadata.csv\"\n",
    "file = path + file_name\n",
    "i = 0\n",
    "with open(file, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for key, value in no_dups_dict.items():\n",
    "        value[0] = value[0].replace(\"\\udca0\", \"\")\n",
    "        try:\n",
    "            writer.writerow([ value[0], value[1], value[2], value[3], value[4], value[5], value[6], value[7], value[8], value[9], value[10], value[11]])\n",
    "        except:\n",
    "            print(\"* \", value[3], \"could not be saved. \", value[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-consumption",
   "metadata": {},
   "source": [
    "### To upload csv form S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adapted-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to AWS\n",
    "folder = '/home/propietari/Documents/claus/' # TODO: change to your local path\n",
    "file_name = 'AWS_S3_keys_WRI.json' # TODO: Change to your filename\n",
    "file = folder + file_name\n",
    "\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "difficult-overhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "# Connection to AWS\n",
    "folder = '/home/propietari/Documents/claus/' # TODO: change to your local path\n",
    "file_name = 'AWS_S3_keys_WRI.json' # TODO: Change to your filename\n",
    "file = folder + file_name\n",
    "\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")\n",
    "\n",
    "# Load file\n",
    "\n",
    "# You can adapt it to get the whole contents of each row of the bucket updating a list or lists that replicates\n",
    "# the csv \n",
    "\n",
    "# The counter just for testing purposes\n",
    "i = 0\n",
    "\n",
    "file_key = \"India_metadata.csv\"\n",
    "\n",
    "obj = s3.Object(bucket_name = s3BucketName, key = file_key)\n",
    "for row in csv.reader(codecs.getreader(\"utf-8\")(obj.get()['Body'])):\n",
    "    if i < 5:\n",
    "        print(row[3])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-harrison",
   "metadata": {},
   "source": [
    "### To compare two different methods to access object in a S3 bucket\n",
    "\n",
    "The conclusion is that accessing all objects with *objects.all()* is much faster that looping through keys and uploading each object one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "starting-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing 0 objects from the S3 bucket has taken 9.8081 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "file_key = \"metadata/India_metadata.csv\"\n",
    "obj_key_dict = {}\n",
    "\n",
    "obj = s3.Object(bucket_name = s3BucketName, key = file_key)\n",
    "for row in csv.reader(codecs.getreader(\"utf-8\")(obj.get()['Body'])):\n",
    "    if i < 1001:\n",
    "#         print(row[3])\n",
    "        obj_key_dict[\"english_documents/text_files/\" + row[3]] = 0\n",
    "    i += 1\n",
    "    \n",
    "# j = 0\n",
    "# k = 1\n",
    "# Ti = time.perf_counter()\n",
    "# for item in obj_key_dict:\n",
    "#     try:\n",
    "#         obj = s3.Object(bucket_name = s3BucketName, key = file_key)\n",
    "#         file = obj.get()['Body'].read()\n",
    "#         j += 1\n",
    "#     except:\n",
    "#         print(f'{k} ** the file {item} was not found')\n",
    "\n",
    "# Tf = time.perf_counter()\n",
    "\n",
    "# print(f\"Accessing {j} objects from the S3 bucket has taken {Tf - Ti:0.4f} seconds\")\n",
    "j = 0\n",
    "Ti = time.perf_counter()\n",
    "for obj in s3.Bucket(s3BucketName).objects.all().filter(Prefix='english_documents/text_files/'):\n",
    "    if obj.key in  obj_key_dict:\n",
    "#         file = obj.get()['Body'].read()\n",
    "#         print(obj.key)\n",
    "        j += 1\n",
    "        \n",
    "Tf = time.perf_counter()\n",
    "print(f\"Accessing {j} objects from the S3 bucket has taken {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "professional-stretch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15594 ** english_documents/text_files/dd0ccf82ae888eca4c9926f3dcb393b85c0d6c76.txt\n"
     ]
    }
   ],
   "source": [
    "keyword = \"dd0ccf82ae888eca4c9926f3dcb393b85c0d6c76\"\n",
    "for obj in s3.Bucket(s3BucketName).objects.all().filter(Prefix=\"english_documents/text_files/\"):\n",
    "    if keyword in obj.key: #lower_limit < i < upper_limit:\n",
    "        print(i, \"**\", obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-computer",
   "metadata": {},
   "source": [
    "### To check the differences between the csv database and the contents of a bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-recall",
   "metadata": {},
   "source": [
    "##### Connection to the AWS servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "adult-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "# Connection to AWS\n",
    "folder = '/home/propietari/Documents/claus/' # TODO: change to your local path\n",
    "file_name = 'AWS_S3_keys_WRI.json' # TODO: Change to your filename\n",
    "file = folder + file_name\n",
    "\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-minneapolis",
   "metadata": {},
   "source": [
    "##### Loading file names from database and building dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "driven-fusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1040 elements in the database out of which 1040 are unique\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "file_key = \"metadata/India_metadata.csv\"\n",
    "obj_key_dict = {}\n",
    "obj_name_dict = {}\n",
    "\n",
    "obj = s3.Object(bucket_name = s3BucketName, key = file_key)\n",
    "for row in csv.reader(codecs.getreader(\"utf-8\")(obj.get()['Body'])):\n",
    "#         print(row[3])\n",
    "    obj_key_dict[\"english_documents/text_files/\" + row[3][:-4]] = 0\n",
    "    i += 1\n",
    "print(f\"There are {i} elements in the database out of which {len(obj_key_dict)} are unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-mining",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "reduced-springfield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 394 objects from the S3 bucket in the database. \n",
      " There are 646 files in the database which haven't been transformed\n",
      " The omputation has taken 11.2305 seconds\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "k = 0\n",
    "Ti = time.perf_counter()\n",
    "in_dict = {}\n",
    "for obj in s3.Bucket(s3BucketName).objects.all().filter(Prefix='english_documents/text_files/'):\n",
    "#     print(obj.key[:-4])\n",
    "    if obj.key[:-4] in  obj_key_dict:\n",
    "        in_dict[obj.key[:-4]] = 0\n",
    "#         print(obj.key)\n",
    "        j += 1\n",
    "out_dict = {}\n",
    "for item in obj_key_dict:\n",
    "    if item not in in_dict:\n",
    "        out_dict[item] = 0\n",
    "        k += 1\n",
    "        \n",
    "Tf = time.perf_counter()\n",
    "print(f\"There are {j} objects from the S3 bucket in the database. \\n There are {k} files in the database which haven't been transformed\\n The omputation has taken {Tf - Ti:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "decreased-milan",
   "metadata": {},
   "outputs": [
    {
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-hunger",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
