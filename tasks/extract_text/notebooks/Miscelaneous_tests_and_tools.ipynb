{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fitted-rescue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import codecs\n",
    "import csv\n",
    "import datetime\n",
    "import holidays\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-television",
   "metadata": {},
   "source": [
    "### To adjust the filter for dates in the USFR spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "solved-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_business_day(date, time_span, time_unit, country_code):\n",
    "    ONE_DAY = datetime.timedelta(days=1)\n",
    "    HOLIDAYS = holidays.CountryHoliday(country_code)\n",
    "    next_day = date + ONE_DAY\n",
    "    while next_day.weekday() in holidays.WEEKEND or next_day in HOLIDAYS:\n",
    "       next_day += ONE_DAY\n",
    "    return next_day\n",
    "\n",
    "def create_date_list(start_date, to_date, time_span, time_unit, country_code):\n",
    "    dates = []\n",
    "    start_date = datetime.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    to_date = datetime.datetime.strptime(to_date, '%Y-%m-%d').date()\n",
    "    while start_date < to_date:\n",
    "        start_date = next_business_day(start_date, time_span, time_unit, country_code)\n",
    "        dates.append(start_date)\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "iraqi-warning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-04\n",
      "2010-01-05\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/propietari/Documents/GitHub/policy-data-analyzer/tasks/Scrapy/scrapy_official_newspapers/output/\"\n",
    "file_name = \"USFR_20210310_1.csv\"\n",
    "file = path + file_name\n",
    "\n",
    "dates_dict = {}\n",
    "\n",
    "with open(file, 'r', errors=\"surrogateescape\") as f:   \n",
    "    reader = csv.reader(f)\n",
    "    for row in reader: #unicode(reader, errors='ignore'):\n",
    "        dates_dict[datetime.datetime.strptime(row[6], '%Y-%m-%d').date()] = 0\n",
    "        \n",
    "from_date = \"2010-01-01\"\n",
    "to_date = \"2010-01-07\"\n",
    "country_code = \"US\"\n",
    "for day in create_date_list(from_date, to_date, 1, \"days\", country_code):\n",
    "    if day in dates_dict:\n",
    "        print(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wrapped-uniform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(from_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-professor",
   "metadata": {},
   "source": [
    "### To remove duplicates from csv by file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sublime-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of entries: 18202 \n",
      "* Number of unique entries: 15594\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/propietari/Documents/GitHub/policy-data-analyzer/tasks/Scrapy/scrapy_official_newspapers/output/\"\n",
    "file_name = \"USFR_20210310.csv\"\n",
    "file = path + file_name\n",
    "\n",
    "no_dups_dict = {}\n",
    "\n",
    "i = 0\n",
    "\n",
    "with open(file, 'r') as f:  #, errors=\"surrogateescape\" \n",
    "    reader = csv.reader(f)\n",
    "    for row in reader: #unicode(reader, errors='ignore'):\n",
    "        i += 1\n",
    "        no_dups_dict[row[3]] = row\n",
    "        \n",
    "print(\"* Number of entries:\", i, \"\\n* Number of unique entries:\", len(no_dups_dict))\n",
    "\n",
    "file_name = \"USFR_metadata.csv\"\n",
    "file = path + file_name\n",
    "i = 0\n",
    "with open(file, 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for key, value in no_dups_dict.items():\n",
    "#         value[0] = value[0].replace(\"\\udca0\", \"\")\n",
    "        try:\n",
    "            writer.writerow([ value[0], value[1], value[2], value[3], value[4], value[5], value[6], value[7], value[8], value[9], value[10], value[11]])\n",
    "        except:\n",
    "            print(\"* \", value[3], \"could not be saved. \", value[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-consumption",
   "metadata": {},
   "source": [
    "### To upload csv form S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adapted-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to AWS\n",
    "folder = '/home/propietari/Documents/claus/' # TODO: change to your local path\n",
    "file_name = 'AWS_S3_keys_WRI.json' # TODO: Change to your filename\n",
    "file = folder + file_name\n",
    "\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "difficult-overhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859644d69fdd0b1e66f1dcf5a42f2d9a3f31fe90.pdf\n",
      "876b66a43ea68e055193465940377107b9cd4fcc.pdf\n",
      "e898428130c73dd8be1dfc58fe2198ab388ebd96.pdf\n",
      "fe8b9097194caaed0b8985f4a4dddeca06833fa4.pdf\n",
      "afca018efb184ada2624bba04c111a054a068037.pdf\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "# Connection to AWS\n",
    "folder = '/home/propietari/Documents/claus/' # TODO: change to your local path\n",
    "file_name = 'AWS_S3_keys_WRI.json' # TODO: Change to your filename\n",
    "file = folder + file_name\n",
    "\n",
    "with open(file, 'r') as dict:\n",
    "    credentials = json.load(dict)\n",
    "                                      \n",
    "KEY = list(credentials)[0]\n",
    "SECRET = list(credentials.values())[0]\n",
    "s3BucketName = \"wri-nlp-policy\"\n",
    "region = 'us-east-1'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = region,\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")\n",
    "\n",
    "# Load file\n",
    "\n",
    "# You can adapt it to get the whole contents of each row of the bucket updating a list or lists that replicates\n",
    "# the csv \n",
    "\n",
    "# The counter just for testing purposes\n",
    "i = 0\n",
    "\n",
    "file_key = \"India_metadata.csv\"\n",
    "\n",
    "obj = s3.Object(bucket_name = s3BucketName, key = file_key)\n",
    "for row in csv.reader(codecs.getreader(\"utf-8\")(obj.get()['Body'])):\n",
    "    if i < 5:\n",
    "        print(row[3])\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-classification",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
