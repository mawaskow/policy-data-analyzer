{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Headings Curation and sentences spliter of Chilean Policies\n",
    "\n",
    "In this notebook there are a series of dictionaries and methods to curate section headings of El Salvador policies. Policies from El Salvador have a rather definite structure, so that the law text is organized under section headings. There are two kinds of sections, the ones that are general and that can be often found in many policies, and the ones which are specific. The sections headings which are more general often come with a whole range of name variants which makes the task of machine recognition difficult.\n",
    "\n",
    "The goal of this notebook is to group all pretreatment methods that would harmonize sections heading to make the further processing machine friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import boto3, json, operator, os, re, string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries of particular vocabularies to help in the curation of section headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most policies come with the final signatures. This is a piece of text that we want to be able to recognize. To make the\n",
    "# detection of signatures easier, this dictionary contain the most common terms that can be found in these lines of text.\n",
    "official_positions = {\"ALCALDE\" : 0,\n",
    "\"Alcalde\" : 0,\n",
    "\"MINISTRA\" : 0,\n",
    "\"Ministra\" : 0,\n",
    "\"MINISTRO\" : 0,\n",
    "\"Ministro\" : 0,\n",
    "\"PRESIDENTA\" : 0,\n",
    "\"Presidenta\" : 0,\n",
    "\"PRESIDENTE\" : 0,\n",
    "\"Presidente\" : 0,\n",
    "\"REGIDOR\" : 0,\n",
    "\"Regidor\"  : 0,\n",
    "\"REGIDORA\" : 0,\n",
    "\"regidora\" : 0,\n",
    "\"SECRETARIA\" : 0,\n",
    "\"Secretaria\" : 0,\n",
    "\"SECRETARIO\" : 0,\n",
    "\"Secretario\" : 0,\n",
    "\"SINDICA\" : 0,\n",
    "\"Sindica\" : 0,\n",
    "\"SINDICO\" : 0,\n",
    "\"Sindico\" : 0,\n",
    "\"VICEPRESIDENTA\" : 0,\n",
    "\"Vicepresidenta\" : 0,\n",
    "\"VICEPRESIDENTE\" : 0,\n",
    "\"Vicepresidente\" : 0\n",
    "}\n",
    "\n",
    "end_of_file_tags = {\n",
    "    \"Anótese\" : 0,\n",
    "    \"Publíquese\" : 0\n",
    "}\n",
    "# This dictionary contains some correspondences among different text headings. This is under development and needs further\n",
    "# improvement.The idea is to merge in a single name all the headings that point to the same conceptual concept. For example,\n",
    "# \"Definiciones\" is a heading that can come alone or together with other terms so it can appear as \"Definiciones básicas\" or\n",
    "# \"Definiciones generales\". With the dictionary we can fetch all headings that contain the word \"Definiciones\" and change the\n",
    "# heading to \"Definiciones\".\n",
    "merges = {\n",
    "    \"CONCEPTOS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"Considerando:\" : \"CONSIDERANDO\",\n",
    "    \"DEFINICIONES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES FINALES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES GENERALES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES PRELIMINARES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES REGULADORAS\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES RELATIVAS\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES ESPECIALES\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES TRANSITORIAS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"INCENTIVOS\" : \"INCENTIVOS\",\n",
    "    \"INFRACCIONES\" : \"INFRACCIONES\",\n",
    "    \"INFRACCION ES\" : \"INFRACCIONES\",\n",
    "    \"OBJETIVO\" : \"OBJETO\",\n",
    "    \"OBJETO\" : \"OBJETO\",\n",
    "    \"DERECHOS\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"DEBERES\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACIONES\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACIONE\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACION\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGATORIEDAD\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"PROHIBICIONES\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"PROHIBICION\" : \"DERECHOS, OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"POR TANTO\" : \"POR TANTO\",\n",
    "    \"POR LO TANTO\" : \"POR TANTO\",\n",
    "    \"Decreto:\" : \"RESUELVO\",\n",
    "    \"Resuelvo:\" : \"RESUELVO\",\n",
    "    \"Se resuelve\" : \"RESUELVO\",\n",
    "    \"S e  r e s u e l v e:\" : \"RESUELVO\",\n",
    "    \"Visto:\" : \"VISTO\",\n",
    "    \"Vistos:\" : \"VISTO\",\n",
    "    \"Vistos estos antecedentes:\" : \"VISTO\",\n",
    "    \"--------------\" : \"HEADING\"\n",
    "}\n",
    "merges_lower = {}\n",
    "for key, value in merges.items():\n",
    "    merges_lower[key.lower()] = value\n",
    "# Eventhough the general gramar rule in Spanish is not to accent uppercase, there are many cases where a word in a heding might\n",
    "# appear accented. This is a dictionary to armonize all headings without accents. The list is rather comprehensive, but there is\n",
    "# still room for improvement.\n",
    "# If we find some bug beyond simple misspelling which will be solved by spell checker, we can include it here. The example is in\n",
    "# the first row with \"ACTIVIDADESUSOS\" which was found several times in headings.\n",
    "bugs = {\"ACTIVIDADESUSOS\" : \"ACTIVIDADES DE USOS\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to the AWS S3 bucket\n",
    "To effectively run this cell you need Omdena's credentials. Please keep them local and do not sync them in GitHub repos nor cloud drives. Before doing anything with this json file, please think of security!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "# json_folder = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "filename = \"Omdena_key.json\"\n",
    "file = json_folder / filename\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    cred = json.load(f) \n",
    "\n",
    "for key in cred:\n",
    "    KEY = key\n",
    "    SECRET = cred[key]\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the uppercase ratio in a string. It is used to detect section headings\n",
    "def uppercase_ratio(string):\n",
    "    if len(re.findall(r'[a-z]',string)) == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return(len(re.findall(r'[A-Z]',string))/len(re.findall(r'[a-z]',string)))\n",
    "\n",
    "def end_of_heading(line):\n",
    "    if \"URL\" in line and \"https:\" in line:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def end_of_document(line):\n",
    "    end_of_file = False\n",
    "    for key in end_of_file_tags:\n",
    "        if key in line:\n",
    "            end_of_file = True\n",
    "            break\n",
    "    return end_of_file\n",
    "# Regular expression to clear html tags (here is basically to remove the page tags)\n",
    "cleanr = re.compile(r'<.*?>')\n",
    "# Te function to clear html tags\n",
    "def clean_html_tags(string):\n",
    "  return cleanr.sub('', string)\n",
    "\n",
    "def is_section(line):\n",
    "    for key in merges:\n",
    "        if key in line:\n",
    "            line = merges[key]\n",
    "            break\n",
    "    return line\n",
    "    \n",
    "def is_por_tanto(line):\n",
    "    if \"POR TANTO\" in line:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Function to remove the last lines of a document, the ones that contain the signatures of the officials. It depends on the\n",
    "# dictionary \"official_positions\"\n",
    "def remove_signatures(line):\n",
    "    signature = False\n",
    "    for key in official_positions:\n",
    "        if key in line:\n",
    "            signature = True\n",
    "            break\n",
    "    return signature\n",
    "\n",
    "# Function to change accented words by non-accented counterparts. It depends on the dictionary \"accent_marks_bugs\" \n",
    "accents_out = re.compile(r'[áéíóúÁÉÍÓÚ]')\n",
    "accents_dict = {\"á\":\"a\",\"é\":\"e\",\"í\":\"i\",\"ó\":\"o\",\"ú\":\"u\",\"Á\":\"A\",\"É\":\"E\",\"Í\":\"I\",\"Ó\":\"O\",\"Ú\":\"U\"}\n",
    "def remove_accents(string):\n",
    "    for accent in accents_out.findall(string):\n",
    "        string = string.replace(accent, accents_dict[accent])\n",
    "    return string\n",
    "\n",
    "# Function to merge headlines expressing the same concept in different words. It depends on the dictionary \"merges\"\n",
    "def merge_concepts(line):\n",
    "    for key in merges:\n",
    "        if key in line:\n",
    "            line = merges[key]\n",
    "            break\n",
    "    return line\n",
    "\n",
    "def clean_bugs(line):\n",
    "    for key in bugs:\n",
    "        if key in line:\n",
    "            line = line.replace(key, bugs[key])\n",
    "    return line\n",
    "\n",
    "clean_special_char = re.compile(r'(\\*\\.)|(\\”\\.)')  \n",
    "def clean_special_characters(line):\n",
    "    char = clean_special_char.findall(line)\n",
    "    for item in char:\n",
    "        for character in item:\n",
    "            if character != '':\n",
    "                line = line.replace(character, \"\")\n",
    "    return line\n",
    "\n",
    "clean_acron = re.compile(r'(A\\s*\\.M\\s*\\.)|(\\bart\\s*\\.)|(\\bArt\\s*\\.)|(\\bART\\s*\\.)|(\\bArts\\s*\\.)|(\\bAV\\s*\\.)|(\\bDr\\s*\\.)|(\\bIng\\s*\\.)|(\\bLic\\s*\\.)|(\\bLicda\\s*\\.)|(\\bLIC\\s*\\.)|(mts\\s*\\.)|(\\bNo\\s*\\.)|(P\\s*\\.M\\s*\\.)|(prof\\s*\\.)|(profa\\s*\\.)|(sp\\s*\\.)|(ssp\\s*\\.)|(to\\s*\\.)|(ta\\s*\\.)|(var\\s*\\.)')  \n",
    "def clean_acronyms(line):\n",
    "    acro = clean_acron.findall(line)\n",
    "    for item in acro:\n",
    "        for acronym in item:\n",
    "            if acronym != '':\n",
    "                line = line.replace(acronym, clean_punct.sub('', acronym))\n",
    "    return line\n",
    "\n",
    "whitespaces = re.compile(r'[ ]{2,}')\n",
    "def clean_whitespace(line):\n",
    "    return whitespaces.sub(' ', line).rstrip().lstrip()\n",
    "\n",
    "decimal_points = re.compile(r'(\\b\\d+\\s*\\.\\s*\\d+)')\n",
    "def change_decimal_points(line):\n",
    "    dec = decimal_points.findall(line)\n",
    "    for decimal in dec:\n",
    "        if decimal != '':\n",
    "#             print(decimal)\n",
    "            line = line.replace(decimal, clean_punct.sub(',', decimal))\n",
    "    return line\n",
    "                \n",
    "# Regular expression to clear punctuation from a string\n",
    "clean_punct = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# Regular expression to clear words that introduce unnecessary variability to headings. Some regex still not work 100% we need\n",
    "# to improve them.\n",
    "clean_capitulo = re.compile(r'(APARTADO \\S*)|(APARTADO\\s)|(^ART\\.\\s*\\S*)|(^ART\\.\\s*)|(^Art\\.\\s*\\S*)|(^Art\\.\\s*)|(^Arts\\.\\s*\\S*)|(Capítulo \\S*)|(CAPITULO \\S*)|(CAPITULO\\S*)|(CAPÍTULO \\S*)|(CAPITULÓ \\S*)|(CAPITULOS \\S*)|(CAPITUO \\S*)|(CATEGORIA\\b)|(CATEGORÍA\\b)|(SUBCATEGORIA\\b)|(SUBCATEGORÍA\\b)|(TITULO\\s\\S*)|(TÍTULO\\s\\S*)')\n",
    "clean_bullet_char = re.compile(r'\\b[A-Za-z]\\s*\\.|\\b[A-Za-z]\\s*\\.\\s*|\\b[A-Za-z]\\s*\\-\\s*|\\b[A-Za-z]\\s*\\)\\s*|\\.\\s*\\b[B-Za-z]\\b|\\b[A-Z]{1,4}\\s*\\.|^\\d+\\s*\\.\\s*\\D+|\\d+\\)')\n",
    "clean_bullet_point = re.compile(r'^-\\s*')\n",
    "# Function sentence\n",
    "def clean_sentence(string):\n",
    "    string = clean_capitulo.sub('', string)\n",
    "    string = clean_bullet_char.sub('', string).rstrip().lstrip()\n",
    "    string = clean_bullet_point.sub('', string).rstrip().lstrip()\n",
    "#     string = clean_punct.sub('', string).rstrip().lstrip()\n",
    "    if string != \"\":\n",
    "        return string\n",
    "    else:\n",
    "        return None    \n",
    "    \n",
    "# points = re.compile(r'(\\b\\w+\\s*\\.\\s*\\b[^\\d\\W]+)')\n",
    "# def check_points(line):\n",
    "#     return points.findall(line)\n",
    "#     print(points.findall(line))\n",
    "\n",
    "points = re.compile(r'(\\b\\w+\\b\\s*){3,}')\n",
    "def check_sentence(line):\n",
    "    if points.findall(line):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def split_into_sentences(line):\n",
    "    sentence_list = []\n",
    "    for sentence in line.split(\".\"):\n",
    "        if check_sentence(sentence):\n",
    "            sentence = sentence.rstrip().lstrip()\n",
    "            sentence_list.append(sentence)\n",
    "    return sentence_list\n",
    "\n",
    "# Function to add items to the dictionary with duplicate removal\n",
    "def add_to_dict(string, dictionary, dupl_dict):\n",
    "    if string in dupl_dict or string == None:\n",
    "        pass\n",
    "    else:\n",
    "        dupl_dict[string] = 0\n",
    "        if string in dictionary:\n",
    "            dictionary[string] = dictionary[string] + 1\n",
    "        else:\n",
    "            dictionary[string] = 1\n",
    "    return dictionary\n",
    "def full_cleaning(line):\n",
    "    line = clean_html_tags(line)\n",
    "    line = remove_accents(line)\n",
    "    line = clean_special_characters(line)\n",
    "    line = clean_bugs(line)\n",
    "    line = clean_acronyms(line)\n",
    "    line = clean_whitespace(line)\n",
    "    line = clean_sentence(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que el Art 204 Ordinal 3 y 5 de la Constitución, regula. Hola, em dic Jordi. No sé massa perquè l'Art 22 conté 22,34€. Tanmateix sembla que la Licda una cosa. voldria 55,22. no fotis\n",
      "['Que el Art 204 Ordinal 3 y 5 de la Constitución, regula', 'Hola, em dic Jordi', \"No sé massa perquè l'Art 22 conté 22,34€\", 'Tanmateix sembla que la Licda una cosa']\n"
     ]
    }
   ],
   "source": [
    "test_string = \"Que el Art. 204 Ordinal 3*. y 5”. de la Constitución, regula. A. Hola, em dic Jordi. B. No sé massa perquè l'Art *. 22 conté 22.34€. Tanmateix sembla que la Licda. una cosa. voldria  55.22. no fotis\"\n",
    "# test_string = \"Prova senzilleta per veure què passa si no hi ha punt\"\n",
    "test_string = clean_sentence(test_string)\n",
    "test_string = clean_special_characters(test_string)\n",
    "test_string = clean_acronyms(test_string)\n",
    "test_string = clean_whitespace(test_string)\n",
    "test_string = change_decimal_points(test_string)\n",
    "print(test_string)\n",
    "sentences = []\n",
    "[sentences.append(sentence) for sentence in split_into_sentences(test_string)]\n",
    "print(sentences)\n",
    "# print(sentences)\n",
    "\n",
    "# if check_sentence(test_string):\n",
    "#     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to process files from S3 bucket\n",
    "By executing this cell you will go through all policies in El Salvador and process section headings that will be saved in a dictionary. This should be merged with the notebook that builds up the final json files out of plain txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = \"text-extraction/\"\n",
    "out_folder = \"JSON/\"\n",
    "counter = 0\n",
    "name4 = {}\n",
    "name5 = {}\n",
    "name6 = {}\n",
    "name7 = {}\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix='text-extraction'):\n",
    "    if in_folder in obj.key and obj.key.replace(in_folder, \"\") != \"\":# and filename in obj.key   # Un comment the previous string to run the code just in one sample document.\n",
    "        file = obj.get()['Body'].read().decode('utf-8')  #get the file from S3 and read the body content\n",
    "        lines = file.split(\"\\n\") # Split by end of line and pipe lines into a list\n",
    "        file_name = obj.key.replace(in_folder, \"\").replace('.pdf.txt', '')        \n",
    "        name4[file_name[0:4]] = 0\n",
    "        name5[file_name[0:5]] = 0\n",
    "        name6[file_name[0:6]] = 0\n",
    "        name7[file_name[0:7]] = 0\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter)\n",
    "print(len(name4))\n",
    "print(len(name5))\n",
    "print(len(name6))\n",
    "print(len(name7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = \"text-extraction/\"\n",
    "out_folder = \"JSON/\"\n",
    "filename = \"00a55afe4f55256567397a68df5d7f97e642480b\" # This is only if you want to test on a single file\n",
    "# bag_of_words = {}\n",
    "# sentences = []\n",
    "# sentences_dict = {}\n",
    "json_file = {}\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix='text-extraction'):\n",
    "    if in_folder in obj.key and obj.key.replace(in_folder, \"\") != \"\":# and filename in obj.key   # Un comment the previous string to run the code just in one sample document.\n",
    "        file = obj.get()['Body'].read().decode('utf-8')  #get the file from S3 and read the body content\n",
    "        lines = file.split(\"\\n\") # Split by end of line and pipe lines into a list\n",
    "        key = obj.key.replace(in_folder, out_folder).replace('pdf.txt', 'json')\n",
    "        file_name = key.replace('.json', '').replace(out_folder, '')\n",
    "#         print(file_name)\n",
    "        json_file[file_name] = {}\n",
    "        duplicates_dict = {} #Sometimes the same heading can be found more than once in a document. This will help on removing them\n",
    "        section = \"\"\n",
    "        line_counter = 0\n",
    "        i = 0\n",
    "        is_title = False\n",
    "        for line in lines:\n",
    "            if is_section(line, i) and is_title is False or is_por_tanto(line):\n",
    "                if remove_signatures(line):\n",
    "                    break\n",
    "                else:\n",
    "                    line = clean_html_tags(line)\n",
    "                    line = remove_accents(line)\n",
    "                    line = clean_bugs(line)\n",
    "                    line = clean_sentence(line)\n",
    "                    if line == None:\n",
    "                        continue\n",
    "                    section = merge_concepts(line)\n",
    "#                     print(\"** Section:\", section)\n",
    "                    json_file[file_name][section] = {\"tags\" : [], \"sentences\" : {}}\n",
    "                    is_title = is_por_tanto(section)\n",
    "    #                     bag_of_words = add_to_dict(line, bag_of_words, duplicates_dict)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                line = clean_html_tags(line)\n",
    "                line = remove_accents(line)\n",
    "                line = clean_special_characters(line)\n",
    "                line = clean_bugs(line)\n",
    "                line = clean_acronyms(line)\n",
    "                line = clean_whitespace(line)\n",
    "                line = clean_sentence(line)\n",
    "                if line == None:\n",
    "                    continue\n",
    "                if is_title:\n",
    "                    line_counter += 1\n",
    "                    sentence_id = file_name[0:7] + '_' + str(line_counter)\n",
    "                    json_file[file_name][section][\"sentences\"][sentence_id] = {\"text\" : line, \"labels\" : []}\n",
    "                    \n",
    "                else:\n",
    "                    for sentence in split_into_sentences(line):\n",
    "                        line_counter += 1\n",
    "                        sentence_id = file_name[0:7] + '_' + str(line_counter)\n",
    "                        json_file[file_name][section][\"sentences\"][sentence_id] = {\"text\" : sentence, \"labels\" : []}\n",
    "                is_title = False\n",
    "            i += 1 \n",
    "#         s3.Object('wri-latin-talent', key).put(Body = str(json.dumps(json_file)))#This will save all the contents in the string variable \"content\" into a txt file in the Pre-processed folder\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/Processed/\")\n",
    "filename = \"ElSalvador.json\"\n",
    "file = out_folder / filename\n",
    "with open(file, 'w') as fp:\n",
    "    json.dump(json_file, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences_dict))\n",
    "for k in sorted(sentences_dict):\n",
    "    print(k, \":\", sentences_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After preprocessing there are {} different headings in El Salvador policies\".format(len(bag_of_words)))\n",
    "print(\"{} documents have been processed\".format(i))\n",
    "print(\"There are {} lines of text as sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict( sorted(bag_of_words.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by heading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bag_of_words):\n",
    "    print(k, \":\", bag_of_words[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving sentences as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/\")\n",
    "path = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/\")\n",
    "filename = \"sentences.npy\"\n",
    "file = path / filename\n",
    "np_sentences = np.array(sentences)\n",
    "with open(file, 'wb') as f:\n",
    "    np.save(f, np_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline to process one file from HD folder\n",
    "This is a pipeline to process a test file in a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['002c53058e85d383b057fa4cc25a6eb8e7d401e3', '0031d55c90473158c09acded547d67d44be22325', '01203a974410a65782afca6ff2c3bdb24a84b158', '019ae0595cb8d53ae0316cf46564755b211cdc9f', '6cef0d1b7182adadd6fe887a5e76e90324c503a1', '74c55bda33a822e06e04492d5f01b9fd864e5ba6', '7546484f6dac7941d25ec5d834ce8666497290c7', '75ffb099a140f6e837c429236ce6f0b33d31a666', '76f84f42d18d124006755dd3a7f17d41c23224a5', 'aa87f53a385577ac05df15c1df9f7876e85a8661', 'aa8d47340d977381c929e5a1737bb7e94333db8f', 'f27de0a7e5242d0c32df1d637e85f2f68f394497', 'f33fed112e8a030bd07a13e9fc02f7b68e50f3e8', 'ff6cdccf62923f94bac18add8875f4b799f0adb0']\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "files = os.listdir(path)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Documents_de_mostra/Chile/\"\n",
    "data_folder = Path(path)\n",
    "filename = \"00a55afe4f55256567397a68df5d7f97e642480b.pdf.txt\"\n",
    "\n",
    "\n",
    "files = os.listdir(path)\n",
    "\n",
    "bag_of_words = {}\n",
    "json_file = {}\n",
    "\n",
    "i = 0\n",
    "for filename in files:\n",
    "    file_ = data_folder / filename\n",
    "    with open(file_, 'r', encoding = 'utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        json_file[filename] = {}\n",
    "        duplicates_dict = {} #Sometimes the same heading can be found more than once in a document. This will help on removing them\n",
    "        section_counter = 0\n",
    "        line_counter = 0\n",
    "        heading_flag = True\n",
    "        heading_content = False\n",
    "        json_file[filename][\"HEADING\"] = {\"tags\" : [], \"sentences\" : {}}\n",
    "        for line in lines:\n",
    "            # Processing document heading\n",
    "            if end_of_heading(line):\n",
    "                heading_flag = False\n",
    "                heading_content = False\n",
    "            if heading_flag:\n",
    "                if \"Tipo Norma\" in line:\n",
    "                    heading_content = True\n",
    "                if heading_content:\n",
    "                    line = full_cleaning(line)\n",
    "                    if \":\" in line:\n",
    "                        line_counter += 1\n",
    "                        sentence_id = filename[0:7] + '_' + str(line_counter)\n",
    "                        json_file[filename][\"HEADING\"][\"sentences\"][sentence_id] = {\"text\" : line, \"labels\" : []}\n",
    "                    else:\n",
    "                        json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] = json_file[filename][\"HEADING\"][\"sentences\"][sentence_id][\"text\"] + \" \" + line\n",
    "                \n",
    "                \n",
    "            line = clean_whitespace(line)\n",
    "            if uppercase_ratio(line) == 1 and len(line) > 60:\n",
    "                if remove_signatures(line):\n",
    "                    break\n",
    "                else:\n",
    "                    line = remove_accents(line)\n",
    "                    line = clean_bugs(line)\n",
    "                    line = clean_sentence(line)\n",
    "                    if line == None:\n",
    "                        continue\n",
    "                    section = merge_concepts(line)\n",
    "    #                 print(\"** Section:\", section)\n",
    "                    json[section] = {\"tags\" : [], \"sentences\" : []}\n",
    "                    bag_of_words = add_to_dict(line, bag_of_words, duplicates_dict)\n",
    "\n",
    "    #         else:\n",
    "    #             line = clean_html_tags(line)\n",
    "    #             line = remove_accents(line)\n",
    "    #             line = clean_special_characters(line)\n",
    "    #             line = clean_bugs(line)\n",
    "    #             line = clean_acronyms(line)\n",
    "    #             line = clean_whitespace(line)\n",
    "    #             line = clean_sentence(line)\n",
    "    #             if line == None:\n",
    "    #                 continue\n",
    "    #             for sentence in split_into_sentences(line):\n",
    "    #                  json[section][\"sentences\"].append({\"text\" : sentence , \"tags\" : []})\n",
    "            i += 1\n",
    "    #     data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'002c53058e85d383b057fa4cc25a6eb8e7d401e3': {'HEADING': {'tags': [],\n",
       "   'sentences': {'002c530_1': {'text': 'Tipo Norma :Decreto 3157 EXENTO',\n",
       "     'labels': []},\n",
       "    '002c530_2': {'text': 'Fecha Publicacion :16-09-2016', 'labels': []},\n",
       "    '002c530_3': {'text': 'Fecha Promulgacion :18-08-2016', 'labels': []},\n",
       "    '002c530_4': {'text': 'Organismo :MUNICIPALIDAD DE PANQUEHUE',\n",
       "     'labels': []},\n",
       "    '002c530_5': {'text': 'Titulo :APRUEBA \"ORDENANZA PARA LA EXTRACCION DE ARIDOS EN CAUCES Y ALVEOS DE CURSOS NATURALES DE AGUA QUE CONSTITUYEN BIENES NACIONALES DE USO PUBLICO Y EN POZOS LASTREROS DE PROPIEDAD PARTICULAR EN LA COMUNA DE PANQUEHUE\" Y SUS RESPECTIVOS ANEXOS',\n",
       "     'labels': []},\n",
       "    '002c530_6': {'text': 'Tipo Version :Unica De : 16-SEP-2016',\n",
       "     'labels': []},\n",
       "    '002c530_7': {'text': 'Inicio Vigencia :16-09-2016', 'labels': []},\n",
       "    '002c530_8': {'text': 'Id Norma :1094879', 'labels': []}}}},\n",
       " '0031d55c90473158c09acded547d67d44be22325': {'HEADING': {'tags': [],\n",
       "   'sentences': {'0031d55_1': {'text': 'Tipo Norma :Resolucion 447 EXENTA',\n",
       "     'labels': []},\n",
       "    '0031d55_2': {'text': 'Fecha Publicacion :27-06-2018', 'labels': []},\n",
       "    '0031d55_3': {'text': 'Fecha Promulgacion :21-06-2018', 'labels': []},\n",
       "    '0031d55_4': {'text': 'Organismo :MINISTERIO DE ENERGIA; COMISION NACIONAL DE ENERGIA',\n",
       "     'labels': []},\n",
       "    '0031d55_5': {'text': 'Titulo :INICIA DE OFICIO PROCEDIMIENTO ADMINISTRATIVO DE INVALIDACION RESPECTO DE LA RESOLUCION CNE N° 771 EXENTA, DE 2017, QUE APRUEBA INFORME TECNICO PRELIMINAR DE CALIFICACION DE INSTALACIONES DE LOS SISTEMAS DE TRANSMISION PARA EL PERIODO 2020-2023; LA RESOLUCION CNE N° 121 EXENTA, DE 2018, QUE APRUEBA LAS RESPUESTAS A OBSERVACIONES FORMULADAS AL INFORME TECNICO PRELIMINAR, Y LA RESOLUCION CNE N° 123 EXENTA, DE 2018, QUE APRUEBA EL INFORME TECNICO FINAL DE CALIFICACION DE INSTALACIONES DE LOS SISTEMAS DE TRANSMISION PARA EL PERIODO 2020-2023',\n",
       "     'labels': []},\n",
       "    '0031d55_6': {'text': 'Tipo Version :Unica De : 27-JUN-2018',\n",
       "     'labels': []},\n",
       "    '0031d55_7': {'text': 'Inicio Vigencia :27-06-2018', 'labels': []},\n",
       "    '0031d55_8': {'text': 'Id Norma :1120102', 'labels': []}}}},\n",
       " '01203a974410a65782afca6ff2c3bdb24a84b158': {'HEADING': {'tags': [],\n",
       "   'sentences': {'01203a9_1': {'text': 'Tipo Norma :Resolucion 562 EXENTA',\n",
       "     'labels': []},\n",
       "    '01203a9_2': {'text': 'Publicacion :02-08-2001', 'labels': []},\n",
       "    '01203a9_3': {'text': 'Promulgacion :25-07-2001', 'labels': []},\n",
       "    '01203a9_4': {'text': 'Organismo :MINISTERIO SECRETARIA GENERAL DE LA PRESIDENCIA; COMISION REGIONAL DEL MEDIO AMBIENTE X REGION DE LOS LAGOS',\n",
       "     'labels': []},\n",
       "    '01203a9_5': {'text': 'Titulo :ACOGE A TRAMITACION ESTUDIO DE IMPACTO AMBIENTAL DEL PROYECTO SISTEMA DE TRATAMIENTO INTEGRAL DE LAS AGUAS SERVIDAS DE PUERTO MONTT, SEGUNDA PARTE',\n",
       "     'labels': []},\n",
       "    '01203a9_6': {'text': 'Version :Unica De : 02-AGO-2001', 'labels': []},\n",
       "    '01203a9_7': {'text': 'Inicio Vigencia :02-08-2001', 'labels': []},\n",
       "    '01203a9_8': {'text': 'Id Norma :188227', 'labels': []}}}},\n",
       " '019ae0595cb8d53ae0316cf46564755b211cdc9f': {'HEADING': {'tags': [],\n",
       "   'sentences': {'019ae05_1': {'text': 'Tipo Norma :Decreto 650 EXENTO',\n",
       "     'labels': []},\n",
       "    '019ae05_2': {'text': 'Fecha Publicacion :20-01-2011', 'labels': []},\n",
       "    '019ae05_3': {'text': 'Fecha Promulgacion :25-11-2010', 'labels': []},\n",
       "    '019ae05_4': {'text': 'Organismo :MINISTERIO DE AGRICULTURA',\n",
       "     'labels': []},\n",
       "    '019ae05_5': {'text': 'Titulo :MODIFICA DECRETO Nº 355 EXENTO, DE 2008, DE LA SUBSECRETARIA DE AGRICULTURA QUE AUTORIZA CIRCULACION EN DIAS Y HORAS INHABILES A VEHICULOS QUE INDICA',\n",
       "     'labels': []},\n",
       "    '019ae05_6': {'text': 'Tipo Version :Unica De : 20-ENE-2011',\n",
       "     'labels': []},\n",
       "    '019ae05_7': {'text': 'Inicio Vigencia :20-01-2011', 'labels': []},\n",
       "    '019ae05_8': {'text': 'Id Norma :1022248', 'labels': []}}}},\n",
       " '6cef0d1b7182adadd6fe887a5e76e90324c503a1': {'HEADING': {'tags': [],\n",
       "   'sentences': {'6cef0d1_1': {'text': 'Tipo Norma :Decreto 28', 'labels': []},\n",
       "    '6cef0d1_2': {'text': 'Fecha Publicacion :19-10-2013', 'labels': []},\n",
       "    '6cef0d1_3': {'text': 'Fecha Promulgacion :05-04-2013', 'labels': []},\n",
       "    '6cef0d1_4': {'text': 'Organismo :MINISTERIO DE AGRICULTURA',\n",
       "     'labels': []},\n",
       "    '6cef0d1_5': {'text': 'Titulo :APRUEBA MODIFICACION AL DECRETO Nº 96, DE 2008, QUE REGLAMENTA LOS RECURSOS DESTINADOS A LA INVESTIGACION DEL BOSQUE NATIVO',\n",
       "     'labels': []},\n",
       "    '6cef0d1_6': {'text': 'Tipo Version :Unica De : 19-OCT-2013',\n",
       "     'labels': []},\n",
       "    '6cef0d1_7': {'text': 'Inicio Vigencia :19-10-2013', 'labels': []},\n",
       "    '6cef0d1_8': {'text': 'Id Norma :1055322', 'labels': []}}}},\n",
       " '74c55bda33a822e06e04492d5f01b9fd864e5ba6': {'HEADING': {'tags': [],\n",
       "   'sentences': {'74c55bd_1': {'text': 'Tipo Norma :Resolucion 233 EXENTA',\n",
       "     'labels': []},\n",
       "    '74c55bd_2': {'text': 'Fecha Publicacion :21-03-2013', 'labels': []},\n",
       "    '74c55bd_3': {'text': 'Fecha Promulgacion :14-03-2013', 'labels': []},\n",
       "    '74c55bd_4': {'text': 'Organismo :MINISTERIO DEL MEDIO AMBIENTE; SUPERINTENDENCIA DEL MEDIO AMBIENTE',\n",
       "     'labels': []},\n",
       "    '74c55bd_5': {'text': 'Titulo :INSTRUYE NORMAS DE CARACTER GENERAL SOBRE DEBERES DE REMISION DE INFORMACION ESTABLECIDOS EN PLANES DE PREVENCION Y/O DESCONTAMINACION RESPECTO DE FUENTES EMISORAS ESTACIONARIAS',\n",
       "     'labels': []},\n",
       "    '74c55bd_6': {'text': 'Tipo Version :Texto Original De : 21-MAR-2013',\n",
       "     'labels': []},\n",
       "    '74c55bd_7': {'text': 'Inicio Vigencia :21-03-2013', 'labels': []},\n",
       "    '74c55bd_8': {'text': 'Fin Vigencia :03-01-2016', 'labels': []},\n",
       "    '74c55bd_9': {'text': 'Derogacion :02-01-2016', 'labels': []},\n",
       "    '74c55bd_10': {'text': 'Id Norma :1049560', 'labels': []},\n",
       "    '74c55bd_11': {'text': 'Texto derogado :02-ENE-2016', 'labels': []}}}},\n",
       " '7546484f6dac7941d25ec5d834ce8666497290c7': {'HEADING': {'tags': [],\n",
       "   'sentences': {'7546484_1': {'text': 'Tipo Norma :Decreto 6837',\n",
       "     'labels': []},\n",
       "    '7546484_2': {'text': 'Fecha Publicacion :28-02-2007', 'labels': []},\n",
       "    '7546484_3': {'text': 'Fecha Promulgacion :29-12-2006', 'labels': []},\n",
       "    '7546484_4': {'text': 'Organismo :MUNICIPALIDAD DE LIMACHE', 'labels': []},\n",
       "    '7546484_5': {'text': 'Titulo :APRUEBA ORDENANZA LOCAL SOBRE NORMAS AMBIENTALES PARA EXTRACCION, PROCESAMIENTO, COMERCIALIZACION Y TRANSPORTE DE ARIDOS EN O DESDE POZOS LASTREROS',\n",
       "     'labels': []},\n",
       "    '7546484_6': {'text': 'Tipo Version :Unica De : 28-FEB-2007',\n",
       "     'labels': []},\n",
       "    '7546484_7': {'text': 'Inicio Vigencia :28-02-2007', 'labels': []},\n",
       "    '7546484_8': {'text': 'Id Norma :258684', 'labels': []}}}},\n",
       " '75ffb099a140f6e837c429236ce6f0b33d31a666': {'HEADING': {'tags': [],\n",
       "   'sentences': {'75ffb09_1': {'text': 'Tipo Norma :Decreto 39', 'labels': []},\n",
       "    '75ffb09_2': {'text': 'Fecha Publicacion :11-10-2013', 'labels': []},\n",
       "    '75ffb09_3': {'text': 'Fecha Promulgacion :31-08-2012', 'labels': []},\n",
       "    '75ffb09_4': {'text': 'Organismo :MINISTERIO DE AGRICULTURA',\n",
       "     'labels': []},\n",
       "    '75ffb09_5': {'text': 'Titulo :APRUEBA MODIFICACION A REGLAMENTO SOBRE ESTRUCTURA Y FUNCIONAMIENTO DE MATADEROS, ESTABLECIMIENTOS FRIGORIFICOS, CAMARAS FRIGORIFICAS Y PLANTAS DE DESPOSTE',\n",
       "     'labels': []},\n",
       "    '75ffb09_6': {'text': 'Tipo Version :Unica De : 11-OCT-2013',\n",
       "     'labels': []},\n",
       "    '75ffb09_7': {'text': 'Inicio Vigencia :11-10-2013', 'labels': []},\n",
       "    '75ffb09_8': {'text': 'Id Norma :1055043', 'labels': []}}}},\n",
       " '76f84f42d18d124006755dd3a7f17d41c23224a5': {'HEADING': {'tags': [],\n",
       "   'sentences': {'76f84f4_1': {'text': 'Tipo Norma :Resolucion 962 EXENTA',\n",
       "     'labels': []},\n",
       "    '76f84f4_2': {'text': 'Fecha Publicacion :02-01-2020', 'labels': []},\n",
       "    '76f84f4_3': {'text': 'Fecha Promulgacion :04-12-2019', 'labels': []},\n",
       "    '76f84f4_4': {'text': 'Organismo :MINISTERIO DE OBRAS PUBLICAS; DIRECCION GENERAL DE AGUAS',\n",
       "     'labels': []},\n",
       "    '76f84f4_5': {'text': \"Titulo :ORDENA A LOS TITULARES DE DERECHOS DE APROVECHAMIENTO DE AGUAS SUBTERRANEAS CUYOS PUNTOS DE CAPTACION SE ENCUENTRAN EN EL SECTOR ACUIFERO TINGUIRIRICA INFERIOR PERTENECIENTE AL ACUIFERO TINGUIRIRICA, DE LA REGION DEL LIBERTADOR GENERAL BERNARDO O'HIGGINS, INSTALAR Y MANTENER SISTEMAS DE MEDICION Y DE TRANSMISION DE EXTRACCIONES EFECTIVAS\",\n",
       "     'labels': []},\n",
       "    '76f84f4_6': {'text': 'Tipo Version :Unica De : 02-ENE-2020',\n",
       "     'labels': []},\n",
       "    '76f84f4_7': {'text': 'Inicio Vigencia :02-01-2020', 'labels': []},\n",
       "    '76f84f4_8': {'text': 'Id Norma :1140604', 'labels': []}}}},\n",
       " 'aa87f53a385577ac05df15c1df9f7876e85a8661': {'HEADING': {'tags': [],\n",
       "   'sentences': {'aa87f53_1': {'text': 'Tipo Norma :Resolucion 1 EXENTA',\n",
       "     'labels': []},\n",
       "    'aa87f53_2': {'text': 'Fecha Publicacion :08-01-2014', 'labels': []},\n",
       "    'aa87f53_3': {'text': 'Fecha Promulgacion :03-01-2014', 'labels': []},\n",
       "    'aa87f53_4': {'text': 'Organismo :MINISTERIO DEL MEDIO AMBIENTE; SUPERINTENDENCIA DEL MEDIO AMBIENTE',\n",
       "     'labels': []},\n",
       "    'aa87f53_5': {'text': 'Titulo :PROGRAMAS Y SUBPROGRAMAS SECTORIALES DE FISCALIZACION AMBIENTAL DE NORMAS DE EMISION PARA EL AÑO 2014',\n",
       "     'labels': []},\n",
       "    'aa87f53_6': {'text': 'Tipo Version :Unica De : 08-ENE-2014',\n",
       "     'labels': []},\n",
       "    'aa87f53_7': {'text': 'Inicio Vigencia :08-01-2014', 'labels': []},\n",
       "    'aa87f53_8': {'text': 'Id Norma :1057928', 'labels': []}}}},\n",
       " 'aa8d47340d977381c929e5a1737bb7e94333db8f': {'HEADING': {'tags': [],\n",
       "   'sentences': {'aa8d473_1': {'text': 'Tipo Norma :Resolucion 87 AFECTA',\n",
       "     'labels': []},\n",
       "    'aa8d473_2': {'text': 'Fecha Publicacion :14-02-2014', 'labels': []},\n",
       "    'aa8d473_3': {'text': 'Fecha Promulgacion :08-10-2013', 'labels': []},\n",
       "    'aa8d473_4': {'text': 'Organismo :GOBIERNO REGIONAL XV REGION DE ARICA Y PARINACOTA',\n",
       "     'labels': []},\n",
       "    'aa8d473_5': {'text': 'Titulo :APRUEBA PLAN REGIONAL DE DESARROLLO URBANO REGION DE ARICA Y PARINACOTA',\n",
       "     'labels': []},\n",
       "    'aa8d473_6': {'text': 'Tipo Version :Unica De : 14-FEB-2014',\n",
       "     'labels': []},\n",
       "    'aa8d473_7': {'text': 'Inicio Vigencia :14-02-2014', 'labels': []},\n",
       "    'aa8d473_8': {'text': 'Id Norma :1059448', 'labels': []}}}},\n",
       " 'f27de0a7e5242d0c32df1d637e85f2f68f394497': {'HEADING': {'tags': [],\n",
       "   'sentences': {'f27de0a_1': {'text': 'Tipo Norma :Resolucion 1498 EXENTA',\n",
       "     'labels': []},\n",
       "    'f27de0a_2': {'text': 'Publicacion :09-12-2004', 'labels': []},\n",
       "    'f27de0a_3': {'text': 'Promulgacion :02-12-2004', 'labels': []},\n",
       "    'f27de0a_4': {'text': 'Organismo :MINISTERIO DE AGRICULTURA; SERVICIO AGRICOLA Y GANADERO VIII REGION DEL BIOBIO',\n",
       "     'labels': []},\n",
       "    'f27de0a_5': {'text': 'Titulo :PROHIBE USO DE CAMPOS DE PASTOREO DE CORDILLERA QUE SEÑALA, POR PERIODO QUE INDICA REGULA USO PECUARIO EN CAMPOS DE PRE Y ALTA CORDILLERA, Y ESTABLECE OTRAS NORMAS DE PREVENCION FIEBRE AFTOSA',\n",
       "     'labels': []},\n",
       "    'f27de0a_6': {'text': 'Version :Unica De : 09-DIC-2004', 'labels': []},\n",
       "    'f27de0a_7': {'text': 'Inicio Vigencia :09-12-2004', 'labels': []},\n",
       "    'f27de0a_8': {'text': 'Id Norma :233300', 'labels': []}}}},\n",
       " 'f33fed112e8a030bd07a13e9fc02f7b68e50f3e8': {'HEADING': {'tags': [],\n",
       "   'sentences': {'f33fed1_1': {'text': 'Tipo Norma :Decreto 50', 'labels': []},\n",
       "    'f33fed1_2': {'text': 'Fecha Publicacion :28-04-2012', 'labels': []},\n",
       "    'f33fed1_3': {'text': 'Fecha Promulgacion :25-03-2011', 'labels': []},\n",
       "    'f33fed1_4': {'text': 'Organismo :MINISTERIO DE ECONOMIA, FOMENTO Y TURISMO; SUBSECRETARIA DE ECONOMIA Y EMPRESAS DE MENOR TAMAÑO',\n",
       "     'labels': []},\n",
       "    'f33fed1_5': {'text': 'Titulo :APRUEBA REGLAMENTO QUE FIJA PROCEDIMIENTO PARA OTORGAMIENTO DE CONCESIONES TURISTICAS EN AREAS SILVESTRES PROTEGIDAS DEL ESTADO',\n",
       "     'labels': []},\n",
       "    'f33fed1_6': {'text': 'Tipo Version :Unica De : 28-ABR-2012',\n",
       "     'labels': []},\n",
       "    'f33fed1_7': {'text': 'Inicio Vigencia :28-04-2012', 'labels': []},\n",
       "    'f33fed1_8': {'text': 'Id Norma :1039477', 'labels': []}}}},\n",
       " 'ff6cdccf62923f94bac18add8875f4b799f0adb0': {'HEADING': {'tags': [],\n",
       "   'sentences': {'ff6cdcc_1': {'text': 'Tipo Norma :Resolucion 1165 EXENTA',\n",
       "     'labels': []},\n",
       "    'ff6cdcc_2': {'text': 'Fecha Publicacion :30-09-2019', 'labels': []},\n",
       "    'ff6cdcc_3': {'text': 'Fecha Promulgacion :25-09-2019', 'labels': []},\n",
       "    'ff6cdcc_4': {'text': 'Organismo :MINISTERIO DEL MEDIO AMBIENTE',\n",
       "     'labels': []},\n",
       "    'ff6cdcc_5': {'text': 'Titulo :DA INICIO A LA ELABORACION DEL ANTEPROYECTO DE NORMA DE EMISION DE CONTAMINANTES EN CENTROS DE CULTIVO Y PLANTAS PROCESADORAS DE RECURSOS HIDROBIOLOGICOS QUE, EN FUNCION DE SUS OLORES, GENERAN MOLESTIA Y CONSTITUYEN UN RIESGO A LA CALIDAD DE VIDA DE LA POBLACION',\n",
       "     'labels': []},\n",
       "    'ff6cdcc_6': {'text': 'Tipo Version :Unica De : 30-SEP-2019',\n",
       "     'labels': []},\n",
       "    'ff6cdcc_7': {'text': 'Inicio Vigencia :30-09-2019', 'labels': []},\n",
       "    'ff6cdcc_8': {'text': 'Id Norma :1136846', 'labels': []}}}}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary items sorted by occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'APRUEBA \"ORDENANZA PARA LA EXTRACCION DE ARIDOS EN CAUCES Y ALVEOS DE CURSOS NATURALES DE AGUA QUE CONSTITUYEN BIENES NACIONALES DE USO PUBLICO Y EN POZOS LASTREROS DE PROPIEDAD PARTICULAR EN LA COMUNA DE PANQUEHUE\" Y SUS RESPECTIVOS ANEXOS': 1,\n",
       " 'ORDENANZA PARA LA EXTRACCION DE ARIDOS EN CAUCES Y ALVEOS DE CURSOS NATURALES DE AGUA QUE CONSTITUYEN BIENES NACIONALES DE USO PUBLICO Y EN POZOS LASTREROS DE PROPIEDAD PARTICULAR, DE LA COMUNA DE PANQUEHUE': 1,\n",
       " 'DE LOS PERMISOS DE EXTRACCION ARTESANAL DE SUBSISTENCIA': 1,\n",
       " 'DEL TERMINO DEL PERMISO, DE LAS SANCIONES Y DE LOS PROCEDIMIENTOS DE FISCALIZACION': 1,\n",
       " 'ANEXO N° 1 - ORDENANZA EXTRACCION DE ARIDOS COMUNA DE PANQUEHUE': 1,\n",
       " 'ANEXO N° 2 - ORDENANZA EXTRACCION DE ARIDOS COMUNA DE PANQUEHUE': 1,\n",
       " 'ACOGE A TRAMITACION ESTUDIO DE IMPACTO AMBIENTAL DEL PROYECTO SISTEMA DE TRATAMIENTO INTEGRAL DE LAS AGUAS SERVIDAS DE PUERTO MONTT, SEGUNDA PARTE': 1,\n",
       " 'MODIFICA DECRETO Nº 355 EXENTO, DE 2008, DE LA SUBSECRETARIA DE AGRICULTURA QUE AUTORIZA CIRCULACION EN DIAS Y HORAS INHABILES A VEHICULOS QUE INDICA': 1,\n",
       " 'APRUEBA MODIFICACION AL DECRETO Nº 96, DE 2008, QUE REGLAMENTA LOS RECURSOS DESTINADOS A LA INVESTIGACION DEL BOSQUE NATIVO': 1,\n",
       " 'INSTRUYE NORMAS DE CARACTER GENERAL SOBRE DEBERES DE REMISION DE INFORMACION ESTABLECIDOS EN PLANES DE PREVENCION Y/O DESCONTAMINACION RESPECTO DE FUENTES EMISORAS ESTACIONARIAS': 1,\n",
       " 'APRUEBA ORDENANZA LOCAL SOBRE NORMAS AMBIENTALES PARA EXTRACCION, PROCESAMIENTO, COMERCIALIZACION Y TRANSPORTE DE ARIDOS EN O DESDE POZOS LASTREROS': 1,\n",
       " 'APRUEBA MODIFICACION A REGLAMENTO SOBRE ESTRUCTURA Y FUNCIONAMIENTO DE MATADEROS, ESTABLECIMIENTOS FRIGORIFICOS, CAMARAS FRIGORIFICAS Y PLANTAS DE DESPOSTE': 1,\n",
       " \"ORDENA A LOS TITULARES DE DERECHOS DE APROVECHAMIENTO DE AGUAS SUBTERRANEAS CUYOS PUNTOS DE CAPTACION SE ENCUENTRAN EN EL SECTOR ACUIFERO TINGUIRIRICA INFERIOR PERTENECIENTE AL ACUIFERO TINGUIRIRICA, DE LA REGION DEL LIBERTADOR GENERAL BERNARDO O'HIGGINS, INSTALAR Y MANTENER SISTEMAS DE MEDICION Y DE TRANSMISION DE EXTRACCIONES EFECTIVAS\": 1,\n",
       " 'PROGRAMAS Y SUBPROGRAMAS SECTORIALES DE FISCALIZACION AMBIENTAL DE NORMAS DE EMISION PARA EL AÑO 2014': 1,\n",
       " 'APRUEBA PLAN REGIONAL DE DESARROLLO URBANO REGION DE ARICA Y PARINACOTA': 1,\n",
       " '\"PLAN REGIONAL DE DESARROLLO URBANO - REGION DE ARICA Y PARINACOTA': 1,\n",
       " 'PROHIBE USO DE CAMPOS DE PASTOREO DE CORDILLERA QUE SEÑALA, POR PERIODO QUE INDICA; REGULA USO PECUARIO EN CAMPOS DE PRE Y ALTA CORDILLERA, Y ESTABLECE OTRAS NORMAS DE PREVENCION FIEBRE AFTOSA': 1,\n",
       " 'APRUEBA REGLAMENTO QUE FIJA PROCEDIMIENTO PARA OTORGAMIENTO DE CONCESIONES TURISTICAS EN AREAS SILVESTRES PROTEGIDAS DEL ESTADO': 1,\n",
       " 'DA INICIO A LA ELABORACION DEL ANTEPROYECTO DE NORMA DE EMISION DE CONTAMINANTES EN CENTROS DE CULTIVO Y PLANTAS PROCESADORAS DE RECURSOS HIDROBIOLOGICOS QUE, EN FUNCION DE SUS OLORES, GENERAN MOLESTIA Y CONSTITUYEN UN RIESGO A LA CALIDAD DE VIDA DE LA POBLACION': 1}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict( sorted(bag_of_words.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary items sorted by heading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bag_of_words):\n",
    "    print(k, \":\", bag_of_words[k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
