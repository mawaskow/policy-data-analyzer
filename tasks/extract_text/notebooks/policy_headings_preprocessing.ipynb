{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Headings Curation of El Salvador Policies\n",
    "\n",
    "In this notebook there are a series of dictionaries and methods to curate section headings of El Salvador policies. Policies from El Salvador have a rather definite structure, so that the law text is organized under section headings. There are two kinds of sections, the ones that are general and that can be often found in many policies, and the ones which are specific. The sections headings which are more general often come with a whole range of name variants which makes the task of machine recognition difficult.\n",
    "\n",
    "The goal of this notebook is to group all pretreatment methods that would harmonize sections heading to make the further processing machine friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re, boto3, json, string, operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries of particular vocabularies to help in the curation of section headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most policies come with the final signatures. This is a piece of text that we want to be able to recognize. To make the\n",
    "# detection of signatures easier, this dictionary contain the most common terms that can be found in these lines of text.\n",
    "official_positions = {\"ALCALDE\" : 0,\n",
    "\"Alcalde\" : 0,\n",
    "\"MINISTRA\" : 0,\n",
    "\"Ministra\" : 0,\n",
    "\"MINISTRO\" : 0,\n",
    "\"Ministro\" : 0,\n",
    "\"PRESIDENTA\" : 0,\n",
    "\"Presidenta\" : 0,\n",
    "\"PRESIDENTE\" : 0,\n",
    "\"Presidente\" : 0,\n",
    "\"REGIDOR\" : 0,\n",
    "\"Regidor\"  : 0,\n",
    "\"REGIDORA\" : 0,\n",
    "\"regidora\" : 0,\n",
    "\"SECRETARIA\" : 0,\n",
    "\"Secretaria\" : 0,\n",
    "\"SECRETARIO\" : 0,\n",
    "\"Secretario\" : 0,\n",
    "\"SINDICA\" : 0,\n",
    "\"Sindica\" : 0,\n",
    "\"SINDICO\" : 0,\n",
    "\"Sindico\" : 0,\n",
    "\"VICEPRESIDENTA\" : 0,\n",
    "\"Vicepresidenta\" : 0,\n",
    "\"VICEPRESIDENTE\" : 0,\n",
    "\"Vicepresidente\" : 0\n",
    "}\n",
    "# This dictionary contains some correspondences among different text headings. This is under development and needs further\n",
    "# improvement.The idea is to merge in a single name all the headings that point to the same conceptual concept. For example,\n",
    "# \"Definiciones\" is a heading that can come alone or together with other terms so it can appear as \"Definiciones básicas\" or\n",
    "# \"Definiciones generales\". With the dictionary we can fetch all headings that contain the word \"Definiciones\" and change the\n",
    "# heading to \"Definiciones\".\n",
    "merges = {\n",
    "    \"CONCEPTOS\" : \"DEFINICIONES\",\n",
    "    \"DEFINICIONES\" : \"DEFINICIONES\",\n",
    "    \"DISPOSICIONES FINALES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES GENERALES\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"DISPOSICIONES PRELIMINARES\" : \"DISPOSICIONES PRELIMINARES\",\n",
    "    \"DISPOSICIONES REGULADORAS\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES RELATIVAS\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES ESPECIALES\" : \"DISPOSICIONES ESPECIALES\",\n",
    "    \"DISPOSICIONES TRANSITORIAS\" : \"DISPOSICIONES GENERALES\",\n",
    "    \"INFRACCIONES\" : \"INFRACCIONES\",\n",
    "    \"INFRACCION ES\" : \"INFRACCIONES\",\n",
    "    \"OBJETIVO\" : \"OBJETO\",\n",
    "    \"OBJETO\" : \"OBJETO\",\n",
    "    \"OBLIGACIONES\" : \"OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACIONE\" : \"OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGACION\" : \"OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"OBLIGATORIEDAD\" : \"OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"POR TANTO\" : \"POR TANTO\",\n",
    "    \"POR LO TANTO\" : \"POR TANTO\",\n",
    "    \"PROHIBICIONES\" : \"OBLIGACIONES Y PROHIBICIONES\",\n",
    "    \"PROHIBICION\" : \"OBLIGACIONES Y PROHIBICIONES\"\n",
    "}\n",
    "# Eventhough the general gramar rule in Spanish is not to accent uppercase, there are many cases where a word in a heding might\n",
    "# appear accented. This is a dictionary to armonize all headings without accents. The list is rather comprehensive, but there is\n",
    "# still room for improvement.\n",
    "# If we find some bug beyond simple misspelling which will be solved by spell checker, we can include it here. The example is in\n",
    "# the first row with \"ACTIVIDADESUSOS\" which was found several times in headings.\n",
    "bugs = {\"ACTIVIDADESUSOS\" : \"ACTIVIDADES DE USOS\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection to the AWS S3 bucket\n",
    "To effectively run this cell you need Omdena's credentials. Please keep them local and do not sync them in GitHub repos nor cloud drives. Before doing anything with this json file, please think of security!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_folder = Path(\"C:/Users/user/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "json_folder = Path(\"C:/Users/jordi/Google Drive/Els_meus_documents/projectes/CompetitiveIntelligence/WRI/Notebooks/credentials/\")\n",
    "filename = \"Omdena_key.json\"\n",
    "file = json_folder / filename\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    key_dict = json.load(f) \n",
    "\n",
    "for key in key_dict:\n",
    "    KEY = key\n",
    "    SECRET = key_dict[key]\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    service_name = 's3',\n",
    "    region_name = 'us-east-2',\n",
    "    aws_access_key_id = KEY,\n",
    "    aws_secret_access_key = SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate the uppercase ratio in a string. It is used to detect section headings\n",
    "def uppercase_ratio(string):\n",
    "    return(len(re.findall(r'[A-Z]',string))/len(string))\n",
    "\n",
    "# Regular expression to clear html tags (here is basically to remove the page tags)\n",
    "cleanr = re.compile(r'<.*?>')\n",
    "# Te function to clear html tags\n",
    "def clean_html_tags(string):\n",
    "  return cleanr.sub('', string)\n",
    "\n",
    "# Function to remove the last lines of a document, the ones that contain the signatures of the officials. It depends on the\n",
    "# dictionary \"official_positions\"\n",
    "def remove_signatures(line):\n",
    "    signature = False\n",
    "    for key in official_positions:\n",
    "        if key in line:\n",
    "            signature = True\n",
    "            break\n",
    "    return signature\n",
    "\n",
    "# Function to change accented words by non-accented counterparts. It depends on the dictionary \"accent_marks_bugs\" \n",
    "accents_out = re.compile(r'[áéíóúÁÉÍÓÚ]')\n",
    "accents_dict = {\"á\":\"a\",\"é\":\"e\",\"í\":\"i\",\"ó\":\"o\",\"ú\":\"u\",\"Á\":\"A\",\"É\":\"E\",\"Í\":\"I\",\"Ó\":\"O\",\"Ú\":\"U\"}\n",
    "def remove_accents(string):\n",
    "    for accent in accents_out.findall(string):\n",
    "        string = string.replace(accent, accents_dict[accent])\n",
    "    return string\n",
    "\n",
    "# Function to merge headlines expressing the same concept in different words. It depends on the dictionary \"merges\"\n",
    "def merge_concepts(line):\n",
    "    for key in merges:\n",
    "        if key in line:\n",
    "            line = merges[key]\n",
    "            break\n",
    "    return line\n",
    "\n",
    "def clean_bugs(line):\n",
    "    for key in bugs:\n",
    "        if key in line:\n",
    "            line = line.replace(key, bugs[key])\n",
    "    return line\n",
    "\n",
    "# Function to add items to the dictionary with duplicate removal\n",
    "def add_to_dict(string, dictionary, dupl_dict):\n",
    "    if string in dupl_dict or string == None:\n",
    "        pass\n",
    "    else:\n",
    "        dupl_dict[string] = 0\n",
    "        if string in dictionary:\n",
    "            dictionary[string] = dictionary[string] + 1\n",
    "        else:\n",
    "            dictionary[string] = 1\n",
    "    return dictionary\n",
    "\n",
    "# Regular expression to clear punctuation from a string\n",
    "clean_punct = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# Regular expression to clear words that introduce unnecessary variability to headings. Some regex still not work 100% we need\n",
    "# to improve them.\n",
    "clean_capitulo = re.compile(r'(APARTADO \\S*)|(APARTADO\\s)|(\\bART. \\S*)|(\\bART. \\d*)|(\\bArt. \\S*)|(Capítulo \\S*)|(CAPITULO \\S*)|(CAPITULO\\S*)|(CAPÍTULO \\S*)|(CAPITULÓ \\S*)|(CAPITULOS \\S*)|(CAPITUO \\S*)|(CATEGORIA\\b)|(CATEGORÍA\\b)|(SUBCATEGORIA\\b)|(SUBCATEGORÍA\\b)|(TITULO\\s\\S*)|(TÍTULO\\s\\S*)')\n",
    "# Function to remove 1. unwanted words; 2. punctuation; 3. leading white spaces\n",
    "def clean_headings(string):\n",
    "    string = clean_capitulo.sub('', string)\n",
    "    clean_string = clean_punct.sub('', string).rstrip().lstrip()\n",
    "    if clean_string != \"\":\n",
    "        return clean_string\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to process files from S3 bucket\n",
    "By executing this cell you will go through all policies in El Salvador and process section headings that will be saved in a dictionary. This should be merged with the notebook that builds up the final json files out of plain txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"text-extraction/\"\n",
    "filename = \"00a55afe4f55256567397a68df5d7f97e642480b\" # This is only if you want to test on a single file\n",
    "bag_of_words = {}\n",
    "sentences = []\n",
    "\n",
    "i = 0\n",
    "for obj in s3.Bucket('wri-latin-talent').objects.all().filter(Prefix='text-extraction'):\n",
    "    if folder in obj.key and obj.key.replace(folder, \"\") != \"\": # and filename in obj.key # Un comment the previous string to run the code just in one sample document.\n",
    "#         print(i, \"**\", obj.key)\n",
    "        file = obj.get()['Body'].read().decode('utf-8')  #get the file from S3 and read the body content\n",
    "        lines = file.split(\"\\n\") # Split by end of line and pipe lines into a list\n",
    "        duplicates_dict = {} #Sometimes the same heading can be found more than once in a document. This will help on removing them\n",
    "        for line in lines:\n",
    "            if uppercase_ratio(clean_html_tags(line)) > 0.6 and len(line) > 6:\n",
    "                if remove_signatures(line):\n",
    "                    break\n",
    "                else:\n",
    "                    line = clean_html_tags(line)\n",
    "                    line = remove_accents(line)\n",
    "                    line = clean_bugs(line)\n",
    "                    line = clean_headings(line)\n",
    "                    if line == None:\n",
    "                        continue\n",
    "                    line = merge_concepts(line)\n",
    "                    bag_of_words = add_to_dict(line, bag_of_words, duplicates_dict)\n",
    "            else:\n",
    "                sentences.append(line)\n",
    "#                 print(\"--\", line)\n",
    "#             s3.Object('wri-latin-talent', key).put(Body = content)#This will save all the contents in the string variable \"content\" into a txt file in the Pre-processed folder\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After preprocessing there are {} different headings in El Salvador policies\".format(len(bag_of_words)))\n",
    "print(\"{} documents have been processed\".format(i))\n",
    "print(\"There are {} lines of text as sentences\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict( sorted(bag_of_words.items(), key=operator.itemgetter(1),reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary items sorted by heading text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in sorted(bag_of_words):\n",
    "    print(k, \":\", bag_of_words[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving sentences as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"C:/Users/user/Google Drive/Els_meus_documents (1)/projectes/CompetitiveIntelligence/WRI/Notebooks/Data/\")\n",
    "filename = \"sentences.npy\"\n",
    "file = path / filename\n",
    "np_sentences = np.array(sentences)\n",
    "with open(file, 'wb') as f:\n",
    "    np.save(f, np_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline to process one file from HD folder\n",
    "This is a pipeline to process a test file in a local folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_folder = Path(\"../Documents_de_mostra/\")\n",
    "filename = \"00a55afe4f55256567397a68df5d7f97e642480b.pdf.txt\"\n",
    "bag_of_words = {}\n",
    "\n",
    "i = 0\n",
    "file = data_folder / filename\n",
    "with open(file, 'r', encoding = 'utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    duplicates_dict = {}\n",
    "    for line in lines:\n",
    "        line = clean_html_tags(line)\n",
    "        if uppercase_ratio(line) > 0.6 and len(line) > 6:\n",
    "            if remove_signatures(line):\n",
    "                break\n",
    "            else:\n",
    "#                 print(line)\n",
    "                line = remove_accents(line)\n",
    "                clean_line = clean_headings(line)\n",
    "                bag_of_words = add_to_dict(clean_line, bag_of_words, duplicates_dict)\n",
    "#                 print(clean_line)\n",
    "        i += 1\n",
    "#     data = file.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
